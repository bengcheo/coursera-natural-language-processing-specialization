{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eM2WI45DjCVp"
   },
   "source": [
    "# Assignment 1:  Sentiment with Deep Neural Networks\n",
    "\n",
    "Welcome to the first assignment of course 3. In this assignment, you will explore sentiment analysis using deep neural networks. \n",
    "## Outline\n",
    "- [Part 1:  Import libraries and try out Trax](#1)\n",
    "- [Part 2:  Importing the data](#2)\n",
    "    - [2.1  Loading in the data](#2.1)\n",
    "    - [2.2  Building the vocabulary](#2.2)\n",
    "    - [2.3  Converting a tweet to a tensor](#2.3)\n",
    "        - [Exercise 01](#ex01)\n",
    "    - [2.4  Creating a batch generator](#2.4)\n",
    "        - [Exercise 02](#ex02)\n",
    "- [Part 3:  Defining classes](#3)\n",
    "    - [3.1  ReLU class](#3.1)\n",
    "        - [Exercise 03](#ex03)\n",
    "    - [3.2  Dense class ](#3.2)\n",
    "        - [Exercise 04](#ex04)\n",
    "    - [3.3  Model](#3.3)\n",
    "        - [Exercise 05](#ex05)\n",
    "- [Part 4:  Training](#4)\n",
    "    - [4.1  Training the model](#4.1)\n",
    "        - [Exercise 06](#ex06)\n",
    "    - [4.2  Practice Making a prediction](#4.2)\n",
    "- [Part 5:  Evaluation  ](#5)\n",
    "    - [5.1  Computing the accuracy on a batch](#5.1)\n",
    "        - [Exercise 07](#ex07)\n",
    "    - [5.2  Testing your model on Validation Data](#5.2)\n",
    "        - [Exercise 08](#ex08)\n",
    "- [Part 6:  Testing with your own input](#6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8aeCPdILgrga"
   },
   "source": [
    "In course 1, you implemented Logistic regression and Naive Bayes for sentiment analysis. However if you were to give your old models an example like:\n",
    "\n",
    "<center> <span style='color:blue'> <b>This movie was almost good.</b> </span> </center>\n",
    "\n",
    "Your model would have predicted a positive sentiment for that review. However, that sentence has a negative sentiment and indicates that the movie was not good. To solve those kinds of misclassifications, you will write a program that uses deep neural networks to identify sentiment in text. By completing this assignment, you will: \n",
    "\n",
    "- Understand how you can build/design a model using layers\n",
    "- Train a model using a training loop\n",
    "- Use a binary cross-entropy loss function\n",
    "- Compute the accuracy of your model\n",
    "- Predict using your own input\n",
    "\n",
    "As you can tell, this model follows a similar structure to the one you previously implemented in the second course of this specialization. \n",
    "- Indeed most of the deep nets you will be implementing will have a similar structure. The only thing that changes is the model architecture, the inputs, and the outputs. Before starting the assignment, we will introduce you to the Google library `trax` that we use for building and training models.\n",
    "\n",
    "\n",
    "Now we will show you how to compute the gradient of a certain function `f` by just using `  .grad(f)`. \n",
    "\n",
    "- Trax source code can be found on Github: [Trax](https://github.com/google/trax)\n",
    "- The Trax code also uses the JAX library: [JAX](https://jax.readthedocs.io/en/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IOK4n9JEjCVs"
   },
   "source": [
    "<a name=\"1\"></a>\n",
    "# Part 1:  Import libraries and try out Trax\n",
    "\n",
    "- Let's import libraries and look at an example of using the Trax library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "colab_type": "code",
    "id": "WOTfm2P0jCVt",
    "outputId": "d4903011-7268-4bea-ae35-ea3fd0b46311"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import random as rnd\n",
    "\n",
    "# import relevant libraries\n",
    "import trax\n",
    "\n",
    "# set random seeds to make this notebook easier to replicate\n",
    "trax.supervised.trainer_lib.init_random_number_generators(31)\n",
    "\n",
    "# import trax.fastmath.numpy\n",
    "import trax.fastmath.numpy as np\n",
    "\n",
    "# import trax.layers\n",
    "from trax import layers as tl\n",
    "\n",
    "# import Layer from the utils.py file\n",
    "from utils import Layer, load_tweets, process_tweet\n",
    "#from utils import \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "EyMnUt38jCVw",
    "outputId": "6981112a-3bf8-48a9-bf7d-6fc6d64150b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(5., dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jax.interpreters.xla.DeviceArray'>\n"
     ]
    }
   ],
   "source": [
    "# Create an array using trax.fastmath.numpy\n",
    "a = np.array(5.0)\n",
    "\n",
    "# View the returned array\n",
    "display(a)\n",
    "\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WtEWAFUIjCVz"
   },
   "source": [
    "Notice that trax.fastmath.numpy returns a DeviceArray from the jax library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2RUtDtrjCV0"
   },
   "outputs": [],
   "source": [
    "# Define a function that will use the trax.fastmath.numpy array\n",
    "def f(x):\n",
    "    \n",
    "    # f = x^2\n",
    "    return (x**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "qvUd-xzqjCV4",
    "outputId": "cdda13b4-b56b-4d10-e6be-abbfcb523e78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(a) for a=5.0 is 25.0\n"
     ]
    }
   ],
   "source": [
    "# Call the function\n",
    "print(f\"f(a) for a={a} is {f(a)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yGKhSeasjCV7"
   },
   "source": [
    "The gradient (derivative) of function `f` with respect to its input `x` is the derivative of $x^2$.\n",
    "- The derivative of $x^2$ is $2x$.  \n",
    "- When x is 5, then $2x=10$.\n",
    "\n",
    "You can calculate the gradient of a function by using `trax.fastmath.grad(fun=)` and passing in the name of the function.\n",
    "- In this case the function you want to take the gradient of is `f`.\n",
    "- The object returned (saved in `grad_f` in this example) is a function that can calculate the gradient of f for a given trax.fastmath.numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2Im5Hkc9jCV8",
    "outputId": "4c963756-9b0d-46b8-b05d-a8e4de5d0740"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directly use trax.fastmath.grad to calculate the gradient (derivative) of the function\n",
    "grad_f = trax.fastmath.grad(fun=f)  # df / dx - Gradient of function f(x) with respect to x\n",
    "\n",
    "# View the type of the retuned object (it's a function)\n",
    "type(grad_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0lDIVvx3jCV_",
    "outputId": "16ad5e34-f634-4d38-ccd8-96786e1d412d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray(10., dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Call the newly created function and pass in a value for x (the DeviceArray stored in 'a')\n",
    "grad_calculation = grad_f(a)\n",
    "\n",
    "# View the result of calling the grad_f function\n",
    "display(grad_calculation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41l3WDBkjCWD"
   },
   "source": [
    "The function returned by trax.fastmath.grad takes in x=5 and calculates the gradient of f, which is 2*x, which is 10. The value is also stored as a DeviceArray from the jax library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZ8RUynQsktn"
   },
   "source": [
    "<a name=\"2\"></a>\n",
    "# Part 2:  Importing the data\n",
    "\n",
    "<a name=\"2.1\"></a>\n",
    "## 2.1  Loading in the data\n",
    "\n",
    "Import the data set.  \n",
    "- You may recognize this from earlier assignments in the specialization.\n",
    "- Details of process_tweet function are available in utils.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "h5ClwIOSuLJh",
    "outputId": "dee50364-c476-405f-8cdd-63d067b848d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of positive tweets: 5000\n",
      "The number of negative tweets: 5000\n",
      "length of train_x 8000\n",
      "length of val_x 2000\n"
     ]
    }
   ],
   "source": [
    "## DO NOT EDIT THIS CELL\n",
    "\n",
    "# Import functions from the utils.py file\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load positive and negative tweets\n",
    "all_positive_tweets, all_negative_tweets = load_tweets()\n",
    "\n",
    "# View the total number of positive and negative tweets.\n",
    "print(f\"The number of positive tweets: {len(all_positive_tweets)}\")\n",
    "print(f\"The number of negative tweets: {len(all_negative_tweets)}\")\n",
    "\n",
    "# Split positive set into validation and training\n",
    "val_pos   = all_positive_tweets[4000:] # generating validation set for positive tweets\n",
    "train_pos  = all_positive_tweets[:4000]# generating training set for positive tweets\n",
    "\n",
    "# Split negative set into validation and training\n",
    "val_neg   = all_negative_tweets[4000:] # generating validation set for negative tweets\n",
    "train_neg  = all_negative_tweets[:4000] # generating training set for nagative tweets\n",
    "\n",
    "# Combine training data into one set\n",
    "train_x = train_pos + train_neg \n",
    "\n",
    "# Combine validation data into one set\n",
    "val_x  = val_pos + val_neg\n",
    "\n",
    "# Set the labels for the training set (1 for positive, 0 for negative)\n",
    "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
    "\n",
    "# Set the labels for the validation set (1 for positive, 0 for negative)\n",
    "val_y  = np.append(np.ones(len(val_pos)), np.zeros(len(val_neg)))\n",
    "\n",
    "print(f\"length of train_x {len(train_x)}\")\n",
    "print(f\"length of val_x {len(val_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNQq4LmbjCWG"
   },
   "source": [
    "Now import a function that processes tweets (we've provided this in the utils.py file).\n",
    "- `process_tweets' removes unwanted characters e.g. hashtag, hyperlinks, stock tickers from tweet.\n",
    "- It also returns a list of words (it tokenizes the original string)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "2bRX6aPDjCWH",
    "outputId": "09497362-bf73-4d63-e99f-460027e91eb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original tweet at training position 0\n",
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
      "Tweet at training position 0 after processing:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import a function that processes the tweets\n",
    "# from utils import process_tweet\n",
    "\n",
    "# Try out function that processes tweets\n",
    "print(\"original tweet at training position 0\")\n",
    "print(train_pos[0])\n",
    "\n",
    "print(\"Tweet at training position 0 after processing:\")\n",
    "process_tweet(train_pos[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "00XdS3LOjCWK"
   },
   "source": [
    "Notice that the function `process_tweet` keeps key words, removes the hash # symbol, and ignores usernames (words that begin with '@').  It also returns a list of the words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ac4D5WSUAVub"
   },
   "source": [
    "<a name=\"2.2\"></a>\n",
    "## 2.2  Building the vocabulary\n",
    "\n",
    "Now build the vocabulary.\n",
    "- Map each word in each tweet to an integer (an \"index\"). \n",
    "- The following code does this for you, but please read it and understand what it's doing.\n",
    "- Note that you will build the vocabulary based on the training data. \n",
    "- To do so, you will assign an index to everyword by iterating over your training set.\n",
    "\n",
    "The vocabulary will also include some special tokens\n",
    "- `__PAD__`: padding\n",
    "- `</e>`: end of line\n",
    "- `__UNK__`: a token representing any word that is not in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "rQaHKs7kAVuc",
    "outputId": "1d360f94-902d-471c-d3c3-0d69c27f5eaf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab are 9088\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'__PAD__': 0,\n",
       " '__</e>__': 1,\n",
       " '__UNK__': 2,\n",
       " 'followfriday': 3,\n",
       " 'top': 4,\n",
       " 'engag': 5,\n",
       " 'member': 6,\n",
       " 'commun': 7,\n",
       " 'week': 8,\n",
       " ':)': 9,\n",
       " 'hey': 10,\n",
       " 'jame': 11,\n",
       " 'odd': 12,\n",
       " ':/': 13,\n",
       " 'pleas': 14,\n",
       " 'call': 15,\n",
       " 'contact': 16,\n",
       " 'centr': 17,\n",
       " '02392441234': 18,\n",
       " 'abl': 19,\n",
       " 'assist': 20,\n",
       " 'mani': 21,\n",
       " 'thank': 22,\n",
       " 'listen': 23,\n",
       " 'last': 24,\n",
       " 'night': 25,\n",
       " 'bleed': 26,\n",
       " 'amaz': 27,\n",
       " 'track': 28,\n",
       " 'scotland': 29,\n",
       " 'congrat': 30,\n",
       " 'yeaaah': 31,\n",
       " 'yipppi': 32,\n",
       " 'accnt': 33,\n",
       " 'verifi': 34,\n",
       " 'rqst': 35,\n",
       " 'succeed': 36,\n",
       " 'got': 37,\n",
       " 'blue': 38,\n",
       " 'tick': 39,\n",
       " 'mark': 40,\n",
       " 'fb': 41,\n",
       " 'profil': 42,\n",
       " '15': 43,\n",
       " 'day': 44,\n",
       " 'one': 45,\n",
       " 'irresist': 46,\n",
       " 'flipkartfashionfriday': 47,\n",
       " 'like': 48,\n",
       " 'keep': 49,\n",
       " 'love': 50,\n",
       " 'custom': 51,\n",
       " 'wait': 52,\n",
       " 'long': 53,\n",
       " 'hope': 54,\n",
       " 'enjoy': 55,\n",
       " 'happi': 56,\n",
       " 'friday': 57,\n",
       " 'lwwf': 58,\n",
       " 'second': 59,\n",
       " 'thought': 60,\n",
       " '‚Äô': 61,\n",
       " 'enough': 62,\n",
       " 'time': 63,\n",
       " 'dd': 64,\n",
       " 'new': 65,\n",
       " 'short': 66,\n",
       " 'enter': 67,\n",
       " 'system': 68,\n",
       " 'sheep': 69,\n",
       " 'must': 70,\n",
       " 'buy': 71,\n",
       " 'jgh': 72,\n",
       " 'go': 73,\n",
       " 'bayan': 74,\n",
       " ':D': 75,\n",
       " 'bye': 76,\n",
       " 'act': 77,\n",
       " 'mischiev': 78,\n",
       " 'etl': 79,\n",
       " 'layer': 80,\n",
       " 'in-hous': 81,\n",
       " 'wareh': 82,\n",
       " 'app': 83,\n",
       " 'katamari': 84,\n",
       " 'well': 85,\n",
       " '‚Ä¶': 86,\n",
       " 'name': 87,\n",
       " 'impli': 88,\n",
       " ':p': 89,\n",
       " 'influenc': 90,\n",
       " 'big': 91,\n",
       " '...': 92,\n",
       " 'juici': 93,\n",
       " 'selfi': 94,\n",
       " 'follow': 95,\n",
       " 'perfect': 96,\n",
       " 'alreadi': 97,\n",
       " 'know': 98,\n",
       " \"what'\": 99,\n",
       " 'great': 100,\n",
       " 'opportun': 101,\n",
       " 'junior': 102,\n",
       " 'triathlet': 103,\n",
       " 'age': 104,\n",
       " '12': 105,\n",
       " '13': 106,\n",
       " 'gatorad': 107,\n",
       " 'seri': 108,\n",
       " 'get': 109,\n",
       " 'entri': 110,\n",
       " 'lay': 111,\n",
       " 'greet': 112,\n",
       " 'card': 113,\n",
       " 'rang': 114,\n",
       " 'print': 115,\n",
       " 'today': 116,\n",
       " 'job': 117,\n",
       " ':-)': 118,\n",
       " \"friend'\": 119,\n",
       " 'lunch': 120,\n",
       " 'yummm': 121,\n",
       " 'nostalgia': 122,\n",
       " 'tb': 123,\n",
       " 'ku': 124,\n",
       " 'id': 125,\n",
       " 'conflict': 126,\n",
       " 'help': 127,\n",
       " \"here'\": 128,\n",
       " 'screenshot': 129,\n",
       " 'work': 130,\n",
       " 'hi': 131,\n",
       " 'liv': 132,\n",
       " 'hello': 133,\n",
       " 'need': 134,\n",
       " 'someth': 135,\n",
       " 'u': 136,\n",
       " 'fm': 137,\n",
       " 'twitter': 138,\n",
       " '‚Äî': 139,\n",
       " 'sure': 140,\n",
       " 'thing': 141,\n",
       " 'dm': 142,\n",
       " 'x': 143,\n",
       " \"i'v\": 144,\n",
       " 'heard': 145,\n",
       " 'four': 146,\n",
       " 'season': 147,\n",
       " 'pretti': 148,\n",
       " 'dope': 149,\n",
       " 'penthous': 150,\n",
       " 'obv': 151,\n",
       " 'gobigorgohom': 152,\n",
       " 'fun': 153,\n",
       " \"y'all\": 154,\n",
       " 'yeah': 155,\n",
       " 'suppos': 156,\n",
       " 'lol': 157,\n",
       " 'chat': 158,\n",
       " 'bit': 159,\n",
       " 'youth': 160,\n",
       " 'üíÖ': 161,\n",
       " 'üèΩ': 162,\n",
       " 'üíã': 163,\n",
       " 'seen': 164,\n",
       " 'year': 165,\n",
       " 'rest': 166,\n",
       " 'goe': 167,\n",
       " 'quickli': 168,\n",
       " 'bed': 169,\n",
       " 'music': 170,\n",
       " 'fix': 171,\n",
       " 'dream': 172,\n",
       " 'spiritu': 173,\n",
       " 'ritual': 174,\n",
       " 'festiv': 175,\n",
       " 'n√©pal': 176,\n",
       " 'begin': 177,\n",
       " 'line-up': 178,\n",
       " 'left': 179,\n",
       " 'see': 180,\n",
       " 'sarah': 181,\n",
       " 'send': 182,\n",
       " 'us': 183,\n",
       " 'email': 184,\n",
       " 'bitsy@bitdefender.com': 185,\n",
       " \"we'll\": 186,\n",
       " 'asap': 187,\n",
       " 'kik': 188,\n",
       " 'hatessuc': 189,\n",
       " '32429': 190,\n",
       " 'kikm': 191,\n",
       " 'lgbt': 192,\n",
       " 'tinder': 193,\n",
       " 'nsfw': 194,\n",
       " 'akua': 195,\n",
       " 'cumshot': 196,\n",
       " 'come': 197,\n",
       " 'hous': 198,\n",
       " 'nsn_supplement': 199,\n",
       " 'effect': 200,\n",
       " 'press': 201,\n",
       " 'releas': 202,\n",
       " 'distribut': 203,\n",
       " 'result': 204,\n",
       " 'link': 205,\n",
       " 'remov': 206,\n",
       " 'pressreleas': 207,\n",
       " 'newsdistribut': 208,\n",
       " 'bam': 209,\n",
       " 'bestfriend': 210,\n",
       " 'lot': 211,\n",
       " 'warsaw': 212,\n",
       " '<3': 213,\n",
       " 'x46': 214,\n",
       " 'everyon': 215,\n",
       " 'watch': 216,\n",
       " 'documentari': 217,\n",
       " 'earthl': 218,\n",
       " 'youtub': 219,\n",
       " 'support': 220,\n",
       " 'buuut': 221,\n",
       " 'oh': 222,\n",
       " 'look': 223,\n",
       " 'forward': 224,\n",
       " 'visit': 225,\n",
       " 'next': 226,\n",
       " 'letsgetmessi': 227,\n",
       " 'jo': 228,\n",
       " 'make': 229,\n",
       " 'feel': 230,\n",
       " 'better': 231,\n",
       " 'never': 232,\n",
       " 'anyon': 233,\n",
       " 'kpop': 234,\n",
       " 'flesh': 235,\n",
       " 'good': 236,\n",
       " 'girl': 237,\n",
       " 'best': 238,\n",
       " 'wish': 239,\n",
       " 'reason': 240,\n",
       " 'epic': 241,\n",
       " 'soundtrack': 242,\n",
       " 'shout': 243,\n",
       " 'ad': 244,\n",
       " 'video': 245,\n",
       " 'playlist': 246,\n",
       " 'would': 247,\n",
       " 'dear': 248,\n",
       " 'jordan': 249,\n",
       " 'okay': 250,\n",
       " 'fake': 251,\n",
       " 'gameplay': 252,\n",
       " ';)': 253,\n",
       " 'haha': 254,\n",
       " 'im': 255,\n",
       " 'kid': 256,\n",
       " 'stuff': 257,\n",
       " 'exactli': 258,\n",
       " 'product': 259,\n",
       " 'line': 260,\n",
       " 'etsi': 261,\n",
       " 'shop': 262,\n",
       " 'check': 263,\n",
       " 'vacat': 264,\n",
       " 'recharg': 265,\n",
       " 'normal': 266,\n",
       " 'charger': 267,\n",
       " 'asleep': 268,\n",
       " 'talk': 269,\n",
       " 'sooo': 270,\n",
       " 'someon': 271,\n",
       " 'text': 272,\n",
       " 'ye': 273,\n",
       " 'bet': 274,\n",
       " \"he'll\": 275,\n",
       " 'fit': 276,\n",
       " 'hear': 277,\n",
       " 'speech': 278,\n",
       " 'piti': 279,\n",
       " 'green': 280,\n",
       " 'garden': 281,\n",
       " 'midnight': 282,\n",
       " 'sun': 283,\n",
       " 'beauti': 284,\n",
       " 'canal': 285,\n",
       " 'dasvidaniya': 286,\n",
       " 'till': 287,\n",
       " 'scout': 288,\n",
       " 'sg': 289,\n",
       " 'futur': 290,\n",
       " 'wlan': 291,\n",
       " 'pro': 292,\n",
       " 'confer': 293,\n",
       " 'asia': 294,\n",
       " 'chang': 295,\n",
       " 'lollipop': 296,\n",
       " 'üç≠': 297,\n",
       " 'nez': 298,\n",
       " 'agnezmo': 299,\n",
       " 'oley': 300,\n",
       " 'mama': 301,\n",
       " 'stand': 302,\n",
       " 'stronger': 303,\n",
       " 'god': 304,\n",
       " 'misti': 305,\n",
       " 'babi': 306,\n",
       " 'cute': 307,\n",
       " 'woohoo': 308,\n",
       " \"can't\": 309,\n",
       " 'sign': 310,\n",
       " 'yet': 311,\n",
       " 'still': 312,\n",
       " 'think': 313,\n",
       " 'mka': 314,\n",
       " 'liam': 315,\n",
       " 'access': 316,\n",
       " 'welcom': 317,\n",
       " 'stat': 318,\n",
       " 'arriv': 319,\n",
       " '1': 320,\n",
       " 'unfollow': 321,\n",
       " 'via': 322,\n",
       " 'surpris': 323,\n",
       " 'figur': 324,\n",
       " 'happybirthdayemilybett': 325,\n",
       " 'sweet': 326,\n",
       " 'talent': 327,\n",
       " '2': 328,\n",
       " 'plan': 329,\n",
       " 'drain': 330,\n",
       " 'gotta': 331,\n",
       " 'timezon': 332,\n",
       " 'parent': 333,\n",
       " 'proud': 334,\n",
       " 'least': 335,\n",
       " 'mayb': 336,\n",
       " 'sometim': 337,\n",
       " 'grade': 338,\n",
       " 'al': 339,\n",
       " 'grand': 340,\n",
       " 'manila_bro': 341,\n",
       " 'chosen': 342,\n",
       " 'let': 343,\n",
       " 'around': 344,\n",
       " '..': 345,\n",
       " 'side': 346,\n",
       " 'world': 347,\n",
       " 'eh': 348,\n",
       " 'take': 349,\n",
       " 'care': 350,\n",
       " 'final': 351,\n",
       " 'fuck': 352,\n",
       " 'weekend': 353,\n",
       " 'real': 354,\n",
       " 'x45': 355,\n",
       " 'join': 356,\n",
       " 'hushedcallwithfraydo': 357,\n",
       " 'gift': 358,\n",
       " 'yeahhh': 359,\n",
       " 'hushedpinwithsammi': 360,\n",
       " 'event': 361,\n",
       " 'might': 362,\n",
       " 'luv': 363,\n",
       " 'realli': 364,\n",
       " 'appreci': 365,\n",
       " 'share': 366,\n",
       " 'wow': 367,\n",
       " 'tom': 368,\n",
       " 'gym': 369,\n",
       " 'monday': 370,\n",
       " 'invit': 371,\n",
       " 'scope': 372,\n",
       " 'friend': 373,\n",
       " 'nude': 374,\n",
       " 'sleep': 375,\n",
       " 'birthday': 376,\n",
       " 'want': 377,\n",
       " 't-shirt': 378,\n",
       " 'cool': 379,\n",
       " 'haw': 380,\n",
       " 'phela': 381,\n",
       " 'mom': 382,\n",
       " 'obvious': 383,\n",
       " 'princ': 384,\n",
       " 'charm': 385,\n",
       " 'stage': 386,\n",
       " 'luck': 387,\n",
       " 'tyler': 388,\n",
       " 'hipster': 389,\n",
       " 'glass': 390,\n",
       " 'marti': 391,\n",
       " 'glad': 392,\n",
       " 'done': 393,\n",
       " 'afternoon': 394,\n",
       " 'read': 395,\n",
       " 'kahfi': 396,\n",
       " 'finish': 397,\n",
       " 'ohmyg': 398,\n",
       " 'yaya': 399,\n",
       " 'dub': 400,\n",
       " 'stalk': 401,\n",
       " 'ig': 402,\n",
       " 'gondooo': 403,\n",
       " 'moo': 404,\n",
       " 'tologooo': 405,\n",
       " 'becom': 406,\n",
       " 'detail': 407,\n",
       " 'zzz': 408,\n",
       " 'xx': 409,\n",
       " 'physiotherapi': 410,\n",
       " 'hashtag': 411,\n",
       " 'üí™': 412,\n",
       " 'monica': 413,\n",
       " 'miss': 414,\n",
       " 'sound': 415,\n",
       " 'morn': 416,\n",
       " \"that'\": 417,\n",
       " 'x43': 418,\n",
       " 'definit': 419,\n",
       " 'tri': 420,\n",
       " 'tonight': 421,\n",
       " 'took': 422,\n",
       " 'advic': 423,\n",
       " 'treviso': 424,\n",
       " 'concert': 425,\n",
       " 'citi': 426,\n",
       " 'countri': 427,\n",
       " \"i'll\": 428,\n",
       " 'start': 429,\n",
       " 'fine': 430,\n",
       " 'gorgeou': 431,\n",
       " 'xo': 432,\n",
       " 'oven': 433,\n",
       " 'roast': 434,\n",
       " 'garlic': 435,\n",
       " 'oliv': 436,\n",
       " 'oil': 437,\n",
       " 'dri': 438,\n",
       " 'tomato': 439,\n",
       " 'basil': 440,\n",
       " 'centuri': 441,\n",
       " 'tuna': 442,\n",
       " 'right': 443,\n",
       " 'back': 444,\n",
       " 'atchya': 445,\n",
       " 'even': 446,\n",
       " 'almost': 447,\n",
       " 'chanc': 448,\n",
       " 'cheer': 449,\n",
       " 'po': 450,\n",
       " 'ice': 451,\n",
       " 'cream': 452,\n",
       " 'agre': 453,\n",
       " '100': 454,\n",
       " 'heheheh': 455,\n",
       " 'that': 456,\n",
       " 'point': 457,\n",
       " 'stay': 458,\n",
       " 'home': 459,\n",
       " 'soon': 460,\n",
       " 'promis': 461,\n",
       " 'web': 462,\n",
       " 'whatsapp': 463,\n",
       " 'volta': 464,\n",
       " 'funcionar': 465,\n",
       " 'com': 466,\n",
       " 'iphon': 467,\n",
       " 'jailbroken': 468,\n",
       " 'later': 469,\n",
       " '34': 470,\n",
       " 'min': 471,\n",
       " 'leia': 472,\n",
       " 'appear': 473,\n",
       " 'hologram': 474,\n",
       " 'r2d2': 475,\n",
       " 'w': 476,\n",
       " 'messag': 477,\n",
       " 'obi': 478,\n",
       " 'wan': 479,\n",
       " 'sit': 480,\n",
       " 'luke': 481,\n",
       " 'inter': 482,\n",
       " '3': 483,\n",
       " 'ucl': 484,\n",
       " 'arsen': 485,\n",
       " 'small': 486,\n",
       " 'team': 487,\n",
       " 'pass': 488,\n",
       " 'üöÇ': 489,\n",
       " 'dewsburi': 490,\n",
       " 'railway': 491,\n",
       " 'station': 492,\n",
       " 'dew': 493,\n",
       " 'west': 494,\n",
       " 'yorkshir': 495,\n",
       " '430': 496,\n",
       " 'smh': 497,\n",
       " '9:25': 498,\n",
       " 'live': 499,\n",
       " 'strang': 500,\n",
       " 'imagin': 501,\n",
       " 'megan': 502,\n",
       " 'masaantoday': 503,\n",
       " 'a4': 504,\n",
       " 'shweta': 505,\n",
       " 'tripathi': 506,\n",
       " '5': 507,\n",
       " '20': 508,\n",
       " 'kurta': 509,\n",
       " 'half': 510,\n",
       " 'number': 511,\n",
       " 'wsalelov': 512,\n",
       " 'ah': 513,\n",
       " 'larri': 514,\n",
       " 'anyway': 515,\n",
       " 'kinda': 516,\n",
       " 'goood': 517,\n",
       " 'life': 518,\n",
       " 'enn': 519,\n",
       " 'could': 520,\n",
       " 'warmup': 521,\n",
       " '15th': 522,\n",
       " 'bath': 523,\n",
       " 'dum': 524,\n",
       " 'andar': 525,\n",
       " 'ram': 526,\n",
       " 'sampath': 527,\n",
       " 'sona': 528,\n",
       " 'mohapatra': 529,\n",
       " 'samantha': 530,\n",
       " 'edward': 531,\n",
       " 'mein': 532,\n",
       " 'tulan': 533,\n",
       " 'razi': 534,\n",
       " 'wah': 535,\n",
       " 'josh': 536,\n",
       " 'alway': 537,\n",
       " 'smile': 538,\n",
       " 'pictur': 539,\n",
       " '16.20': 540,\n",
       " 'giveitup': 541,\n",
       " 'given': 542,\n",
       " 'ga': 543,\n",
       " 'subsidi': 544,\n",
       " 'initi': 545,\n",
       " 'propos': 546,\n",
       " 'delight': 547,\n",
       " 'yesterday': 548,\n",
       " 'x42': 549,\n",
       " 'lmaoo': 550,\n",
       " 'song': 551,\n",
       " 'ever': 552,\n",
       " 'shall': 553,\n",
       " 'littl': 554,\n",
       " 'throwback': 555,\n",
       " 'outli': 556,\n",
       " 'island': 557,\n",
       " 'cheung': 558,\n",
       " 'chau': 559,\n",
       " 'mui': 560,\n",
       " 'wo': 561,\n",
       " 'total': 562,\n",
       " 'differ': 563,\n",
       " 'kfckitchentour': 564,\n",
       " 'kitchen': 565,\n",
       " 'clean': 566,\n",
       " \"i'm\": 567,\n",
       " 'cusp': 568,\n",
       " 'test': 569,\n",
       " 'water': 570,\n",
       " 'reward': 571,\n",
       " 'arummzz': 572,\n",
       " \"let'\": 573,\n",
       " 'drive': 574,\n",
       " 'travel': 575,\n",
       " 'yogyakarta': 576,\n",
       " 'jeep': 577,\n",
       " 'indonesia': 578,\n",
       " 'instamood': 579,\n",
       " 'wanna': 580,\n",
       " 'skype': 581,\n",
       " 'may': 582,\n",
       " 'nice': 583,\n",
       " 'friendli': 584,\n",
       " 'pretend': 585,\n",
       " 'film': 586,\n",
       " 'congratul': 587,\n",
       " 'winner': 588,\n",
       " 'cheesydelight': 589,\n",
       " 'contest': 590,\n",
       " 'address': 591,\n",
       " 'guy': 592,\n",
       " 'market': 593,\n",
       " '24/7': 594,\n",
       " '14': 595,\n",
       " 'hour': 596,\n",
       " 'leav': 597,\n",
       " 'without': 598,\n",
       " 'delay': 599,\n",
       " 'actual': 600,\n",
       " 'easi': 601,\n",
       " 'guess': 602,\n",
       " 'train': 603,\n",
       " 'wd': 604,\n",
       " 'shift': 605,\n",
       " 'engin': 606,\n",
       " 'etc': 607,\n",
       " 'sunburn': 608,\n",
       " 'peel': 609,\n",
       " 'blog': 610,\n",
       " 'huge': 611,\n",
       " 'warm': 612,\n",
       " '‚òÜ': 613,\n",
       " 'complet': 614,\n",
       " 'triangl': 615,\n",
       " 'northern': 616,\n",
       " 'ireland': 617,\n",
       " 'sight': 618,\n",
       " 'smthng': 619,\n",
       " 'fr': 620,\n",
       " 'hug': 621,\n",
       " 'xoxo': 622,\n",
       " 'uu': 623,\n",
       " 'jaann': 624,\n",
       " 'topnewfollow': 625,\n",
       " 'connect': 626,\n",
       " 'wonder': 627,\n",
       " 'made': 628,\n",
       " 'fluffi': 629,\n",
       " 'insid': 630,\n",
       " 'pirouett': 631,\n",
       " 'moos': 632,\n",
       " 'trip': 633,\n",
       " 'philli': 634,\n",
       " 'decemb': 635,\n",
       " \"i'd\": 636,\n",
       " 'dude': 637,\n",
       " 'x41': 638,\n",
       " 'question': 639,\n",
       " 'flaw': 640,\n",
       " 'pain': 641,\n",
       " 'negat': 642,\n",
       " 'strength': 643,\n",
       " 'went': 644,\n",
       " 'solo': 645,\n",
       " 'move': 646,\n",
       " 'fav': 647,\n",
       " 'nirvana': 648,\n",
       " 'smell': 649,\n",
       " 'teen': 650,\n",
       " 'spirit': 651,\n",
       " 'rip': 652,\n",
       " 'ami': 653,\n",
       " 'winehous': 654,\n",
       " 'coupl': 655,\n",
       " 'tomhiddleston': 656,\n",
       " 'elizabetholsen': 657,\n",
       " 'yaytheylookgreat': 658,\n",
       " 'goodnight': 659,\n",
       " 'vid': 660,\n",
       " 'wake': 661,\n",
       " 'gonna': 662,\n",
       " 'shoot': 663,\n",
       " 'itti': 664,\n",
       " 'bitti': 665,\n",
       " 'teeni': 666,\n",
       " 'bikini': 667,\n",
       " 'much': 668,\n",
       " '4th': 669,\n",
       " 'togeth': 670,\n",
       " 'end': 671,\n",
       " 'xfile': 672,\n",
       " 'content': 673,\n",
       " 'rain': 674,\n",
       " 'fabul': 675,\n",
       " 'fantast': 676,\n",
       " '‚ô°': 677,\n",
       " 'jb': 678,\n",
       " 'forev': 679,\n",
       " 'belieb': 680,\n",
       " 'nighti': 681,\n",
       " 'bug': 682,\n",
       " 'bite': 683,\n",
       " 'bracelet': 684,\n",
       " 'idea': 685,\n",
       " 'foundri': 686,\n",
       " 'game': 687,\n",
       " 'sens': 688,\n",
       " 'pic': 689,\n",
       " 'ef': 690,\n",
       " 'phone': 691,\n",
       " 'woot': 692,\n",
       " 'derek': 693,\n",
       " 'use': 694,\n",
       " 'parkshar': 695,\n",
       " 'gloucestershir': 696,\n",
       " 'aaaahhh': 697,\n",
       " 'man': 698,\n",
       " 'traffic': 699,\n",
       " 'stress': 700,\n",
       " 'reliev': 701,\n",
       " \"how'r\": 702,\n",
       " 'arbeloa': 703,\n",
       " 'turn': 704,\n",
       " '17': 705,\n",
       " 'omg': 706,\n",
       " 'say': 707,\n",
       " 'europ': 708,\n",
       " 'rise': 709,\n",
       " 'find': 710,\n",
       " 'hard': 711,\n",
       " 'believ': 712,\n",
       " 'uncount': 713,\n",
       " 'coz': 714,\n",
       " 'unlimit': 715,\n",
       " 'cours': 716,\n",
       " 'teamposit': 717,\n",
       " 'aldub': 718,\n",
       " '‚òï': 719,\n",
       " 'rita': 720,\n",
       " 'info': 721,\n",
       " \"we'd\": 722,\n",
       " 'way': 723,\n",
       " 'boy': 724,\n",
       " 'x40': 725,\n",
       " 'true': 726,\n",
       " 'sethi': 727,\n",
       " 'high': 728,\n",
       " 'exe': 729,\n",
       " 'skeem': 730,\n",
       " 'saam': 731,\n",
       " 'peopl': 732,\n",
       " 'polit': 733,\n",
       " 'izzat': 734,\n",
       " 'wese': 735,\n",
       " 'trust': 736,\n",
       " 'khawateen': 737,\n",
       " 'k': 738,\n",
       " 'sath': 739,\n",
       " 'mana': 740,\n",
       " 'kar': 741,\n",
       " 'deya': 742,\n",
       " 'sort': 743,\n",
       " 'smart': 744,\n",
       " 'hair': 745,\n",
       " 'tbh': 746,\n",
       " 'jacob': 747,\n",
       " 'g': 748,\n",
       " 'upgrad': 749,\n",
       " 'tee': 750,\n",
       " 'famili': 751,\n",
       " 'person': 752,\n",
       " 'two': 753,\n",
       " 'convers': 754,\n",
       " 'onlin': 755,\n",
       " 'mclaren': 756,\n",
       " 'fridayfeel': 757,\n",
       " 'tgif': 758,\n",
       " 'squar': 759,\n",
       " 'enix': 760,\n",
       " 'bissmillah': 761,\n",
       " 'ya': 762,\n",
       " 'allah': 763,\n",
       " \"we'r\": 764,\n",
       " 'socent': 765,\n",
       " 'startup': 766,\n",
       " 'drop': 767,\n",
       " 'your': 768,\n",
       " 'arnd': 769,\n",
       " 'town': 770,\n",
       " 'basic': 771,\n",
       " 'piss': 772,\n",
       " 'cup': 773,\n",
       " 'also': 774,\n",
       " 'terribl': 775,\n",
       " 'complic': 776,\n",
       " 'discuss': 777,\n",
       " 'snapchat': 778,\n",
       " 'lynettelow': 779,\n",
       " 'kikmenow': 780,\n",
       " 'snapm': 781,\n",
       " 'hot': 782,\n",
       " 'amazon': 783,\n",
       " 'kikmeguy': 784,\n",
       " 'defin': 785,\n",
       " 'grow': 786,\n",
       " 'sport': 787,\n",
       " 'rt': 788,\n",
       " 'rakyat': 789,\n",
       " 'write': 790,\n",
       " 'sinc': 791,\n",
       " 'mention': 792,\n",
       " 'fli': 793,\n",
       " 'fish': 794,\n",
       " 'promot': 795,\n",
       " 'post': 796,\n",
       " 'cyber': 797,\n",
       " 'ourdaughtersourprid': 798,\n",
       " 'mypapamyprid': 799,\n",
       " 'papa': 800,\n",
       " 'coach': 801,\n",
       " 'posit': 802,\n",
       " 'kha': 803,\n",
       " 'atleast': 804,\n",
       " 'x39': 805,\n",
       " 'mango': 806,\n",
       " \"lassi'\": 807,\n",
       " \"monty'\": 808,\n",
       " 'marvel': 809,\n",
       " 'though': 810,\n",
       " 'suspect': 811,\n",
       " 'meant': 812,\n",
       " '24': 813,\n",
       " 'hr': 814,\n",
       " 'touch': 815,\n",
       " 'kepler': 816,\n",
       " '452b': 817,\n",
       " 'chalna': 818,\n",
       " 'hai': 819,\n",
       " 'thankyou': 820,\n",
       " 'hazel': 821,\n",
       " 'food': 822,\n",
       " 'brooklyn': 823,\n",
       " 'pta': 824,\n",
       " 'awak': 825,\n",
       " 'okayi': 826,\n",
       " 'awww': 827,\n",
       " 'ha': 828,\n",
       " 'doc': 829,\n",
       " 'splendid': 830,\n",
       " 'spam': 831,\n",
       " 'folder': 832,\n",
       " 'amount': 833,\n",
       " 'nigeria': 834,\n",
       " 'claim': 835,\n",
       " 'rted': 836,\n",
       " 'leg': 837,\n",
       " 'hurt': 838,\n",
       " 'bad': 839,\n",
       " 'mine': 840,\n",
       " 'saturday': 841,\n",
       " 'thaaank': 842,\n",
       " 'puhon': 843,\n",
       " 'happinesss': 844,\n",
       " 'tnc': 845,\n",
       " 'prior': 846,\n",
       " 'notif': 847,\n",
       " 'fat': 848,\n",
       " 'co': 849,\n",
       " 'probabl': 850,\n",
       " 'ate': 851,\n",
       " 'yuna': 852,\n",
       " 'tamesid': 853,\n",
       " '¬¥': 854,\n",
       " 'googl': 855,\n",
       " 'account': 856,\n",
       " 'scouser': 857,\n",
       " 'everyth': 858,\n",
       " 'zoe': 859,\n",
       " 'mate': 860,\n",
       " 'liter': 861,\n",
       " \"they'r\": 862,\n",
       " 'samee': 863,\n",
       " 'edgar': 864,\n",
       " 'updat': 865,\n",
       " 'log': 866,\n",
       " 'bring': 867,\n",
       " 'abe': 868,\n",
       " 'meet': 869,\n",
       " 'x38': 870,\n",
       " 'sigh': 871,\n",
       " 'dreamili': 872,\n",
       " 'pout': 873,\n",
       " 'eye': 874,\n",
       " 'quacketyquack': 875,\n",
       " 'funni': 876,\n",
       " 'happen': 877,\n",
       " 'phil': 878,\n",
       " 'em': 879,\n",
       " 'del': 880,\n",
       " 'rodder': 881,\n",
       " 'els': 882,\n",
       " 'play': 883,\n",
       " 'newest': 884,\n",
       " 'gamejam': 885,\n",
       " 'irish': 886,\n",
       " 'literatur': 887,\n",
       " 'inaccess': 888,\n",
       " \"kareena'\": 889,\n",
       " 'fan': 890,\n",
       " 'brain': 891,\n",
       " 'dot': 892,\n",
       " 'braindot': 893,\n",
       " 'fair': 894,\n",
       " 'rush': 895,\n",
       " 'either': 896,\n",
       " 'brandi': 897,\n",
       " '18': 898,\n",
       " 'carniv': 899,\n",
       " 'men': 900,\n",
       " 'put': 901,\n",
       " 'mask': 902,\n",
       " 'xavier': 903,\n",
       " 'forneret': 904,\n",
       " 'jennif': 905,\n",
       " 'site': 906,\n",
       " 'free': 907,\n",
       " '50.000': 908,\n",
       " '8': 909,\n",
       " 'ball': 910,\n",
       " 'pool': 911,\n",
       " 'coin': 912,\n",
       " 'edit': 913,\n",
       " 'trish': 914,\n",
       " '‚ô•': 915,\n",
       " 'grate': 916,\n",
       " 'three': 917,\n",
       " 'comment': 918,\n",
       " 'wakeup': 919,\n",
       " 'besid': 920,\n",
       " 'dirti': 921,\n",
       " 'sex': 922,\n",
       " 'lmaooo': 923,\n",
       " 'üò§': 924,\n",
       " 'loui': 925,\n",
       " \"he'\": 926,\n",
       " 'throw': 927,\n",
       " 'caus': 928,\n",
       " 'inspir': 929,\n",
       " 'ff': 930,\n",
       " 'twoof': 931,\n",
       " 'gr8': 932,\n",
       " 'wkend': 933,\n",
       " 'kind': 934,\n",
       " 'exhaust': 935,\n",
       " 'word': 936,\n",
       " 'cheltenham': 937,\n",
       " 'area': 938,\n",
       " 'kale': 939,\n",
       " 'crisp': 940,\n",
       " 'ruin': 941,\n",
       " 'x37': 942,\n",
       " 'open': 943,\n",
       " 'worldwid': 944,\n",
       " 'outta': 945,\n",
       " 'sfvbeta': 946,\n",
       " 'vantast': 947,\n",
       " 'xcylin': 948,\n",
       " 'bundl': 949,\n",
       " 'show': 950,\n",
       " 'internet': 951,\n",
       " 'price': 952,\n",
       " 'realisticli': 953,\n",
       " 'pay': 954,\n",
       " 'net': 955,\n",
       " 'educ': 956,\n",
       " 'power': 957,\n",
       " 'weapon': 958,\n",
       " 'nelson': 959,\n",
       " 'mandela': 960,\n",
       " 'recent': 961,\n",
       " 'j': 962,\n",
       " 'chenab': 963,\n",
       " 'flow': 964,\n",
       " 'pakistan': 965,\n",
       " 'incredibleindia': 966,\n",
       " 'teenchoic': 967,\n",
       " 'choiceinternationalartist': 968,\n",
       " 'superjunior': 969,\n",
       " 'caught': 970,\n",
       " 'first': 971,\n",
       " 'salmon': 972,\n",
       " 'super-blend': 973,\n",
       " 'project': 974,\n",
       " 'youth@bipolaruk.org.uk': 975,\n",
       " 'awesom': 976,\n",
       " 'stream': 977,\n",
       " 'alma': 978,\n",
       " 'mater': 979,\n",
       " 'highschoolday': 980,\n",
       " 'clientvisit': 981,\n",
       " 'faith': 982,\n",
       " 'christian': 983,\n",
       " 'school': 984,\n",
       " 'lizaminnelli': 985,\n",
       " 'upcom': 986,\n",
       " 'uk': 987,\n",
       " 'üòÑ': 988,\n",
       " 'singl': 989,\n",
       " 'hill': 990,\n",
       " 'everi': 991,\n",
       " 'beat': 992,\n",
       " 'wrong': 993,\n",
       " 'readi': 994,\n",
       " 'natur': 995,\n",
       " 'pefumeri': 996,\n",
       " 'workshop': 997,\n",
       " 'neal': 998,\n",
       " 'yard': 999,\n",
       " ...}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the vocabulary\n",
    "# Unit Test Note - There is no test set here only train/val\n",
    "\n",
    "# Include special tokens \n",
    "# started with pad, end of line and unk tokens\n",
    "Vocab = {'__PAD__': 0, '__</e>__': 1, '__UNK__': 2} \n",
    "\n",
    "# Note that we build vocab using training data\n",
    "for tweet in train_x: \n",
    "    processed_tweet = process_tweet(tweet)\n",
    "    for word in processed_tweet:\n",
    "        if word not in Vocab: \n",
    "            Vocab[word] = len(Vocab)\n",
    "    \n",
    "print(\"Total words in vocab are\",len(Vocab))\n",
    "display(Vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gP6K9qcCAVue"
   },
   "source": [
    "The dictionary `Vocab` will look like this:\n",
    "```CPP\n",
    "{'__PAD__': 0,\n",
    " '__</e>__': 1,\n",
    " '__UNK__': 2,\n",
    " 'followfriday': 3,\n",
    " 'top': 4,\n",
    " 'engag': 5,\n",
    " ...\n",
    "```\n",
    "\n",
    "- Each unique word has a unique integer associated with it.\n",
    "- The total number of words in Vocab: 9088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0x8pND8tAVuf"
   },
   "source": [
    "<a name=\"2.3\"></a>\n",
    "## 2.3  Converting a tweet to a tensor\n",
    "\n",
    "Write a function that will convert each tweet to a tensor (a list of unique integer IDs representing the processed tweet).\n",
    "- Note, the returned data type will be a **regular Python `list()`**\n",
    "    - You won't use TensorFlow in this function\n",
    "    - You also won't use a numpy array\n",
    "    - You also won't use trax.fastmath.numpy array\n",
    "- For words in the tweet that are not in the vocabulary, set them to the unique ID for the token `__UNK__`.\n",
    "\n",
    "##### Example\n",
    "Input a tweet:\n",
    "```CPP\n",
    "'@happypuppy, is Maria happy?'\n",
    "```\n",
    "\n",
    "The tweet_to_tensor will first conver the tweet into a list of tokens (including only relevant words)\n",
    "```CPP\n",
    "['maria', 'happi']\n",
    "```\n",
    "\n",
    "Then it will convert each word into its unique integer\n",
    "\n",
    "```CPP\n",
    "[2, 56]\n",
    "```\n",
    "- Notice that the word \"maria\" is not in the vocabulary, so it is assigned the unique integer associated with the `__UNK__` token, because it is considered \"unknown.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QtQhtv0kjCWQ"
   },
   "source": [
    "<a name=\"ex01\"></a>\n",
    "### Exercise 01\n",
    "**Instructions:** Write a program `tweet_to_tensor` that takes in a tweet and converts it to an array of numbers. You can use the `Vocab` dictionary you just found to help create the tensor. \n",
    "\n",
    "- Use the vocab_dict parameter and not a global variable.\n",
    "- Do not hard code the integer value for the `__UNK__` token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sKSx1SBYjCWR"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Map each word in tweet to corresponding token in 'Vocab'</li>\n",
    "    <li>Use Python's Dictionary.get(key,value) so that the function returns a default value if the key is not found in the dictionary.</li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ft1zNGMaAVuf"
   },
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: tweet_to_tensor\n",
    "def tweet_to_tensor(tweet, vocab_dict, unk_token='__UNK__', verbose=True):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet - A string containing a tweet\n",
    "        vocab_dict - The words dictionary\n",
    "        unk_token - The special string for unknown tokens\n",
    "        verbose - Print info durign runtime\n",
    "    Output:\n",
    "        tensor_l - A python list with\n",
    "        \n",
    "    '''  \n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"List of words from the processed tweet:\")\n",
    "        print(word_l)\n",
    "        \n",
    "    # Initialize the list that will contain the unique integer IDs of each word\n",
    "    tensor_l = []\n",
    "    \n",
    "    # Get the unique integer ID of the __UNK__ token\n",
    "    unk_ID = vocab_dict[unk_token]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"The unique integer ID for the unk_token is {unk_ID}\")\n",
    "        \n",
    "    # for each word in the list:\n",
    "    for word in word_l:\n",
    "        \n",
    "        # Get the unique integer ID.\n",
    "        # If the word doesn't exist in the vocab dictionary,\n",
    "        # use the unique ID for __UNK__ instead.\n",
    "        word_ID = vocab_dict[word] if word in vocab_dict.keys() else unk_ID\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "        # Append the unique integer ID to the tensor list.\n",
    "        tensor_l.append(word_ID) \n",
    "    \n",
    "    return tensor_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "ze0Zx_5UjCWU",
    "outputId": "a9427907-b364-4c5f-fed5-bd2502e18bab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual tweet is\n",
      " Bro:U wan cut hair anot,ur hair long Liao bo\n",
      "Me:since ord liao,take it easy lor treat as save $ leave it longer :)\n",
      "Bro:LOL Sibei xialan\n",
      "List of words from the processed tweet:\n",
      "['bro', 'u', 'wan', 'cut', 'hair', 'anot', 'ur', 'hair', 'long', 'liao', 'bo', 'sinc', 'ord', 'liao', 'take', 'easi', 'lor', 'treat', 'save', 'leav', 'longer', ':)', 'bro', 'lol', 'sibei', 'xialan']\n",
      "The unique integer ID for the unk_token is 2\n",
      "\n",
      "Tensor of tweet:\n",
      " [1065, 136, 479, 2351, 745, 8148, 1123, 745, 53, 2, 2672, 791, 2, 2, 349, 601, 2, 3489, 1017, 597, 4559, 9, 1065, 157, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(\"Actual tweet is\\n\", val_pos[0])\n",
    "print(\"\\nTensor of tweet:\\n\", tweet_to_tensor(val_pos[0], vocab_dict=Vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0MbWVeijCWW"
   },
   "source": [
    "##### Expected output\n",
    "\n",
    "```CPP\n",
    "Actual tweet is\n",
    " Bro:U wan cut hair anot,ur hair long Liao bo\n",
    "Me:since ord liao,take it easy lor treat as save $ leave it longer :)\n",
    "Bro:LOL Sibei xialan\n",
    "\n",
    "Tensor of tweet:\n",
    " [1065, 136, 479, 2351, 745, 8148, 1123, 745, 53, 2, 2672, 791, 2, 2, 349, 601, 2, 3489, 1017, 597, 4559, 9, 1065, 157, 2, 2]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bMmf0pPcjCWX",
    "outputId": "7b2a4f49-12df-4dde-fa27-15c55d4c409b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words from the processed tweet:\n",
      "['back', 'thnx', 'god', \"i'm\", 'happi', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['back', 'thnx', 'god', \"i'm\", 'happi', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['back', 'thnx', 'god', \"i'm\", 'happi', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# test tweet_to_tensor\n",
    "\n",
    "def test_tweet_to_tensor():\n",
    "    test_cases = [\n",
    "        \n",
    "        {\n",
    "            \"name\":\"simple_test_check\",\n",
    "            \"input\": [val_pos[1], Vocab],\n",
    "            \"expected\":[444, 2, 304, 567, 56, 9],\n",
    "            \"error\":\"The function gives bad output for val_pos[1]. Test failed\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"datatype_check\",\n",
    "            \"input\":[val_pos[1], Vocab],\n",
    "            \"expected\":type([]),\n",
    "            \"error\":\"Datatype mismatch. Need only list not np.array\"\n",
    "        },\n",
    "        {\n",
    "            \"name\":\"without_unk_check\",\n",
    "            \"input\":[val_pos[1], Vocab],\n",
    "            \"expected\":6,\n",
    "            \"error\":\"Unk word check not done- Please check if you included mapping for unknown word\"\n",
    "        }\n",
    "    ]\n",
    "    count = 0\n",
    "    for test_case in test_cases:\n",
    "        \n",
    "        try:\n",
    "            if test_case['name'] == \"simple_test_check\":\n",
    "                assert test_case[\"expected\"] == tweet_to_tensor(*test_case['input'])\n",
    "                count += 1\n",
    "            if test_case['name'] == \"datatype_check\":\n",
    "                assert isinstance(tweet_to_tensor(*test_case['input']), test_case[\"expected\"])\n",
    "                count += 1\n",
    "            if test_case['name'] == \"without_unk_check\":\n",
    "                assert None not in tweet_to_tensor(*test_case['input'])\n",
    "                count += 1\n",
    "                \n",
    "            \n",
    "            \n",
    "        except:\n",
    "            print(test_case['error'])\n",
    "    if count == 3:\n",
    "        print(\"\\033[92m All tests passed\")\n",
    "    else:\n",
    "        print(count,\" Tests passed out of 3\")\n",
    "test_tweet_to_tensor()            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rwAZZIYYAVuj"
   },
   "source": [
    "<a name=\"2.4\"></a>\n",
    "## 2.4  Creating a batch generator\n",
    "\n",
    "Most of the time in Natural Language Processing, and AI in general we use batches when training our data sets. \n",
    "- If instead of training with batches of examples, you were to train a model with one example at a time, it would take a very long time to train the model. \n",
    "- You will now build a data generator that takes in the positive/negative tweets and returns a batch of training examples. It returns the model inputs, the targets (positive or negative labels) and the weight for each target (ex: this allows us to can treat some examples as more important to get right than others, but commonly this will all be 1.0). \n",
    "\n",
    "Once you create the generator, you could include it in a for loop\n",
    "\n",
    "```CPP\n",
    "for batch_inputs, batch_targets, batch_example_weights in data_generator:\n",
    "    ...\n",
    "```\n",
    "\n",
    "You can also get a single batch like this:\n",
    "\n",
    "```CPP\n",
    "batch_inputs, batch_targets, batch_example_weights = next(data_generator)\n",
    "```\n",
    "The generator returns the next batch each time it's called. \n",
    "- This generator returns the data in a format (tensors) that you could directly use in your model.\n",
    "- It returns a triple: the inputs, targets, and loss weights:\n",
    "-- Inputs is a tensor that contains the batch of tweets we put into the model.\n",
    "-- Targets is the corresponding batch of labels that we train to generate.\n",
    "-- Loss weights here are just 1s with same shape as targets. Next week, you will use it to mask input padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sR-sF_o0jCWa"
   },
   "source": [
    "<a name=\"ex02\"></a>\n",
    "### Exercise 02\n",
    "Implement `data_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fPd9HNT7AVuk"
   },
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED: Data generator\n",
    "def data_generator(data_pos, data_neg, batch_size, loop, vocab_dict, shuffle=False):\n",
    "    '''\n",
    "    Input: \n",
    "        data_pos - Set of posstive examples\n",
    "        data_neg - Set of negative examples\n",
    "        batch_size - number of samples per batch. Must be even\n",
    "        loop - True or False\n",
    "        vocab_dict - The words dictionary\n",
    "        shuffle - Shuffle the data order\n",
    "    Yield:\n",
    "        inputs - Subset of positive and negative examples\n",
    "        targets - The corresponding labels for the subset\n",
    "        example_weights - An array specifying the importance of each example\n",
    "        \n",
    "    '''     \n",
    "### START GIVEN CODE ###\n",
    "    # make sure the batch size is an even number\n",
    "    # to allow an equal number of positive and negative samples\n",
    "    assert batch_size % 2 == 0\n",
    "    \n",
    "    # Number of positive examples in each batch is half of the batch size\n",
    "    # same with number of negative examples in each batch\n",
    "    n_to_take = batch_size // 2\n",
    "    \n",
    "    # Use pos_index to walk through the data_pos array\n",
    "    # same with neg_index and data_neg\n",
    "    pos_index = 0\n",
    "    neg_index = 0\n",
    "    \n",
    "    len_data_pos = len(data_pos)\n",
    "    len_data_neg = len(data_neg)\n",
    "    \n",
    "    # Get and array with the data indexes\n",
    "    pos_index_lines = list(range(len_data_pos))\n",
    "    neg_index_lines = list(range(len_data_neg))\n",
    "    \n",
    "    # shuffle lines if shuffle is set to True\n",
    "    if shuffle:\n",
    "        rnd.shuffle(pos_index_lines)\n",
    "        rnd.shuffle(neg_index_lines)\n",
    "        \n",
    "    stop = False\n",
    "    \n",
    "    # Loop indefinitely\n",
    "    while not stop:  \n",
    "        \n",
    "        # create a batch with positive and negative examples\n",
    "        batch = []\n",
    "        \n",
    "        # First part: Pack n_to_take positive examples\n",
    "        \n",
    "        # Start from pos_index and increment i up to n_to_take\n",
    "        for i in range(n_to_take):\n",
    "                    \n",
    "            # If the positive index goes past the positive dataset lenght,\n",
    "            if pos_index >= len_data_pos: \n",
    "                \n",
    "                # If loop is set to False, break once we reach the end of the dataset\n",
    "                if not loop:\n",
    "                    stop = True;\n",
    "                    break;\n",
    "                \n",
    "                # If user wants to keep re-using the data, reset the index\n",
    "                pos_index = 0\n",
    "                \n",
    "                if shuffle:\n",
    "                    # Shuffle the index of the positive sample\n",
    "                    rnd.shuffle(pos_index_lines)\n",
    "                    \n",
    "            # get the tweet as pos_index\n",
    "            tweet = data_pos[pos_index_lines[pos_index]]\n",
    "            \n",
    "            # convert the tweet into tensors of integers representing the processed words\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            \n",
    "            # append the tensor to the batch list\n",
    "            batch.append(tensor)\n",
    "            \n",
    "            # Increment pos_index by one\n",
    "            pos_index = pos_index + 1\n",
    "\n",
    "### END GIVEN CODE ###\n",
    "            \n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "        # Second part: Pack n_to_take negative examples\n",
    "    \n",
    "        # Using the same batch list, start from neg_index and increment i up to n_to_take\n",
    "        for i in range(n_to_take):\n",
    "            \n",
    "            # If the negative index goes past the negative dataset length,\n",
    "            if neg_index >= len_data_neg:\n",
    "                \n",
    "                # If loop is set to False, break once we reach the end of the dataset\n",
    "                if not loop:\n",
    "                    stop = True;\n",
    "                    break;\n",
    "                    \n",
    "                # If user wants to keep re-using the data, reset the index\n",
    "                neg_index = 0\n",
    "                \n",
    "                if shuffle:\n",
    "                    # Shuffle the index of the negative sample\n",
    "                    rnd.shuffle(neg_index_lines)\n",
    "            # get the tweet as neg_index\n",
    "            tweet = data_neg[neg_index_lines[neg_index]]\n",
    "            \n",
    "            # convert the tweet into tensors of integers representing the processed words\n",
    "            tensor = tweet_to_tensor(tweet, vocab_dict)\n",
    "            \n",
    "            # append the tensor to the batch list\n",
    "            batch.append(tensor)\n",
    "            \n",
    "            # Increment neg_index by one\n",
    "            neg_index = neg_index + 1\n",
    "\n",
    "### END CODE HERE ###        \n",
    "\n",
    "### START GIVEN CODE ###\n",
    "        if stop:\n",
    "            break;\n",
    "\n",
    "        # Update the start index for positive data \n",
    "        # so that it's n_to_take positions after the current pos_index\n",
    "        pos_index += n_to_take\n",
    "        \n",
    "        # Update the start index for negative data \n",
    "        # so that it's n_to_take positions after the current neg_index\n",
    "        neg_index += n_to_take\n",
    "        \n",
    "        # Get the max tweet length (the length of the longest tweet) \n",
    "        # (you will pad all shorter tweets to have this length)\n",
    "        max_len = max([len(t) for t in batch]) \n",
    "        \n",
    "        \n",
    "        # Initialize the input_l, which will \n",
    "        # store the padded versions of the tensors\n",
    "        tensor_pad_l = []\n",
    "        # Pad shorter tweets with zeros\n",
    "        for tensor in batch:\n",
    "### END GIVEN CODE ###\n",
    "\n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "            # Get the number of positions to pad for this tensor so that it will be max_len long\n",
    "            n_pad = max_len - len(tensor)\n",
    "            \n",
    "            # Generate a list of zeros, with length n_pad\n",
    "            pad_l = list(np.zeros(n_pad, dtype=int))\n",
    "            \n",
    "            # concatenate the tensor and the list of padded zeros\n",
    "            tensor_pad = tensor + pad_l\n",
    "            \n",
    "            # append the padded tensor to the list of padded tensors\n",
    "            tensor_pad_l.append(tensor_pad)\n",
    "\n",
    "        # convert the list of padded tensors to a numpy array\n",
    "        # and store this as the model inputs\n",
    "        inputs = np.array(tensor_pad_l)\n",
    "  \n",
    "        # Generate the list of targets for the positive examples (a list of ones)\n",
    "        # The length is the number of positive examples in the batch\n",
    "        target_pos = [1 for i in range(n_to_take)]\n",
    "        \n",
    "        # Generate the list of targets for the negative examples (a list of zeros)\n",
    "        # The length is the number of negative examples in the batch\n",
    "        target_neg = [0 for i in range(n_to_take)]\n",
    "        \n",
    "        # Concatenate the positve and negative targets\n",
    "        target_l = target_pos + target_neg\n",
    "        \n",
    "        # Convert the target list into a numpy array\n",
    "        targets = np.array(target_l)\n",
    "\n",
    "        # Example weights: Treat all examples equally importantly.It should return an np.array. Hint: Use np.ones_like()\n",
    "        example_weights = np.ones_like(targets)\n",
    "        \n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "### GIVEN CODE ###\n",
    "        # note we use yield and not return\n",
    "        yield inputs, targets, example_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kI9gEdqpjCWd"
   },
   "source": [
    "Now you can use your data generator to create a data generator for the training data, and another data generator for the validation data.\n",
    "\n",
    "We will create a third data generator that does not loop, for testing the final accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "iIwM4YHtAVum",
    "outputId": "d25aa4ae-bad3-41f5-963c-0a091f8fb108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words from the processed tweet:\n",
      "['part', 'elit', 'group', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['aug', \"i'm\", 'super', 'pack', 'howev', 'rather', 'thing', 'rather', 'work', 'home', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'get', 'u', 'nice', 'lil', 'gf']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['okay', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Inputs: [[2005 4451 3201    9    0    0    0    0    0    0    0]\n",
      " [4954  567 2000 1454 5174 3499  141 3499  130  459    9]\n",
      " [3761  109  136  583 2930 3969    0    0    0    0    0]\n",
      " [ 250 3761    0    0    0    0    0    0    0    0    0]]\n",
      "Targets: [1 1 0 0]\n",
      "Example Weights: [1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Set the random number generator for the shuffle procedure\n",
    "rnd.seed(30) \n",
    "\n",
    "# Create the training data generator\n",
    "def train_generator(batch_size, shuffle = False):\n",
    "    return data_generator(train_pos, train_neg, batch_size, True, Vocab, shuffle)\n",
    "\n",
    "# Create the validation data generator\n",
    "def val_generator(batch_size, shuffle = False):\n",
    "    return data_generator(val_pos, val_neg, batch_size, True, Vocab, shuffle)\n",
    "\n",
    "# Create the validation data generator\n",
    "def test_generator(batch_size, shuffle = False):\n",
    "    return data_generator(val_pos, val_neg, batch_size, False, Vocab, shuffle)\n",
    "\n",
    "# Get a batch from the train_generator and inspect.\n",
    "inputs, targets, example_weights = next(train_generator(4, shuffle=True))\n",
    "\n",
    "# this will print a list of 4 tensors padded with zeros\n",
    "print(f'Inputs: {inputs}')\n",
    "print(f'Targets: {targets}')\n",
    "print(f'Example Weights: {example_weights}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "mcDOyrx9jCWh",
    "outputId": "1adae96d-fb14-4cc6-d39e-7e4d9517c239"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words from the processed tweet:\n",
      "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hey', 'jame', 'odd', ':/', 'pleas', 'call', 'contact', 'centr', '02392441234', 'abl', 'assist', ':)', 'mani', 'thank']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hopeless', 'tmr', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['everyth', 'kid', 'section', 'ikea', 'cute', 'shame', \"i'm\", 'nearli', '19', '2', 'month', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "The inputs shape is (4, 14)\n",
      "The targets shape is (4,)\n",
      "The example weights shape is (4,)\n",
      "input tensor: [3 4 5 6 7 8 9 0 0 0 0 0 0 0]; target 1; example weights 1\n",
      "input tensor: [10 11 12 13 14 15 16 17 18 19 20  9 21 22]; target 1; example weights 1\n",
      "input tensor: [5738 2901 3761    0    0    0    0    0    0    0    0    0    0    0]; target 0; example weights 1\n",
      "input tensor: [ 858  256 3652 5739  307 4458  567 1230 2767  328 1202 3761    0    0]; target 0; example weights 1\n"
     ]
    }
   ],
   "source": [
    "# Test the train_generator\n",
    "\n",
    "# Create a data generator for training data,\n",
    "# which produces batches of size 4 (for tensors and their respective targets)\n",
    "tmp_data_gen = train_generator(batch_size = 4)\n",
    "\n",
    "# Call the data generator to get one batch and its targets\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = next(tmp_data_gen)\n",
    "\n",
    "print(f\"The inputs shape is {tmp_inputs.shape}\")\n",
    "print(f\"The targets shape is {tmp_targets.shape}\")\n",
    "print(f\"The example weights shape is {tmp_example_weights.shape}\")\n",
    "\n",
    "for i,t in enumerate(tmp_inputs):\n",
    "    print(f\"input tensor: {t}; target {tmp_targets[i]}; example weights {tmp_example_weights[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCTl7w1kjCWj"
   },
   "source": [
    "##### Expected output\n",
    "\n",
    "```CPP\n",
    "The inputs shape is (4, 14)\n",
    "The targets shape is (4,)\n",
    "The example weights shape is (4,)\n",
    "input tensor: [3 4 5 6 7 8 9 0 0 0 0 0 0 0]; target 1; example weights 1\n",
    "input tensor: [10 11 12 13 14 15 16 17 18 19 20  9 21 22]; target 1; example weights 1\n",
    "input tensor: [5738 2901 3761    0    0    0    0    0    0    0    0    0    0    0]; target 0; example weights 1\n",
    "input tensor: [ 858  256 3652 5739  307 4458  567 1230 2767  328 1202 3761    0    0]; target 0; example weights 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J3HrgcJmAVup"
   },
   "source": [
    "Now that you have your train/val generators, you can just call them and they will return tensors which correspond to your tweets in the first column and their corresponding labels in the second column. Now you can go ahead and start building your neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X591GrH_stXq"
   },
   "source": [
    "<a name=\"3\"></a>\n",
    "# Part 3:  Defining classes\n",
    "\n",
    "In this part, you will write your own library of layers. It will be very similar\n",
    "to the one used in Trax and also in Keras and PyTorch. Writing your own small\n",
    "framework will help you understand how they all work and use them effectively\n",
    "in the future.\n",
    "\n",
    "Your framework will be based on the following `Layer` class from utils.py.\n",
    "\n",
    "```CPP\n",
    "class Layer(object):\n",
    "    \"\"\" Base class for layers.\n",
    "    \"\"\"\n",
    "      \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        # set weights to None\n",
    "        self.weights = None\n",
    "\n",
    "    # The forward propagation should be implemented\n",
    "    # by subclasses of this Layer class\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # This function initializes the weights\n",
    "    # based on the input signature and random key,\n",
    "    # should be implemented by subclasses of this Layer class\n",
    "    def init_weights_and_state(self, input_signature, random_key):\n",
    "        pass\n",
    "\n",
    "    # This initializes and returns the weights, do not override.\n",
    "    def init(self, input_signature, random_key):\n",
    "        self.init_weights_and_state(input_signature, random_key)\n",
    "        return self.weights\n",
    " \n",
    "    # __call__ allows an object of this class\n",
    "    # to be called like it's a function.\n",
    "    def __call__(self, x):\n",
    "        # When this layer object is called, \n",
    "        # it calls its forward propagation function\n",
    "        return self.forward(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TcWUXFaPzS-m"
   },
   "source": [
    "<a name=\"3.1\"></a>\n",
    "## 3.1  ReLU class\n",
    "You will now implement the ReLU activation function in a class below. The ReLU function looks as follows: \n",
    "<img src = \"relu.jpg\" style=\"width:300px;height:150px;\"/>\n",
    "\n",
    "$$ \\mathrm{ReLU}(x) = \\mathrm{max}(0,x) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVVRVuhjjCWl"
   },
   "source": [
    "<a name=\"ex03\"></a>\n",
    "### Exercise 03\n",
    "**Instructions:** Implement the ReLU activation function below. Your function should take in a matrix or vector and it should transform all the negative numbers into 0 while keeping all the positive numbers intact. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dPH_EVcHjCWl"
   },
   "source": [
    "<details>    \n",
    "<summary>\n",
    "    <font size=\"3\" color=\"darkgreen\"><b>Hints</b></font>\n",
    "</summary>\n",
    "<p>\n",
    "<ul>\n",
    "    <li>Please use numpy.maximum(A,k) to find the maximum between each element in A and a scalar k</li>\n",
    "</ul>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VGE5zZ5mzF9x"
   },
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: Relu\n",
    "class Relu(Layer):\n",
    "    \"\"\"Relu activation function implementation\"\"\"\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Input: \n",
    "            - x (a numpy array): the input\n",
    "        Output:\n",
    "            - activation (numpy array): all positive or 0 version of x\n",
    "        '''\n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        \n",
    "        activation = np.maximum(x, 0)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "hVQ3YtoZ1uYP",
    "outputId": "6565529b-abe1-4b01-f5a8-36f8c7d0d903"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data is:\n",
      "[[-2. -1.  0.]\n",
      " [ 0.  1.  2.]]\n",
      "Output of Relu is:\n",
      "[[0. 0. 0.]\n",
      " [0. 1. 2.]]\n"
     ]
    }
   ],
   "source": [
    "# Test your relu function\n",
    "x = np.array([[-2.0, -1.0, 0.0], [0.0, 1.0, 2.0]], dtype=float)\n",
    "relu_layer = Relu()\n",
    "print(\"Test data is:\")\n",
    "print(x)\n",
    "print(\"Output of Relu is:\")\n",
    "print(relu_layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "niL6mIuBAVuu"
   },
   "source": [
    "##### Expected Outout\n",
    "```CPP\n",
    "Test data is:\n",
    "[[-2. -1.  0.]\n",
    " [ 0.  1.  2.]]\n",
    "Output of Relu is:\n",
    "[[0. 0. 0.]\n",
    " [0. 1. 2.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XepjDxCQ1G8p"
   },
   "source": [
    "<a name=\"3.2\"></a>\n",
    "## 3.2  Dense class \n",
    "\n",
    "### Exercise\n",
    "\n",
    "Implement the forward function of the Dense class. \n",
    "- The forward function multiplies the input to the layer (`x`) by the weight matrix (`W`)\n",
    "\n",
    "$$\\mathrm{forward}(\\mathbf{x},\\mathbf{W}) = \\mathbf{xW} $$\n",
    "\n",
    "- You can use `numpy.dot` to perform the matrix multiplication.\n",
    "\n",
    "Note that for more efficient code execution, you will use the trax version of `math`, which includes a trax version of `numpy` and also `random`.\n",
    "\n",
    "Implement the weight initializer `new_weights` function\n",
    "- Weights are initialized with a random key.\n",
    "- The second parameter is a tuple for the desired shape of the weights (num_rows, num_cols)\n",
    "- The num of rows for weights should equal the number of columns in x, because for forward propagation, you will multiply x times weights.\n",
    "\n",
    "Please use `trax.fastmath.random.normal(key, shape, dtype=tf.float32)` to generate random values for the weight matrix. The key difference between this function\n",
    "and the standard `numpy` randomness is the explicit use of random keys, which\n",
    "need to be passed. While it can look tedious at the first sight to pass the random key everywhere, you will learn in Course 4 why this is very helpful when\n",
    "implementing some advanced models.\n",
    "- `key` can be generated by calling `random.get_prng(seed=)` and passing in a number for the `seed`.\n",
    "- `shape` is a tuple with the desired shape of the weight matrix.\n",
    "    - The number of rows in the weight matrix should equal the number of columns in the variable `x`.  Since `x` may have 2 dimensions if it reprsents a single training example (row, col), or three dimensions (batch_size, row, col), get the last dimension from the tuple that holds the dimensions of x.\n",
    "    - The number of columns in the weight matrix is the number of units chosen for that dense layer.  Look at the `__init__` function to see which variable stores the number of units.\n",
    "- `dtype` is the data type of the values in the generated matrix; keep the default of `tf.float32`. In this case, don't explicitly set the dtype (just let it use the default value).\n",
    "\n",
    "Set the standard deviation of the random values to 0.1\n",
    "- The values generated have a mean of 0 and standard deviation of 1.\n",
    "- Set the default standard deviation `stdev` to be 0.1 by multiplying the standard deviation to each of the values in the weight matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJqiv5KnjCWr"
   },
   "outputs": [],
   "source": [
    "# use the fastmath module within trax\n",
    "from trax import fastmath\n",
    "\n",
    "# use the numpy module from trax\n",
    "np = fastmath.numpy\n",
    "\n",
    "# use the fastmath.random module from trax\n",
    "random = fastmath.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "6reTe6asjCWt",
    "outputId": "94480024-f9ec-43d2-f657-f523c9c78504"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The random seed generated by random.get_prng\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([0, 1], dtype=uint32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose a matrix with 2 rows and 3 columns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight matrix generated with a normal distribution with mean 0 and stdev of 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.95730704, -0.96992904,  1.0070664 ],\n",
       "             [ 0.36619025,  0.17294823,  0.29092228]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# See how the fastmath.trax.random.normal function works\n",
    "tmp_key = random.get_prng(seed=1)\n",
    "print(\"The random seed generated by random.get_prng\")\n",
    "display(tmp_key)\n",
    "\n",
    "print(\"choose a matrix with 2 rows and 3 columns\")\n",
    "tmp_shape=(2,3)\n",
    "display(tmp_shape)\n",
    "\n",
    "# Generate a weight matrix\n",
    "# Note that you'll get an error if you try to set dtype to tf.float32, where tf is tensorflow\n",
    "# Just avoid setting the dtype and allow it to use the default data type\n",
    "tmp_weight = trax.fastmath.random.normal(key=tmp_key, shape=tmp_shape)\n",
    "\n",
    "print(\"Weight matrix generated with a normal distribution with mean 0 and stdev of 1\")\n",
    "display(tmp_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IpiJ87L9jCWw"
   },
   "source": [
    "<a name=\"ex04\"></a>\n",
    "### Exercise 04\n",
    "\n",
    "Implement the `Dense` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "783FfWt70660"
   },
   "outputs": [],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: Dense\n",
    "\n",
    "class Dense(Layer):\n",
    "    \"\"\"\n",
    "    A dense (fully-connected) layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # __init__ is implemented for you\n",
    "    def __init__(self, n_units, init_stdev=0.1):\n",
    "        \n",
    "        # Set the number of units in this layer\n",
    "        self._n_units = n_units\n",
    "        self._init_stdev = init_stdev\n",
    "\n",
    "    # Please implement 'forward()'\n",
    "    def forward(self, x):\n",
    "\n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "        # Matrix multiply x and the weight matrix\n",
    "        dense = np.dot(x, self.weights)\n",
    "        \n",
    "### END CODE HERE ###\n",
    "        return dense\n",
    "\n",
    "    # init_weights\n",
    "    def init_weights_and_state(self, input_signature, random_key):\n",
    "        \n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "        # The input_signature has a .shape attribute that gives the shape as a tuple\n",
    "        input_shape = (input_signature.shape[-1], self._n_units)\n",
    "\n",
    "        # Generate the weight matrix from a normal distribution, \n",
    "        # and standard deviation of 'stdev'        \n",
    "        w = trax.fastmath.random.normal(key=random_key, shape=input_shape) * self._init_stdev\n",
    "        \n",
    "### END CODE HERE ###     \n",
    "        self.weights = w\n",
    "        return self.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "vw-z6n8SAVuy",
    "outputId": "60b4a7ca-f8b6-4dce-9a35-b72eba48db1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights are\n",
      "  [[-0.02837108  0.09368162 -0.10050076  0.14165013  0.10543301  0.09108126\n",
      "  -0.04265672  0.0986188  -0.05575325  0.00153249]\n",
      " [-0.20785688  0.0554837   0.09142365  0.05744595  0.07227863  0.01210617\n",
      "  -0.03237354  0.16234995  0.02450038 -0.13809784]\n",
      " [-0.06111237  0.01403724  0.08410042 -0.1094358  -0.10775021 -0.11396459\n",
      "  -0.05933381 -0.01557652 -0.03832145 -0.11144515]]\n",
      "Foward function output is  [[-3.0395496   0.9266802   2.5414743  -2.050473   -1.9769388  -2.582209\n",
      "  -1.7952735   0.94427425 -0.8980402  -3.7497487 ]]\n"
     ]
    }
   ],
   "source": [
    "# Testing your Dense layer \n",
    "dense_layer = Dense(n_units=10)  #sets  number of units in dense layer\n",
    "random_key = random.get_prng(seed=0)  # sets random seed\n",
    "z = np.array([[2.0, 7.0, 25.0]]) # input array \n",
    "\n",
    "dense_layer.init(z, random_key)\n",
    "print(\"Weights are\\n \",dense_layer.weights) #Returns randomly generated weights\n",
    "print(\"Foward function output is \", dense_layer(z)) # Returns multiplied values of units and weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expected Outout\n",
    "```CPP\n",
    "Weights are\n",
    "  [[-0.02837108  0.09368162 -0.10050076  0.14165013  0.10543301  0.09108126\n",
    "  -0.04265672  0.0986188  -0.05575325  0.00153249]\n",
    " [-0.20785688  0.0554837   0.09142365  0.05744595  0.07227863  0.01210617\n",
    "  -0.03237354  0.16234995  0.02450038 -0.13809784]\n",
    " [-0.06111237  0.01403724  0.08410042 -0.1094358  -0.10775021 -0.11396459\n",
    "  -0.05933381 -0.01557652 -0.03832145 -0.11144515]]\n",
    "Foward function output is  [[-3.0395496   0.9266802   2.5414743  -2.050473   -1.9769388  -2.582209\n",
    "  -1.7952735   0.94427425 -0.8980402  -3.7497487 ]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZEY8vBCgrgy"
   },
   "source": [
    "<a name=\"3.3\"></a>\n",
    "## 3.3  Model\n",
    "\n",
    "Now you will implement a classifier using neural networks. Here is the model architecture you will be implementing. \n",
    "\n",
    "<img src = \"nn.jpg\" style=\"width:400px;height:250px;\"/>\n",
    "\n",
    "For the model implementation, you will use the Trax layers library `tl`.\n",
    "Note that the second character of `tl` is the lowercase of letter `L`, not the number 1. Trax layers are very similar to the ones you implemented above,\n",
    "but in addition to trainable weights also have a non-trainable state.\n",
    "State is used in layers like batch normalization and for inference, you will learn more about it in course 4.\n",
    "\n",
    "First, look at the code of the Trax Dense layer and compare to your implementation above.\n",
    "- [tl.Dense](https://github.com/google/trax/blob/master/trax/layers/core.py#L29): Trax Dense layer implementation\n",
    "\n",
    "One other important layer that you will use a lot is one that allows to execute one layer after another in sequence.\n",
    "- [tl.Serial](https://github.com/google/trax/blob/master/trax/layers/combinators.py#L26): Combinator that applies layers serially.  \n",
    "    - You can pass in the layers as arguments to `Serial`, separated by commas. \n",
    "    - For example: `tl.Serial(tl.Embeddings(...), tl.Mean(...), tl.Dense(...), tl.LogSoftmax(...))`\n",
    "\n",
    "Please use the `help` function to view documentation for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RpbiDzN9jCW2",
    "outputId": "b3d40cc7-c133-404b-db86-fef5fa664a48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Dense in module trax.layers.core:\n",
      "\n",
      "class Dense(trax.layers.base.Layer)\n",
      " |  Dense(n_units, kernel_initializer=<function ScaledInitializer.<locals>.Init at 0x7f1970e28620>, bias_initializer=<function RandomNormalInitializer.<locals>.<lambda> at 0x7f1970e286a8>, use_bias=True)\n",
      " |  \n",
      " |  A dense (a.k.a. fully-connected, affine) layer.\n",
      " |  \n",
      " |  Dense layers are the prototypical example of a trainable layer, i.e., a layer\n",
      " |  with trainable weights. Each node in a dense layer computes a weighted sum of\n",
      " |  all node values from the preceding layer and adds to that sum a node-specific\n",
      " |  bias term. The full layer computation is expressed compactly in linear\n",
      " |  algebra as an affine map `y = Wx + b`, where `W` is a matrix and `y`, `x`,\n",
      " |  and `b` are vectors. The layer is trained, or \"learns\", by updating the\n",
      " |  values in `W` and `b`.\n",
      " |  \n",
      " |  Less commonly, a dense layer can omit the bias term and be a pure linear map:\n",
      " |  `y = Wx`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dense\n",
      " |      trax.layers.base.Layer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_units, kernel_initializer=<function ScaledInitializer.<locals>.Init at 0x7f1970e28620>, bias_initializer=<function RandomNormalInitializer.<locals>.<lambda> at 0x7f1970e286a8>, use_bias=True)\n",
      " |      Returns a dense (fully connected) layer of width `n_units`.\n",
      " |      \n",
      " |      A dense layer maps collections of `R^m` vectors to `R^n`, where `n`\n",
      " |      (`= n_units`) is fixed at layer creation time, and `m` is set at layer\n",
      " |      initialization time.\n",
      " |      \n",
      " |      Args:\n",
      " |        n_units: Number of nodes in the layer, also known as the width of the\n",
      " |            layer.\n",
      " |        kernel_initializer: Function that creates a matrix of (random) initial\n",
      " |            connection weights `W` for the layer.\n",
      " |        bias_initializer: Function that creates a vector of (random) initial\n",
      " |            bias weights `b` for the layer.\n",
      " |        use_bias: If `True`, compute an affine map `y = Wx + b`; else compute\n",
      " |            a linear map `y = Wx`.\n",
      " |  \n",
      " |  forward(self, x)\n",
      " |      Executes this layer as part of a forward pass through the model.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: Tensor of same shape and dtype as the input signature used to\n",
      " |            initialize this layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Tensor of same shape and dtype as the input, except the final dimension\n",
      " |        is the layer's `n_units` value.\n",
      " |  \n",
      " |  init_weights_and_state(self, input_signature)\n",
      " |      Returns newly initialized weights for this layer.\n",
      " |      \n",
      " |      Weights are a `(w, b)` tuple for layers created with `use_bias=True` (the\n",
      " |      default case), or a `w` tensor for layers created with `use_bias=False`.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: `ShapeDtype` instance characterizing the input this layer\n",
      " |            should compute on.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from trax.layers.base.Layer:\n",
      " |  \n",
      " |  __call__(self, x, weights=None, state=None, rng=None)\n",
      " |      Makes layers callable; for use in tests or interactive settings.\n",
      " |      \n",
      " |      This convenience method helps library users play with, test, or otherwise\n",
      " |      probe the behavior of layers outside of a full training environment. It\n",
      " |      presents the layer as callable function from inputs to outputs, with the\n",
      " |      option of manually specifying weights and non-parameter state per individual\n",
      " |      call. For convenience, weights and non-parameter state are cached per layer\n",
      " |      instance, starting from default values of `EMPTY_WEIGHTS` and `EMPTY_STATE`,\n",
      " |      and acquiring non-empty values either by initialization or from values\n",
      " |      explicitly provided via the weights and state keyword arguments.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: Zero or more input tensors, packaged as described in the `Layer` class\n",
      " |            docstring.\n",
      " |        weights: Weights or `None`; if `None`, use self's cached weights value.\n",
      " |        state: State or `None`; if `None`, use self's cached state value.\n",
      " |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n",
      " |            if `None`, use a default computed from an integer 0 seed.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Zero or more output tensors, packaged as described in the `Layer` class\n",
      " |        docstring.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  backward(self, inputs, output, grad, weights, state, new_state, rng)\n",
      " |      Custom backward pass to propagate gradients in a custom way.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensors; can be a (possibly nested) tuple.\n",
      " |        output: The result of running this layer on inputs.\n",
      " |        grad: Gradient signal computed based on subsequent layers; its structure\n",
      " |            and shape must match output.\n",
      " |        weights: This layer's weights.\n",
      " |        state: This layer's state prior to the current forward pass.\n",
      " |        new_state: This layer's state after the current forward pass.\n",
      " |        rng: Single-use random number generator (JAX PRNG key).\n",
      " |      \n",
      " |      Returns:\n",
      " |        The custom gradient signal for the input. Note that we need to return\n",
      " |        a gradient for each argument of forward, so it will usually be a tuple\n",
      " |        of signals: the gradient for inputs and weights.\n",
      " |  \n",
      " |  init(self, input_signature, rng=None, use_cache=False)\n",
      " |      Initializes weights/state of this layer and its sublayers recursively.\n",
      " |      \n",
      " |      Initialization creates layer weights and state, for layers that use them.\n",
      " |      It derives the necessary array shapes and data types from the layer's input\n",
      " |      signature, which is itself just shape and data type information.\n",
      " |      \n",
      " |      For layers without weights or state, this method safely does nothing.\n",
      " |      \n",
      " |      This method is designed to create weights/state only once for each layer\n",
      " |      instance, even if the same layer instance occurs in multiple places in the\n",
      " |      network. This enables weight sharing to be implemented as layer sharing.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: `ShapeDtype` instance (if this layer takes one input)\n",
      " |            or list/tuple of `ShapeDtype` instances.\n",
      " |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n",
      " |            if `None`, use a default computed from an integer 0 seed.\n",
      " |        use_cache: If `True`, and if this layer instance has already been\n",
      " |            initialized elsewhere in the network, then return special marker\n",
      " |            values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`.\n",
      " |            Else return this layer's newly initialized weights and state.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `(weights, state)` tuple.\n",
      " |  \n",
      " |  init_from_file(self, file_name, weights_only=False, input_signature=None)\n",
      " |      Initializes this layer and its sublayers from a pickled checkpoint.\n",
      " |      \n",
      " |      In the common case (`weights_only=False`), the file must be a gziped pickled\n",
      " |      dictionary containing items with keys `'flat_weights', `'flat_state'` and\n",
      " |      `'input_signature'`, which are used to initialize this layer.\n",
      " |      If `input_signature` is specified, it's used instead of the one in the file.\n",
      " |      If `weights_only` is `True`, the dictionary does not need to have the\n",
      " |      `'flat_state'` item and the state it not restored either.\n",
      " |      \n",
      " |      Args:\n",
      " |        file_name: Name/path of the pickeled weights/state file.\n",
      " |        weights_only: If `True`, initialize only the layer's weights. Else\n",
      " |            initialize both weights and state.\n",
      " |        input_signature: Input signature to be used instead of the one from file.\n",
      " |  \n",
      " |  output_signature(self, input_signature)\n",
      " |      Returns output signature this layer would give for `input_signature`.\n",
      " |  \n",
      " |  pure_fn(self, x, weights, state, rng, use_cache=False)\n",
      " |      Applies this layer as a pure function with no optional args.\n",
      " |      \n",
      " |      This method exposes the layer's computation as a pure function. This is\n",
      " |      especially useful for JIT compilation. Do not override, use `forward`\n",
      " |      instead.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: Zero or more input tensors, packaged as described in the `Layer` class\n",
      " |            docstring.\n",
      " |        weights: A tuple or list of trainable weights, with one element for this\n",
      " |            layer if this layer has no sublayers, or one for each sublayer if\n",
      " |            this layer has sublayers. If a layer (or sublayer) has no trainable\n",
      " |            weights, the corresponding weights element is an empty tuple.\n",
      " |        state: Layer-specific non-parameter state that can update between batches.\n",
      " |        rng: Single-use random number generator (JAX PRNG key).\n",
      " |        use_cache: if `True`, cache weights and state in the layer object; used\n",
      " |          to implement layer sharing in combinators.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tuple of `(tensors, state)`. The tensors match the number (`n_out`)\n",
      " |        promised by this layer, and are packaged as described in the `Layer`\n",
      " |        class docstring.\n",
      " |  \n",
      " |  weights_and_state_signature(self, input_signature)\n",
      " |      Return a pair containing the signatures of weights and state.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from trax.layers.base.Layer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  has_backward\n",
      " |      Returns `True` if this layer provides its own custom backward pass code.\n",
      " |      \n",
      " |      A layer subclass that provides custom backward pass code (for custom\n",
      " |      gradients) must override this method to return `True`.\n",
      " |  \n",
      " |  n_in\n",
      " |      Returns how many tensors this layer expects as input.\n",
      " |  \n",
      " |  n_out\n",
      " |      Returns how many tensors this layer promises as output.\n",
      " |  \n",
      " |  name\n",
      " |      Returns the name of this layer.\n",
      " |  \n",
      " |  rng\n",
      " |      Returns a single-use random number generator without advancing it.\n",
      " |  \n",
      " |  state\n",
      " |      Returns a tuple containing this layer's state; may be empty.\n",
      " |  \n",
      " |  sublayers\n",
      " |      Returns a tuple containing this layer's sublayers; may be empty.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns this layer's weights.\n",
      " |      \n",
      " |      Depending on the layer, the weights can be in the form of:\n",
      " |      \n",
      " |        - an empty tuple\n",
      " |        - a tensor (ndarray)\n",
      " |        - a nested structure of tuples and tensors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View documentation on tl.Dense\n",
    "help(tl.Dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Hrblw_uJ4zmF",
    "outputId": "729a5a60-3d38-4457-e2ae-c9abefae7eac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Serial in module trax.layers.combinators:\n",
      "\n",
      "class Serial(trax.layers.base.Layer)\n",
      " |  Serial(*sublayers, name=None, sublayers_to_print=None)\n",
      " |  \n",
      " |  Combinator that applies layers serially (by function composition).\n",
      " |  \n",
      " |  This combinator is commonly used to construct deep networks, e.g., like this::\n",
      " |  \n",
      " |      mlp = tl.Serial(\n",
      " |        tl.Dense(128),\n",
      " |        tl.Relu(),\n",
      " |        tl.Dense(10),\n",
      " |        tl.LogSoftmax()\n",
      " |      )\n",
      " |  \n",
      " |  A Serial combinator uses stack semantics to manage data for its sublayers.\n",
      " |  Each sublayer sees only the inputs it needs and returns only the outputs it\n",
      " |  has generated. The sublayers interact via the data stack. For instance, a\n",
      " |  sublayer k, following sublayer j, gets called with the data stack in the\n",
      " |  state left after layer j has applied. The Serial combinator then:\n",
      " |  \n",
      " |    - takes n_in items off the top of the stack (n_in = k.n_in) and calls\n",
      " |      layer k, passing those items as arguments; and\n",
      " |  \n",
      " |    - takes layer k's n_out return values (n_out = k.n_out) and pushes\n",
      " |      them onto the data stack.\n",
      " |  \n",
      " |  A Serial instance with no sublayers acts as a special-case (but useful)\n",
      " |  1-input 1-output no-op.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Serial\n",
      " |      trax.layers.base.Layer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *sublayers, name=None, sublayers_to_print=None)\n",
      " |      Creates a partially initialized, unconnected layer instance.\n",
      " |      \n",
      " |      Args:\n",
      " |        n_in: Number of inputs expected by this layer.\n",
      " |        n_out: Number of outputs promised by this layer.\n",
      " |        name: Class-like name for this layer; for use when printing this layer.\n",
      " |        sublayers_to_print: Sublayers to display when printing out this layer;\n",
      " |          By default (when None) we display all sublayers.\n",
      " |  \n",
      " |  forward(self, xs)\n",
      " |      Computes this layer's output as part of a forward pass through the model.\n",
      " |      \n",
      " |      Authors of new layer subclasses should override this method to define the\n",
      " |      forward computation that their layer performs. Use `self.weights` to access\n",
      " |      trainable weights of this layer. If you need to use local non-trainable\n",
      " |      state or randomness, use `self.rng` for the random seed (no need to set it)\n",
      " |      and use `self.state` for non-trainable state (and set it to the new value).\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Zero or more input tensors, packaged as described in the `Layer`\n",
      " |            class docstring.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Zero or more output tensors, packaged as described in the `Layer` class\n",
      " |        docstring.\n",
      " |  \n",
      " |  init_weights_and_state(self, input_signature)\n",
      " |      Initializes weights and state for inputs with the given signature.\n",
      " |      \n",
      " |      Authors of new layer subclasses should override this method if their layer\n",
      " |      uses trainable weights or non-trainable state. To initialize trainable\n",
      " |      weights, set `self.weights` and to initialize non-trainable state,\n",
      " |      set `self.state` to the intended value.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: A `ShapeDtype` instance (if this layer takes one input)\n",
      " |            or a list/tuple of `ShapeDtype` instances; signatures of inputs.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  state\n",
      " |      Returns a tuple containing this layer's state; may be empty.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns this layer's weights.\n",
      " |      \n",
      " |      Depending on the layer, the weights can be in the form of:\n",
      " |      \n",
      " |        - an empty tuple\n",
      " |        - a tensor (ndarray)\n",
      " |        - a nested structure of tuples and tensors\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from trax.layers.base.Layer:\n",
      " |  \n",
      " |  __call__(self, x, weights=None, state=None, rng=None)\n",
      " |      Makes layers callable; for use in tests or interactive settings.\n",
      " |      \n",
      " |      This convenience method helps library users play with, test, or otherwise\n",
      " |      probe the behavior of layers outside of a full training environment. It\n",
      " |      presents the layer as callable function from inputs to outputs, with the\n",
      " |      option of manually specifying weights and non-parameter state per individual\n",
      " |      call. For convenience, weights and non-parameter state are cached per layer\n",
      " |      instance, starting from default values of `EMPTY_WEIGHTS` and `EMPTY_STATE`,\n",
      " |      and acquiring non-empty values either by initialization or from values\n",
      " |      explicitly provided via the weights and state keyword arguments.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: Zero or more input tensors, packaged as described in the `Layer` class\n",
      " |            docstring.\n",
      " |        weights: Weights or `None`; if `None`, use self's cached weights value.\n",
      " |        state: State or `None`; if `None`, use self's cached state value.\n",
      " |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n",
      " |            if `None`, use a default computed from an integer 0 seed.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Zero or more output tensors, packaged as described in the `Layer` class\n",
      " |        docstring.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  backward(self, inputs, output, grad, weights, state, new_state, rng)\n",
      " |      Custom backward pass to propagate gradients in a custom way.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensors; can be a (possibly nested) tuple.\n",
      " |        output: The result of running this layer on inputs.\n",
      " |        grad: Gradient signal computed based on subsequent layers; its structure\n",
      " |            and shape must match output.\n",
      " |        weights: This layer's weights.\n",
      " |        state: This layer's state prior to the current forward pass.\n",
      " |        new_state: This layer's state after the current forward pass.\n",
      " |        rng: Single-use random number generator (JAX PRNG key).\n",
      " |      \n",
      " |      Returns:\n",
      " |        The custom gradient signal for the input. Note that we need to return\n",
      " |        a gradient for each argument of forward, so it will usually be a tuple\n",
      " |        of signals: the gradient for inputs and weights.\n",
      " |  \n",
      " |  init(self, input_signature, rng=None, use_cache=False)\n",
      " |      Initializes weights/state of this layer and its sublayers recursively.\n",
      " |      \n",
      " |      Initialization creates layer weights and state, for layers that use them.\n",
      " |      It derives the necessary array shapes and data types from the layer's input\n",
      " |      signature, which is itself just shape and data type information.\n",
      " |      \n",
      " |      For layers without weights or state, this method safely does nothing.\n",
      " |      \n",
      " |      This method is designed to create weights/state only once for each layer\n",
      " |      instance, even if the same layer instance occurs in multiple places in the\n",
      " |      network. This enables weight sharing to be implemented as layer sharing.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: `ShapeDtype` instance (if this layer takes one input)\n",
      " |            or list/tuple of `ShapeDtype` instances.\n",
      " |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n",
      " |            if `None`, use a default computed from an integer 0 seed.\n",
      " |        use_cache: If `True`, and if this layer instance has already been\n",
      " |            initialized elsewhere in the network, then return special marker\n",
      " |            values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`.\n",
      " |            Else return this layer's newly initialized weights and state.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `(weights, state)` tuple.\n",
      " |  \n",
      " |  init_from_file(self, file_name, weights_only=False, input_signature=None)\n",
      " |      Initializes this layer and its sublayers from a pickled checkpoint.\n",
      " |      \n",
      " |      In the common case (`weights_only=False`), the file must be a gziped pickled\n",
      " |      dictionary containing items with keys `'flat_weights', `'flat_state'` and\n",
      " |      `'input_signature'`, which are used to initialize this layer.\n",
      " |      If `input_signature` is specified, it's used instead of the one in the file.\n",
      " |      If `weights_only` is `True`, the dictionary does not need to have the\n",
      " |      `'flat_state'` item and the state it not restored either.\n",
      " |      \n",
      " |      Args:\n",
      " |        file_name: Name/path of the pickeled weights/state file.\n",
      " |        weights_only: If `True`, initialize only the layer's weights. Else\n",
      " |            initialize both weights and state.\n",
      " |        input_signature: Input signature to be used instead of the one from file.\n",
      " |  \n",
      " |  output_signature(self, input_signature)\n",
      " |      Returns output signature this layer would give for `input_signature`.\n",
      " |  \n",
      " |  pure_fn(self, x, weights, state, rng, use_cache=False)\n",
      " |      Applies this layer as a pure function with no optional args.\n",
      " |      \n",
      " |      This method exposes the layer's computation as a pure function. This is\n",
      " |      especially useful for JIT compilation. Do not override, use `forward`\n",
      " |      instead.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: Zero or more input tensors, packaged as described in the `Layer` class\n",
      " |            docstring.\n",
      " |        weights: A tuple or list of trainable weights, with one element for this\n",
      " |            layer if this layer has no sublayers, or one for each sublayer if\n",
      " |            this layer has sublayers. If a layer (or sublayer) has no trainable\n",
      " |            weights, the corresponding weights element is an empty tuple.\n",
      " |        state: Layer-specific non-parameter state that can update between batches.\n",
      " |        rng: Single-use random number generator (JAX PRNG key).\n",
      " |        use_cache: if `True`, cache weights and state in the layer object; used\n",
      " |          to implement layer sharing in combinators.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tuple of `(tensors, state)`. The tensors match the number (`n_out`)\n",
      " |        promised by this layer, and are packaged as described in the `Layer`\n",
      " |        class docstring.\n",
      " |  \n",
      " |  weights_and_state_signature(self, input_signature)\n",
      " |      Return a pair containing the signatures of weights and state.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from trax.layers.base.Layer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  has_backward\n",
      " |      Returns `True` if this layer provides its own custom backward pass code.\n",
      " |      \n",
      " |      A layer subclass that provides custom backward pass code (for custom\n",
      " |      gradients) must override this method to return `True`.\n",
      " |  \n",
      " |  n_in\n",
      " |      Returns how many tensors this layer expects as input.\n",
      " |  \n",
      " |  n_out\n",
      " |      Returns how many tensors this layer promises as output.\n",
      " |  \n",
      " |  name\n",
      " |      Returns the name of this layer.\n",
      " |  \n",
      " |  rng\n",
      " |      Returns a single-use random number generator without advancing it.\n",
      " |  \n",
      " |  sublayers\n",
      " |      Returns a tuple containing this layer's sublayers; may be empty.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View documentation on tl.Serial\n",
    "help(tl.Serial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n6PptsvwjCW3"
   },
   "source": [
    "- [tl.Embedding](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L113): Layer constructor function for an embedding layer.  \n",
    "    - `tl.Embedding(vocab_size, d_feature)`.\n",
    "    - `vocab_size` is the number of unique words in the given vocabulary.\n",
    "    - `d_feature` is the number of elements in the word embedding (some choices for a word embedding size range from 150 to 300, for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Y5FAphBWjCW4",
    "outputId": "de4678bf-0bcd-456b-988d-66497fb12929"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Embedding in module trax.layers.core:\n",
      "\n",
      "class Embedding(trax.layers.base.Layer)\n",
      " |  Embedding(vocab_size, d_feature, kernel_initializer=<function RandomNormalInitializer.<locals>.<lambda> at 0x7f1970e288c8>)\n",
      " |  \n",
      " |  Trainable layer that maps discrete tokens/ids to vectors.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Embedding\n",
      " |      trax.layers.base.Layer\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, vocab_size, d_feature, kernel_initializer=<function RandomNormalInitializer.<locals>.<lambda> at 0x7f1970e288c8>)\n",
      " |      Returns an embedding layer with given vocabulary size and vector size.\n",
      " |      \n",
      " |      The layer clips input values (token ids) to the range `[0, vocab_size)`.\n",
      " |      That is, negative token ids all clip to `0` before being mapped to a\n",
      " |      vector, and token ids with value `vocab_size` or greater all clip to\n",
      " |      `vocab_size - 1` before being mapped to a vector.\n",
      " |      \n",
      " |      Args:\n",
      " |        vocab_size: Size of the input vocabulary. The layer will assign a unique\n",
      " |            vector to each id in `range(vocab_size)`.\n",
      " |        d_feature: Dimensionality/depth of the output vectors.\n",
      " |        kernel_initializer: Function that creates (random) initial vectors for\n",
      " |            the embedding.\n",
      " |  \n",
      " |  forward(self, x)\n",
      " |      Returns embedding vectors corresponding to input token id's.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: Tensor of token id's.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Tensor of embedding vectors.\n",
      " |  \n",
      " |  init_weights_and_state(self, input_signature)\n",
      " |      Returns tensor of newly initialized embedding vectors.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from trax.layers.base.Layer:\n",
      " |  \n",
      " |  __call__(self, x, weights=None, state=None, rng=None)\n",
      " |      Makes layers callable; for use in tests or interactive settings.\n",
      " |      \n",
      " |      This convenience method helps library users play with, test, or otherwise\n",
      " |      probe the behavior of layers outside of a full training environment. It\n",
      " |      presents the layer as callable function from inputs to outputs, with the\n",
      " |      option of manually specifying weights and non-parameter state per individual\n",
      " |      call. For convenience, weights and non-parameter state are cached per layer\n",
      " |      instance, starting from default values of `EMPTY_WEIGHTS` and `EMPTY_STATE`,\n",
      " |      and acquiring non-empty values either by initialization or from values\n",
      " |      explicitly provided via the weights and state keyword arguments.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: Zero or more input tensors, packaged as described in the `Layer` class\n",
      " |            docstring.\n",
      " |        weights: Weights or `None`; if `None`, use self's cached weights value.\n",
      " |        state: State or `None`; if `None`, use self's cached state value.\n",
      " |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n",
      " |            if `None`, use a default computed from an integer 0 seed.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Zero or more output tensors, packaged as described in the `Layer` class\n",
      " |        docstring.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  backward(self, inputs, output, grad, weights, state, new_state, rng)\n",
      " |      Custom backward pass to propagate gradients in a custom way.\n",
      " |      \n",
      " |      Args:\n",
      " |        inputs: Input tensors; can be a (possibly nested) tuple.\n",
      " |        output: The result of running this layer on inputs.\n",
      " |        grad: Gradient signal computed based on subsequent layers; its structure\n",
      " |            and shape must match output.\n",
      " |        weights: This layer's weights.\n",
      " |        state: This layer's state prior to the current forward pass.\n",
      " |        new_state: This layer's state after the current forward pass.\n",
      " |        rng: Single-use random number generator (JAX PRNG key).\n",
      " |      \n",
      " |      Returns:\n",
      " |        The custom gradient signal for the input. Note that we need to return\n",
      " |        a gradient for each argument of forward, so it will usually be a tuple\n",
      " |        of signals: the gradient for inputs and weights.\n",
      " |  \n",
      " |  init(self, input_signature, rng=None, use_cache=False)\n",
      " |      Initializes weights/state of this layer and its sublayers recursively.\n",
      " |      \n",
      " |      Initialization creates layer weights and state, for layers that use them.\n",
      " |      It derives the necessary array shapes and data types from the layer's input\n",
      " |      signature, which is itself just shape and data type information.\n",
      " |      \n",
      " |      For layers without weights or state, this method safely does nothing.\n",
      " |      \n",
      " |      This method is designed to create weights/state only once for each layer\n",
      " |      instance, even if the same layer instance occurs in multiple places in the\n",
      " |      network. This enables weight sharing to be implemented as layer sharing.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: `ShapeDtype` instance (if this layer takes one input)\n",
      " |            or list/tuple of `ShapeDtype` instances.\n",
      " |        rng: Single-use random number generator (JAX PRNG key), or `None`;\n",
      " |            if `None`, use a default computed from an integer 0 seed.\n",
      " |        use_cache: If `True`, and if this layer instance has already been\n",
      " |            initialized elsewhere in the network, then return special marker\n",
      " |            values -- tuple `(GET_WEIGHTS_FROM_CACHE, GET_STATE_FROM_CACHE)`.\n",
      " |            Else return this layer's newly initialized weights and state.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `(weights, state)` tuple.\n",
      " |  \n",
      " |  init_from_file(self, file_name, weights_only=False, input_signature=None)\n",
      " |      Initializes this layer and its sublayers from a pickled checkpoint.\n",
      " |      \n",
      " |      In the common case (`weights_only=False`), the file must be a gziped pickled\n",
      " |      dictionary containing items with keys `'flat_weights', `'flat_state'` and\n",
      " |      `'input_signature'`, which are used to initialize this layer.\n",
      " |      If `input_signature` is specified, it's used instead of the one in the file.\n",
      " |      If `weights_only` is `True`, the dictionary does not need to have the\n",
      " |      `'flat_state'` item and the state it not restored either.\n",
      " |      \n",
      " |      Args:\n",
      " |        file_name: Name/path of the pickeled weights/state file.\n",
      " |        weights_only: If `True`, initialize only the layer's weights. Else\n",
      " |            initialize both weights and state.\n",
      " |        input_signature: Input signature to be used instead of the one from file.\n",
      " |  \n",
      " |  output_signature(self, input_signature)\n",
      " |      Returns output signature this layer would give for `input_signature`.\n",
      " |  \n",
      " |  pure_fn(self, x, weights, state, rng, use_cache=False)\n",
      " |      Applies this layer as a pure function with no optional args.\n",
      " |      \n",
      " |      This method exposes the layer's computation as a pure function. This is\n",
      " |      especially useful for JIT compilation. Do not override, use `forward`\n",
      " |      instead.\n",
      " |      \n",
      " |      Args:\n",
      " |        x: Zero or more input tensors, packaged as described in the `Layer` class\n",
      " |            docstring.\n",
      " |        weights: A tuple or list of trainable weights, with one element for this\n",
      " |            layer if this layer has no sublayers, or one for each sublayer if\n",
      " |            this layer has sublayers. If a layer (or sublayer) has no trainable\n",
      " |            weights, the corresponding weights element is an empty tuple.\n",
      " |        state: Layer-specific non-parameter state that can update between batches.\n",
      " |        rng: Single-use random number generator (JAX PRNG key).\n",
      " |        use_cache: if `True`, cache weights and state in the layer object; used\n",
      " |          to implement layer sharing in combinators.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A tuple of `(tensors, state)`. The tensors match the number (`n_out`)\n",
      " |        promised by this layer, and are packaged as described in the `Layer`\n",
      " |        class docstring.\n",
      " |  \n",
      " |  weights_and_state_signature(self, input_signature)\n",
      " |      Return a pair containing the signatures of weights and state.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from trax.layers.base.Layer:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  has_backward\n",
      " |      Returns `True` if this layer provides its own custom backward pass code.\n",
      " |      \n",
      " |      A layer subclass that provides custom backward pass code (for custom\n",
      " |      gradients) must override this method to return `True`.\n",
      " |  \n",
      " |  n_in\n",
      " |      Returns how many tensors this layer expects as input.\n",
      " |  \n",
      " |  n_out\n",
      " |      Returns how many tensors this layer promises as output.\n",
      " |  \n",
      " |  name\n",
      " |      Returns the name of this layer.\n",
      " |  \n",
      " |  rng\n",
      " |      Returns a single-use random number generator without advancing it.\n",
      " |  \n",
      " |  state\n",
      " |      Returns a tuple containing this layer's state; may be empty.\n",
      " |  \n",
      " |  sublayers\n",
      " |      Returns a tuple containing this layer's sublayers; may be empty.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns this layer's weights.\n",
      " |      \n",
      " |      Depending on the layer, the weights can be in the form of:\n",
      " |      \n",
      " |        - an empty tuple\n",
      " |        - a tensor (ndarray)\n",
      " |        - a nested structure of tuples and tensors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View documentation for tl.Embedding\n",
    "help(tl.Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Bi4OhkZbjCW6",
    "outputId": "61a46a9c-ef12-42ec-99e7-1ec888937c9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding_3_2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp_embed = tl.Embedding(vocab_size=3, d_feature=2)\n",
    "display(tmp_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OD0XVH5jjCW8"
   },
   "source": [
    "- [tl.Mean](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L276): Calculates means across an axis.  In this case, please choose axis = 1 to get an average embedding vector (an embedding vector that is an average of all words in the vocabulary).  \n",
    "- For example, if the embedding matrix is 300 elements and vocab size is 10,000 words, taking the mean of the embedding matrix along axis=1 will yield a vector of 300 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "CO0uMOOmjCW8",
    "outputId": "066c2690-e572-4e0b-8a98-ca6873f8918f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function Mean in module trax.layers.core:\n",
      "\n",
      "Mean(axis=-1, keepdims=False)\n",
      "    Returns a layer that computes mean values using one tensor axis.\n",
      "    \n",
      "    `Mean` uses one tensor axis to form groups of values and replaces each group\n",
      "    with the mean value of that group. The resulting values can either remain\n",
      "    in their own size 1 axis (`keepdims=True`), or that axis can be removed from\n",
      "    the overall tensor (default `keepdims=False`), lowering the rank of the\n",
      "    tensor by one.\n",
      "    \n",
      "    Args:\n",
      "      axis: Axis along which values are grouped for computing a mean.\n",
      "      keepdims: If `True`, keep the resulting size 1 axis as a separate tensor\n",
      "          axis; else, remove that axis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the documentation for tl.mean\n",
    "help(tl.Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "eSS-_d38jCW-",
    "outputId": "eb19ac1d-7f11-4e5c-e7b4-97544a34f7d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean along axis 0 creates a vector whose length equals the vocabulary size\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([2.5, 3.5, 4.5], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean along axis 1 creates a vector whose length equals the number of elements in a word embedding\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([2., 5.], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pretend the embedding matrix uses \n",
    "# 2 elements for embedding the meaning of a word\n",
    "# and has a vocabulary size of 3\n",
    "# So it has shape (2,3)\n",
    "tmp_embed = np.array([[1,2,3,],\n",
    "                    [4,5,6]\n",
    "                   ])\n",
    "\n",
    "# take the mean along axis 0\n",
    "print(\"The mean along axis 0 creates a vector whose length equals the vocabulary size\")\n",
    "display(np.mean(tmp_embed,axis=0))\n",
    "\n",
    "print(\"The mean along axis 1 creates a vector whose length equals the number of elements in a word embedding\")\n",
    "display(np.mean(tmp_embed,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "08G5yUa_jCXE"
   },
   "source": [
    "- [tl.LogSoftmax](https://github.com/google/trax/blob/1372b903bb66b0daccee19fd0b1fdf44f659330b/trax/layers/core.py#L242): Implements log softmax function\n",
    "- Here, you don't need to set any parameters for `LogSoftMax()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "0UsQjFrAjCXF",
    "outputId": "4ce93870-33b6-47a3-97d4-4e01564e3893"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function LogSoftmax in module trax.layers.core:\n",
      "\n",
      "LogSoftmax(axis=-1)\n",
      "    Returns a layer that applies log softmax along one tensor axis.\n",
      "    \n",
      "    `LogSoftmax` acts on a group of values and normalizes them to look like a set\n",
      "    of log probability values. (Probability values must be non-negative, and as\n",
      "    a set must sum to 1. A group of log probability values can be seen as the\n",
      "    natural logarithm function applied to a set of probability values.)\n",
      "    \n",
      "    Args:\n",
      "      axis: Axis along which values are grouped for computing log softmax.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tl.LogSoftmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Online documentation**\n",
    "\n",
    "- [tl.Dense](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Dense)\n",
    "\n",
    "- [tl.Serial](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#module-trax.layers.combinators)\n",
    "\n",
    "- [tl.Embedding](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Embedding)\n",
    "\n",
    "- [tl.Mean](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.Mean)\n",
    "\n",
    "- [tl.LogSoftmax](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.core.LogSoftmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W8ONXnJsjCXH"
   },
   "source": [
    "<a name=\"ex05\"></a>\n",
    "### Exercise 05\n",
    "Implement the classifier function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wh33Hk8lgrgz"
   },
   "outputs": [],
   "source": [
    "# UNQ_C5 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: classifier\n",
    "def classifier(vocab_size=len(Vocab), embedding_dim=256, output_dim=2, mode='train'):\n",
    "        \n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    # create embedding layer\n",
    "    embed_layer = tl.Embedding(\n",
    "        vocab_size=vocab_size, # Size of the vocabulary\n",
    "        d_feature=embedding_dim)  # Embedding dimension\n",
    "    \n",
    "    # Create a mean layer, to create an \"average\" word embedding\n",
    "    mean_layer = tl.Mean(axis=1)\n",
    "    \n",
    "    # Create a dense layer, one unit for each output\n",
    "    dense_output_layer = tl.Dense(n_units = output_dim)\n",
    "\n",
    "    \n",
    "    # Create the log softmax layer (no parameters needed)\n",
    "    log_softmax_layer = tl.LogSoftmax()\n",
    "    \n",
    "    # Use tl.Serial to combine all layers\n",
    "    # and create the classifier\n",
    "    # of type trax.layers.combinators.Serial\n",
    "    model = tl.Serial(\n",
    "      embed_layer, # embedding layer\n",
    "      mean_layer, # mean layer\n",
    "      dense_output_layer, # dense output layer \n",
    "      log_softmax_layer # log softmax layer\n",
    "    )\n",
    "### END CODE HERE ###     \n",
    "    \n",
    "    # return the model of type\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OwJCu3e9jCXK"
   },
   "outputs": [],
   "source": [
    "tmp_model = classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "ZsMzvK8YjCXM",
    "outputId": "dbc365af-2a5a-4423-98f1-371d2ee6bba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'trax.layers.combinators.Serial'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Serial[\n",
       "  Embedding_9088_256\n",
       "  Mean\n",
       "  Dense_2\n",
       "  LogSoftmax\n",
       "]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(type(tmp_model))\n",
    "display(tmp_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DV0LEuRVjCXO"
   },
   "source": [
    "##### Expected Outout\n",
    "```CPP\n",
    "<class 'trax.layers.combinators.Serial'>\n",
    "Serial[\n",
    "  Embedding_9088_256\n",
    "  Mean\n",
    "  Dense_2\n",
    "  LogSoftmax\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1FaugA_7grg6"
   },
   "source": [
    "<a name=\"4\"></a>\n",
    "# Part 4:  Training\n",
    "\n",
    "To train a model on a task, Trax defines an abstraction [`trax.supervised.training.TrainTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) which packages the train data, loss and optimizer (among other things) together into an object.\n",
    "\n",
    "Similarly to evaluate a model, Trax defines an abstraction [`trax.supervised.training.EvalTask`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) which packages the eval data and metrics (among other things) into another object.\n",
    "\n",
    "The final piece tying things together is the [`trax.supervised.training.Loop`](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) abstraction that is a very simple and flexible way to put everything together and train the model, all the while evaluating it and saving checkpoints.\n",
    "Using `Loop` will save you a lot of code compared to always writing the training loop by hand, like you did in courses 1 and 2. More importantly, you are less likely to have a bug in that code that would ruin your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "UGgKw03jjCXP",
    "outputId": "014a4326-53ac-4408-878f-0202f6c4828e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TrainTask in module trax.supervised.training:\n",
      "\n",
      "class TrainTask(builtins.object)\n",
      " |  TrainTask(labeled_data, loss_layer, optimizer, lr_schedule=None, n_steps_per_checkpoint=100)\n",
      " |  \n",
      " |  A supervised task (labeled data + feedback mechanism) for training.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, labeled_data, loss_layer, optimizer, lr_schedule=None, n_steps_per_checkpoint=100)\n",
      " |      Configures a training task.\n",
      " |      \n",
      " |      Args:\n",
      " |        labeled_data: Iterator of batches of labeled data tuples. Each tuple has\n",
      " |            1+ data (input value) tensors followed by 1 label (target value)\n",
      " |            tensor.  All tensors are NumPy ndarrays or their JAX counterparts.\n",
      " |        loss_layer: Layer that computes a scalar value (the \"loss\") by comparing\n",
      " |            model output :math:`\\hat{y}=f(x)` to the target :math:`y`.\n",
      " |        optimizer: Optimizer object that computes model weight updates from\n",
      " |            loss-function gradients.\n",
      " |        lr_schedule: Learning rate schedule, a function step -> learning_rate.\n",
      " |        n_steps_per_checkpoint: How many steps to run between checkpoints.\n",
      " |  \n",
      " |  learning_rate(self, step)\n",
      " |      Return the learning rate for the given step.\n",
      " |  \n",
      " |  next_batch(self)\n",
      " |      Returns one batch of labeled data: a tuple of input(s) plus label.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  labeled_data\n",
      " |  \n",
      " |  loss_layer\n",
      " |  \n",
      " |  n_steps_per_checkpoint\n",
      " |  \n",
      " |  optimizer\n",
      " |  \n",
      " |  sample_batch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# View documentation for trax.supervised.training.TrainTask\n",
    "help(trax.supervised.training.TrainTask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "id": "Tr2MmdWDn6hV",
    "outputId": "daec4adb-694d-407e-f1cc-8eb26628ed05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class EvalTask in module trax.supervised.training:\n",
      "\n",
      "class EvalTask(builtins.object)\n",
      " |  EvalTask(labeled_data, metrics, metric_names=None, n_eval_batches=1)\n",
      " |  \n",
      " |  Labeled data plus scalar functions for (periodically) measuring a model.\n",
      " |  \n",
      " |  An eval task specifies how (`labeled_data` + `metrics`) and with what\n",
      " |  precision (`n_eval_batches`) to measure a model as it is training.\n",
      " |  The variance of each scalar output is reduced by measuring over multiple\n",
      " |  (`n_eval_batches`) batches and reporting the average from those measurements.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, labeled_data, metrics, metric_names=None, n_eval_batches=1)\n",
      " |      Configures an eval task: named metrics run with a given data source.\n",
      " |      \n",
      " |      Args:\n",
      " |        labeled_data: Iterator of batches of labeled data tuples. Each tuple has\n",
      " |            1+ data tensors (NumPy ndarrays) followed by 1 label (target value)\n",
      " |            tensor.\n",
      " |        metrics: List of layers; each computes a scalar value per batch by\n",
      " |            comparing model output :math:`\\hat{y}=f(x)` to the target :math:`y`.\n",
      " |        metric_names: List of names, one for each item in `metrics`, in matching\n",
      " |             order, to be used when recording/reporting eval output. If None,\n",
      " |             generate default names using layer names from metrics.\n",
      " |        n_eval_batches: Integer N that specifies how many eval batches to run;\n",
      " |            the output is then the average of the outputs from the N batches.\n",
      " |  \n",
      " |  next_batch(self)\n",
      " |      Returns one batch of labeled data: a tuple of input(s) plus label.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  labeled_data\n",
      " |  \n",
      " |  metric_names\n",
      " |  \n",
      " |  metrics\n",
      " |  \n",
      " |  n_eval_batches\n",
      " |  \n",
      " |  sample_batch\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View documentation for trax.supervised.training.EvalTask\n",
    "help(trax.supervised.training.EvalTask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XkUVMzVXn_8f",
    "outputId": "b5bdbd12-ec1c-4a4c-99ac-a122e5534434"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Loop in module trax.supervised.training:\n",
      "\n",
      "class Loop(builtins.object)\n",
      " |  Loop(model, task, eval_model=None, eval_task=None, output_dir=None, checkpoint_at=None, eval_at=None)\n",
      " |  \n",
      " |  Loop that can run for a given number of steps to train a supervised model.\n",
      " |  \n",
      " |  The typical supervised training process randomly initializes a model and\n",
      " |  updates its weights via feedback (loss-derived gradients) from a training\n",
      " |  task, by looping through batches of labeled data. A training loop can also\n",
      " |  be configured to run periodic evals and save intermediate checkpoints.\n",
      " |  \n",
      " |  For speed, the implementation takes advantage of JAX's composable function\n",
      " |  transformations (specifically, `jit` and `grad`). It creates JIT-compiled\n",
      " |  pure functions derived from variants of the core model; schematically:\n",
      " |  \n",
      " |    - training variant: jit(grad(pure_function(model+loss)))\n",
      " |    - evals variant: jit(pure_function(model+evals))\n",
      " |  \n",
      " |  In training or during evals, these variants are called with explicit\n",
      " |  arguments for all relevant input data, model weights/state, optimizer slots,\n",
      " |  and random number seeds:\n",
      " |  \n",
      " |    - batch: labeled data\n",
      " |    - model weights/state: trainable weights and input-related state (e.g., as\n",
      " |      used by batch norm)\n",
      " |    - optimizer slots: weights in the optimizer that evolve during the training\n",
      " |      process\n",
      " |    - random number seeds: JAX PRNG keys that enable high-quality, distributed,\n",
      " |      repeatable generation of pseudo-random numbers\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model, task, eval_model=None, eval_task=None, output_dir=None, checkpoint_at=None, eval_at=None)\n",
      " |      Configures a training `Loop`, including a random initialization.\n",
      " |      \n",
      " |      Args:\n",
      " |        model: Trax layer, representing the core model to be trained. Loss\n",
      " |            functions and eval functions (a.k.a. metrics) are considered to be\n",
      " |            outside the core model, taking core model output and data labels as\n",
      " |            their two inputs.\n",
      " |        task: TrainTask instance, which defines the training data, loss function,\n",
      " |            and optimizer to be used in this training loop.\n",
      " |        eval_model: Optional Trax layer, representing model used for evaluation,\n",
      " |          e.g., with dropout turned off. If None, the training model (model)\n",
      " |          will be used.\n",
      " |        eval_task: EvalTask instance or None. If None, don't do any evals.\n",
      " |        output_dir: Path telling where to save outputs (evals and checkpoints).\n",
      " |            Can be None if both `eval_task` and `checkpoint_at` are None.\n",
      " |        checkpoint_at: Function (integer --> boolean) telling, for step n, whether\n",
      " |            that step should have its checkpoint saved. If None, the default is\n",
      " |            periodic checkpointing at `task.n_steps_per_checkpoint`.\n",
      " |        eval_at: Function (integer --> boolean) that says, for training step n,\n",
      " |            whether that step should run evals. If None, run when checkpointing.\n",
      " |  \n",
      " |  new_rng(self)\n",
      " |      Returns a new single-use random number generator (JAX PRNG key).\n",
      " |  \n",
      " |  run(self, n_steps=1)\n",
      " |      Runs this training loop for n steps.\n",
      " |      \n",
      " |      Optionally runs evals and saves checkpoints at specified points.\n",
      " |      \n",
      " |      Args:\n",
      " |        n_steps: Stop training after completing n steps.\n",
      " |  \n",
      " |  run_evals(self, weights=None, state=None)\n",
      " |      Runs and records evals for this training session.\n",
      " |      \n",
      " |      Args:\n",
      " |        weights: Current weights from model in training.\n",
      " |        state: Current state from model in training.\n",
      " |  \n",
      " |  save_checkpoint(self, weights=None, state=None, slots=None)\n",
      " |      Saves checkpoint to disk for the current training step.\n",
      " |      \n",
      " |      Args:\n",
      " |        weights: Weights from model being trained.\n",
      " |        state: State (non-weight parameters) from model being trained.\n",
      " |        slots: Updatable weights for the optimizer in this training loop.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  current_step\n",
      " |      Returns current step number in this training session.\n",
      " |  \n",
      " |  eval_model\n",
      " |      Returns the model used for evaluation.\n",
      " |  \n",
      " |  model\n",
      " |      Returns the model that is training.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View documentation for trax.supervised.training.Loop\n",
    "help(trax.supervised.training.Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "Ooekq1F305bt",
    "outputId": "9f60f810-9d6a-47b6-e977-b6a3c478e9ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package trax.optimizers in trax:\n",
      "\n",
      "NAME\n",
      "    trax.optimizers - Optimizers for use with Trax layers.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    adafactor\n",
      "    adam\n",
      "    base\n",
      "    momentum\n",
      "    optimizers_test\n",
      "    rms_prop\n",
      "    sm3\n",
      "\n",
      "FUNCTIONS\n",
      "    opt_configure(*args, **kwargs)\n",
      "\n",
      "FILE\n",
      "    /opt/conda/lib/python3.7/site-packages/trax/optimizers/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View optimizers that you could choose from\n",
    "help(trax.optimizers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OmR3BhV41Cxs"
   },
   "source": [
    "Notice some available optimizers include:\n",
    "```CPP\n",
    "    adafactor\n",
    "    adam\n",
    "    momentum\n",
    "    rms_prop\n",
    "    sm3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HA01H6K7grg_"
   },
   "source": [
    "<a name=\"4.1\"></a>\n",
    "## 4.1  Training the model\n",
    "\n",
    "Now you are going to train your model. \n",
    "\n",
    "Let's define the `TrainTask`, `EvalTask` and `Loop` in preparation to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ogMtJgHSoiZj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words from the processed tweet:\n",
      "['reserv', 'password', 'okay']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'see', 'later', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lay', 'greet', 'card', 'rang', 'print', 'today', 'love', 'job', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lol', 'woman', 'want', 'share', 'greek', 'god', 'bodi', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hello', ':)', 'get', 'youth', 'job', 'opportun', 'follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thought', 'mubank', '..', 'lol', \"dongwoo'\", 'unexpect', 'tweet', '..', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['last', 'day', 'work', 'time', 'real', ':D', 'summer', 'job', 'school', 'start', 'three', 'week', ':/', 'mysumm', 'happi']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['look', 'fun', 'snapchat', 'jennyjean', '22', 'snapchat', 'hornykik', 'eboni', 'trade', 'model', 'elfindelmundo', 'webcamsex', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['im', 'forc', 'go', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tomorrow', 'pleas', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['disgust', ':(', 'wish', 'peopl', 'respect']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sooo', 'tire', \"can't\", 'sleep', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pop', 'like', 'helium', 'balloon', '..', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['citi', 'shit', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oop', '...', 'server', 'error', 'occur', 'email', 'sent', ':(', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['remedi', 'media', 'tell', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['know', ':)', 'hot', 'alway']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['becom', 'better', 'atrack', 'better', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nice', 'one', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['aha', \"that'\", 'alright', '‚ù§', 'Ô∏è', 'love', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['know', 'averag', 'take', '6', 'second', 'read', 'god', 'dam', 'mother', 'fuck', 'tweet', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fantast', 'thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['kik', 'qualki', '808', 'kik', 'kikmenow', 'milf', 'like', '4like', 'bore', 'summer', 'sexysaturday', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['think', 'she', 'busi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ur', 'cat', 'super', 'nice', 'cuddli', 'suddenli', 'scratch', 'tri', 'bite', ':(', 'like', 'trust', 'anymor']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['although', 'mum', 'ask', 'watch', 'w', '‚Ä¶', 'stilll', ':(', 'read', 'yet', \"she'd\", 'ask', 'question', 'middl', 'movi']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['imagin', 'would', 'shatter', 'dream', ':-(', \"we'll\", 'let', 'colleagu', 'know', 'sb']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['peng', 'bestfriend', ':(', 'psygustokita']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bad', \"i'm\", 'poor', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['man', 'right', 'wish', 'someon', 'would', 'bring', 'food', 'offic', \"i'm\", 'starv', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['zayn_come_back_we_miss_y', '<3', '<3', ':(', 'much']\n",
      "The unique integer ID for the unk_token is 2\n"
     ]
    }
   ],
   "source": [
    "from trax.supervised import training\n",
    "\n",
    "batch_size = 16\n",
    "rnd.seed(271)\n",
    "\n",
    "train_task = training.TrainTask(\n",
    "    labeled_data=train_generator(batch_size=batch_size, shuffle=True),\n",
    "    loss_layer=tl.CrossEntropyLoss(),\n",
    "    optimizer=trax.optimizers.Adam(0.01),\n",
    "    n_steps_per_checkpoint=10,\n",
    ")\n",
    "\n",
    "eval_task = training.EvalTask(\n",
    "    labeled_data=val_generator(batch_size=batch_size, shuffle=True),\n",
    "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
    ")\n",
    "\n",
    "model = classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_sw8EGd0Sjk"
   },
   "source": [
    "This defines a model trained using [`tl.CrossEntropyLoss`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.CrossEntropyLoss) optimized with the [`trax.optimizers.Adam`](https://trax-ml.readthedocs.io/en/latest/trax.optimizers.html#trax.optimizers.adam.Adam) optimizer, all the while tracking the accuracy using [`tl.Accuracy`](https://trax-ml.readthedocs.io/en/latest/trax.layers.html#trax.layers.metrics.Accuracy) metric. We also track `tl.CrossEntropyLoss` on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yB78IIUerIVG"
   },
   "source": [
    "Now let's make an output directory and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "CNx4LnP9rMsO",
    "outputId": "359fab84-7b89-4eea-b64e-5c681f6952c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/model/\n"
     ]
    }
   ],
   "source": [
    "output_dir = '~/model/'\n",
    "output_dir_expand = os.path.expanduser(output_dir)\n",
    "print(output_dir_expand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e4R4EHUcrwqe"
   },
   "source": [
    "<a name=\"ex06\"></a>\n",
    "### Exercise 06\n",
    "**Instructions:** Implement `train_model` to train the model (`classifier` that you wrote earlier) for the given number of training steps (`n_steps`) using `TrainTask`, `EvalTask` and `Loop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tolygrj7rpFX"
   },
   "outputs": [],
   "source": [
    "# UNQ_C6 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: train_model\n",
    "def train_model(classifier, train_task, eval_task, n_steps, output_dir):\n",
    "    '''\n",
    "    Input: \n",
    "        classifier - the model you are building\n",
    "        train_task - Training task\n",
    "        eval_task - Evaluation task\n",
    "        n_steps - the evaluation steps\n",
    "        output_dir - folder to save your files\n",
    "    Output:\n",
    "        trainer -  trax trainer\n",
    "    '''\n",
    "### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    training_loop = training.Loop(\n",
    "                                classifier, # The learning model\n",
    "                                train_task, # The training task\n",
    "                                eval_task = eval_task, # The evaluation task\n",
    "                                output_dir = output_dir) # The output directory\n",
    "\n",
    "    training_loop.run(n_steps = n_steps)\n",
    "### END CODE HERE ###\n",
    "\n",
    "    # Return the training_loop, since it has the model.\n",
    "    return training_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "colab_type": "code",
    "id": "d-AtiqAYs_rH",
    "outputId": "32fd06b6-9d04-4391-fe3a-689a28734b72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words from the processed tweet:\n",
      "['mum', 'beg', 'watch', 'soprano', \"i'll\", 'watch', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happi', 'friday', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['guy', 'great', 'last', 'night', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['done', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['friend', 'make', 'fun', 'fact', 'mexican', 'hurt', 'feel', 'leav', 'scar', 'life', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ok', 'nice', ':)', 'btw', '0.7', '1', 'come', 'today', '..', 'tri', 'fix', 'packag', 'depend']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['kinda', 'wanna', 'fangirl', 'apolog', 'even', 'tho', 'alreadi', 'fangirl', 'lot', 'first', 'heard', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['even', 'think', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['never', 'drink', 'gin', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['reunit', 'one', 'day', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['haha', 'japan', ':(', 'sorri', 'love']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['realli', 'one', \"who'\", 'annoy', 'youtub', 'got', 'rid', 'music', 'tab', 'love', 'tab', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', 'much', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wait', 'fuck', 'ffvi', '>:(', 'ugh']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sob', \"that'\", 'realli', 'sweet', '__', \"i'll\", 'busier', 'next', 'week', 'onward', 'idk', \"i'll\", 'find', 'time', 'ol', 'anymor', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hp', 'low', 'dong', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step      1: train CrossEntropyLoss |  0.88939196\n",
      "List of words from the processed tweet:\n",
      "['hi', \"here'\", 'vid', 'stydia', 'take', 'look', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['congrat', '100k', 'man', \"can't\", 'wait', 'till', 'hit', 'mileston', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['found', 'song', 'mention', 'nylon', 'gvb', 'cd', 'take', 'mountain.titl', 'unto', 'other', 'theworldwouldchang', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['daddi', 'af', '...', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yvw', 'great', 'day', 'weekend', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['glyon', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ye', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ye', 'alway', 'selfish', 'fuck', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['way', 'get', 'bandana', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['motiv', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oh', 'ndabenhl', ':-(', 'pleas', 'let', 'us', 'know', 'feel', 'way', 'sa']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['need', 'boyfriend', 'rn', 'answer', 'calll', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wish', 'could', 'go', 'shop', 'whole', 'weekend', 'taken', ':(', 'annoy']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['layout', 'match', 'closest', 'could', 'find', 'header', ':(', 'someon', 'help']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'world', 'aw']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['kik', 'twer', '782', 'kik', 'kikhorni', 'lesbian', 'hornykik', 'girl', 'countrymus', 'hornykik', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step      1: eval  CrossEntropyLoss |  0.68833977\n",
      "Step      1: eval          Accuracy |  0.50000000\n",
      "List of words from the processed tweet:\n",
      "['realli', \"that'\", 'great', 'cool', 'bit', 'az', 'might', 'give', 'cauliflow', 'one', 'attempt', 'thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['trust', 'favourit', 'airlin', 'deliv', 'alway', ':)', ':)', 'good', 'luck']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stat', 'week', 'arriv', '1', 'new', 'follow', 'unfollow', ':)', 'via']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['corner', 'fuck', 'dope', 'get', 'addict', '..', 'great', 'job', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'nice', 'final', 'see', 'end', 'work', 'nearli', '...', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['agre', 'sarah', 'phone', 'realli', 'busi', 'time', 'give', 'option', ':)', 'c']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yesss', 'thank', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['everi', 'dog', 'day', ':)', \"can't\", 'stress', 'enough']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['40', 'day', 'sinc', 'gone', 'still', 'miss', 'like', 'crazi', 'bc', 'mani', 'hard', 'time', 'need', 'support', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pleas', 'come', 'netherland', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['left', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['havent', 'seen', 'earlier', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['spoke', 'age', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['well', 'rude', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['white', 'wash', 'dude', 'polaroid', 'effect', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['never', 'fix', 'sleep', 'schedul', 'school', 'start', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['done', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['asian', 'sjw', 'tri', 'cri', '...', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'bam', 'follow', 'bestfriend', 'love', 'lot', ':)', 'see', 'warsaw', '<3', 'love', '<3', 'x46']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['agre', 'weed', 'anyway', 'like', 'definit', 'plant', 'grow', 'wrong', 'place', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['speak', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['onkey', 'nice', 'edit', 'lol', 'sexi', 'umma', 'pervert', 'onyu', 'appa', 'kid', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['mail', 'follback', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oh', 'sorri', 'hear', ':(', 'pleas', 'email', 'inform', 'pro@illamasqua.com', 'instead', 'manual', 'set', 'x']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wish', 'id', 'put', '¬£', '50', 'real', 'madrid', 'shithous', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['iran', 'irand', ':-(', 'us-iran', 'nuclear', 'deal', \"mit'\", 'expert', 'size', 'deal', 'sever', 'li', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['shanzay', 'salabrati', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['em', 'haha', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ya', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['look', 'forward', \"tomorrow'\", 'gather', 'omg', 'la', 'sudden', 'chang', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['takfaham', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['got', 'netfilx', 'today', 'whoop', 'whoop', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ourdaughtersourprid', 'great', 'bapu', 'u', 'r', 'best', 'coach', 'ji', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['one', 'time', 'went', 'outsid', 'look', 'star', 'peac', 'saw', 'opossum', 'run', 'thru', 'bush', 'shit', \"ain't\", ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['c', 'least', 'duke', 'earl', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['think', 'matt', 'ever', 'get', 'back', 'togeth', 'stay', 'friend', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ahah', 'thank', 'candi', 'axio', 'remind', 'lot', 'white', 'rabbit', 'candi', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['seed', '3', 'beat', 'seed', '9', 'solar', '6-5', 'shown', 'thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['kangin', 'oppa', ':)', 'teenchoic', 'choiceinternationalartist', 'superjunior']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['actual', 'decid', 'come', 'see', 'ive', 'tri', 'give', 'like', '3', 'time', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['guess', \"who'\", 'spent', 'half', 'hr', 'sit', 'outsid', 'apt', 'build', 'bc', 'left', 'key', 'work', 'hope', 'someon', 'need', 'leav', 'bldg', 'soon', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ask', 'boss', 'rais', 'hand', 'ladder', 'told', 'climb', 'ladder', 'success', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bruh', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['evil', 'pepper', 'system', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['feel', 'good', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['‚Äú', 'damn', 'stole', 'home', 'girl', 'phone', '‚Äù', 'got', 'fuck', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['end', 'soon', 't_t', 'hurt', 'one', 'elimin', \"they'r\", 'good', ':(', 'teamzip', 'smtm', '4']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['mind', 'follow', 'back', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hiya', 'email', 'web', 'exec', 'loryn.good@lincs-chamber.co.uk', 'abl', 'help', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['would', 'anyway', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['realli', 'love', '‚Äî', 'love', 'thing', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'topnewfollow', 'happi', 'connect', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'bam', 'follow', 'bestfriend', 'love', 'lot', ':)', 'see', 'warsaw', '<3', 'love', '<3', 'x5']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['prob.nic', 'choker', 'btw', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['kind', 'thk', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hope', 'get', 'see', 'bff', 'tomorrow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['guy', 'add', 'kik', 'abouti', '797', 'kik', 'kiksex', 'interraci', 'tagsforlik', 'kikm', 'travel', 'kikmenow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"what'\", 'wrong', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['want', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['no1', 'onlin', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['awh', ':(', 'nice']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', \"that'\", 'long', 'time']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"can't\", 'finish', 'sanum', 'today', ':(', 'llaollao', 'dessert', 'full']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['love', 'littl', 'bit', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['good', 'morn', 'gorgeou', 'mistress', 'saphir', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['put', 'due', 'releas', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['absolut', 'cuti', 'congratul', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'd\", 'call', 'em', 'del', 'rodder', ':)', 'els']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['awesom', ':D', 'okay', 'na', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['great', 'new', 'opportun', 'junior', 'triathlet', 'age', '12', '13', 'gatorad', 'seri', 'get', 'entri', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hell', 'go', 'wrong', ':(', 'understand', 'peopl', 'choos', 'hurt']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'hahaha', 'wish']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['rain', 'morn', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['gay', 'togeth', 'least', 'leav', 'alon', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['dream', 'around', 'tabl', 'eat', 'someth', 'heavi', 'day', 'came', 'burst', 'song', 'happen', 'irl', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeah', 'horribl', ':(', 'big', 'hug', 'mean', 'lot', 'x']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happen', 'hk', 'weather', 'sensor', 'broker', 'week', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'sorrryyi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['inde', 'thank', 'link', 'articl', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['haha', 'got', 'dont', 'worri', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['alway', 'tweet', 'hit', 'snap', '...', 'one', 'ever', 'worth', 'tri', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['glad', 'one', 'went', 'better', ':)', 'make', 'incred', 'beer']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'paulin', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeeeyyy', 'congrat', '..', ':-)', 'gohf', '2015', 'barsostay']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tell', \"i'm\", ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[]\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cant', 'go', 'sleep', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['video', 'sararoc', 'angri', 'world', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['someon', 'hit', 'head', 'voic', \"i'm\", 'like', 'would', 'sound', 'amaz', 'could', '...', 'falsetto', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['zayn', 'malik', 'pleas', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hope', 'djderek', 'found', 'safe', 'well', 'mani', 'fun', 'time', 'gig', 'absolut', 'legend', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['month', 'sinc', 'saw', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['need', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['help', 'god', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['see', 'leg', 'lycra', ':p']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['aa', 'soon', 'get', 'listen', 'cohes', 'point', \"let'\", 'meet', 'great', 'get', 'feedback', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['use', 'chrome', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['brill', 'let', 'us', 'know', 'camera', 'readi', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ff', 'nyc', 'girl', 'must', 'follow', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"we'r\", 'hire', 'mayb', 'public', 'would', 'like', 'job', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'made', 'chang', 'design', 'hope', 'like', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['rip', 'rodfanta', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['money', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['talk', 'lmao', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wanna', 'watch', 'paper', 'town', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['shit', 'good', 'shepherd', 'lami', ':(', '3']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wish', ':(', ':(', 'pleas', 'get', 'sign', 'xxx']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['need', 'clean', 'window', 'point', 'today', 'bc', 'hugh', 'keep', 'paw', 'leav', 'muddi', 'mark', 'everywher', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'skye', \"i'm\", 'sorri', 'hear', ':(', 'dm', 'store', 'pleas', 'name', 'descript', 'colleagu', 'pleas', '1']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ff', 'excit', 'meet', 'skype', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"let'\", 'start', 'chocol', 'fudg', 'coat', 'superday', 'chocolatey', 'fudg', 'coat', 'super', 'yasu', 'way', 'ooooh', 'sound', 'sexi', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['head', 'need', 'urgent', '...', 'pl', 'appli', 'within', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thnk', 'progrmr', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['phone', 'realli', 'cool', 'thing', 'tell', 'text', '5', 'minut', 'realli', 'got', 'text', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hehe', ':-)', 'smile', 'ear', 'ear', ':-d']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['usual', ':)', 'front', 'back', 'squat']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['selfiee', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ill', 'worst', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['truli', 'better', 'feel', 'mtg', 'get', 'thoughtseiz', 'twice', 'first', '2', 'round', 'game', '1', 'take', 'playabl', 'away', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['love', 'intern', 'leav', 'us', 'today', ':(', 'look', 'yummi', 'treat', 'brought', 'us', 'yumyum']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['long', 'distanc', ':(', 'üíî']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'v\", 'support', 'sinc', 'start', 'alway', 'ignor', ':(', 'pleas', 'let', 'today', 'day', 'final', 'follow', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', '‚ô•', '‚ô•', '‚ô•']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lol', 'job', 'sinc', 'realiz', 'mani', 'fuckboy', 'world', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wew', 'ramo', 'ground', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yup', 'thank', 'huhu', 'mahirap', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stat', 'day', 'arriv', '1', 'new', 'follow', 'unfollow', ':)', 'via']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'come', 'last', 'night', 'great', 'see', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['duh', 'emesh', ':p']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['love', 'lord', 'better', 'life', ':)', '<3']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'retweet', 'noe', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['come', 'fli', 'babi', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['gorgeou', 'fun', \"y'all\", ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['go', 'miami', 'tomorrow', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wanna', 'meet', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['3am', \"can't\", 'sleep', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['live', 'stream', 'suck', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ur', 'nawf', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['shit', 'rlli', ':(', 'heard', 'hamster', 'eat', 'that', 'pretti', 'fuck', 'hamster', 'r', 'weird']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeyi', 'final', 'hd', 'brow', 'kit', 'broken', 'slightli', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hate', 'adult', 'sometim', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['problem', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step     10: train CrossEntropyLoss |  0.61036736\n",
      "List of words from the processed tweet:\n",
      "['...', 'play', 'al', 'n', \"he'll\", 'score', 'fuck', 'night', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sister', 'best', 'best', 'pre', 'bday', 'celebr', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sketchbook', 'art', 'love', '4wild', 'draw', 'hair', 'turn', 'pretti', 'cool', ':D', 'art', 'hair', 'color', 'colorpencil', 'cray', '‚Ä¶']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['back', '80', 'brand', 'new', 'commodor', '64', 'program', 'commun', 'town', ':)', '12th', 'august']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'much', 'mom', 'get', 'ticket', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'favorit', ':)', 'make', 'sure', 'keep', 'touch', 'news', 'light', 'bulb', 'made', 'corn', 'wast']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['obvious', 'get', 'better', 'tweet', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hah', '...', 'thousand', 'lie', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['mood', 'whole', 'day', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['unfortun', 'smell', 'like', 'doo', 'doo', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['aw', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['mtap', 'tomorrow', 'mean', 'sleep', 'earli', 'tonight', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['biggest', 'dream', 'u', 'follow', 'brooo', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'bitch', 'realli', 'went']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['peopl', 'usual', 'stay', 'asleep', 'wtf', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['disappoint', ':(', 'never', 'ever', 'lucki', 'handl', 'jabongatpumaurbanstamped']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step     10: eval  CrossEntropyLoss |  0.52182281\n",
      "Step     10: eval          Accuracy |  0.68750000\n",
      "List of words from the processed tweet:\n",
      "['chang', 'know', 'realli', ':)', 'credit']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['congrat', 'omgg', 'good', 'job', \"today'\", 'race', ':)', '‚Äî', 'thanki', 'good', 'job', 'tdi', 'race', '(-:']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['leav', 'note', 'fridg', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pay', 'end', 'campaign', 'kickstart', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'retweet', 'orchard', 'art', 'show', 'super', 'weekend', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['final', 'got', 'pli', 'copi', 'icon', 'happi', 'ye', 'love', 'alchemist', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['u', 'start', 'compar', 'religion', 'rant', 'well', ':)', 'ur', 'student', 'guess', 'last', 'bencher', 'probabl', '1/5']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['realli', 'good', 'luck', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['still', 'fck', 'nae', 'nae', '...', ':(', 'liter', 'made', 'fuck', 'cri', 'bc', 'deep']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'ok']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['‚Äú', 'paper', 'town', '‚Äù', 'sm', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['woza', 'pleas', 'take', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sad', 'girl', 'oh', 'win', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', ':(', ':(', 'target', 'ftw']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['feel', 'pain', 'like', 'welsh', 'pod', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thankyou', 'pretti', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['look', 'fun', 'kik', 'goictiv', '70685', 'kik', 'kikmeguy', 'xxx', 'tagsforlik', 'webcam', 'trapmus', 'hotmusicdeloco', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hahaha', 'bailey', 'may', ':)', 'fan', 'boy', 'ariana', 'grand', 'pbb', '737gold']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['give', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tell', 'better', 'normal', 'menu', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stat', 'day', 'arriv', '2', 'new', 'follow', 'unfollow', ':)', 'via']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['..', \"that'\", 'best', 'one', '...', ':)', '‚ô°']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['new', 'product', 'line', 'etsi', 'shop', 'check', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"can't\", 'get', 'anywher', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hibb', 'follow', 'back', 'nh', 'gi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sad', 'sad', 'sad', 'kid', ':(', 'ok', 'help', 'watch', 'match', 'hahahahaha']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fever', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['u', 'older', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['steven', 'william', 'umboh', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['christ', \"what'\", 'scale', 'deck', 'chair', 'art', 'kill', 'head', 'man', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cnn', 'run', 'straplin', 'movi', 'theatr', 'safe', '...', 'ye', \"that'\", 'problem', 'lie', ':-(', 'guncontrol', 'lafayett']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yepp', 'cours', 'mo', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follback', ':)', 'x']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['accept', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sound', 'romant', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['morn', ':)', 'kill', 'futur', 'start', 'slow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['maganda', 'look', 'pretti', 'short', 'hair', 'cute', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"here'\", 'life', 'lesson', 'teach', 'kid', 'weekend', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['count', ':p', '28', 'bnte', 'hain', ';p']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ignor', 'video', 'game', 'post', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', \"can't\", 'parent', 'na', 'plan', 'usual', 'work', \"they'r\", 'huhu', 'could', 'tri', 'make', 'paalam']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pfff', 'privat', 'bugbounti', 'find', 'self-xss', 'host', 'header', 'poison', ':(', 'need', 'code', 'execut', 'ktksbye']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['time', 'looww', '(:']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['got', 'one', 'month', 'june', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['long', 'till', 'internet', 'go', 'late', 'pay', 'bill', 'exp', 'wast', 'kill', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['someon', 'pleas', 'gift', 'calibraksaep', 'dont', 'money', 'left', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thought', 'true', 'stop', 'play', 'feel', 'guy', ':(', 'zayniscomingbackonjuli', '26']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['photoset', 'tenyai', 'ya', '‚Äô', 'know', 'think', '‚Ä¶', 'hee', 'hee', ':D', 'first', 'ss', 'month', 'well', 'day', '1', 'semi', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', ':)', 'zzz', 'xx']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ty', ':)', 'av', 'gd', 'wknd', 'u', 'n', 'tweet']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stat', 'day', 'arriv', '2', 'new', 'follow', 'unfollow', ':)', 'via']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'dave', 'love', 'day', 'thank', 'love', 'famili', 'friend', 'miss', 'congratul', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['itti', 'bitti', 'teeni', 'bikini', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nice', 'know', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['one', ':(', 'perhap', 'someth', 'els', '87.7', '2fm']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tf', \"can't\", 'find', 'subtitl', 'oitnb', 'need', 'understand', 'chang', 'backstori', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['realli', 'need', 'pleas', ':(', 'beg', '..']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['30', 'minut', 'count', 'pass', 'edsa', 'ayala', 'tunnel', '...', 'still', 'complet', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'v\", 'support', 'sinc', 'start', 'alway', 'ignor', ':(', 'pleas', 'let', 'today', 'day', 'final', 'follow', 'ÔøΩ', 'ÔøΩ', 'ÔøΩ', '‚ô•']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['gyu', 'r', 'u', 'cute', ':-(', 'hurt', 'ok']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'sorri', 'give', 'enough', 'hug', 'awhil', 'ago', 'b', 'tire', ':(', 'nice', 'see', 'luv']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wish', 'could', 'mayb', 'one', 'year', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['inde', 'ran', 'charact', 'knew', 'get', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['aww', 'bestfriend', 'look', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['jedzjab≈Çka', 'pijjab≈Çka', 'whatev', ':)', 'articl', 'polish', 'cider', 'mustread', 'anyway']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['see', 'flickr', 'pro', 'back', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oh', 'beauti', 'world', 'water', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['good', 'morn', 'dear', 'friend', '<3', 'listen', 'tranc', 'lover', 'enjoy', ':D', 'happi', 'friday', '<3']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['watch', 'abus', 'relationship', 'video', 'fuck', 'hard', \"i'm\", 'tri', 'cri', 'like', '...', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['crave', 'wing', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lover', ':(', 'go', 'zoo']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['play', 'give', 'heart', 'break', 'memori', 'breakup', 'episod', ':(', 'love', 'song', 'tho']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['even', 'sleepi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sorri', ':(', \"i'll\", 'let', 'know', 'see', '‚ô°']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['feel', 'let', ':(', 'thought', 'favourit', 'favourit']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['look', 'shock', 'omggg', ':(', 'yall', 'deserv', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['littl', 'suggest', 'anoth', '1d', 'day', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['love', 'look', 'cake', 'bet', 'tast', 'good', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['soon', '...', 'forgotten', 'peopl', 'pta', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wake', 'nice', 'weather', 'koyal', 'melodi', 'unexpect', 'surpris', 'creat', 'memori', 'fresh', 'make', 'day', ':)', 'maxfreshmov']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeah', 'suppos', 'lol', 'chat', 'bit', 'x', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thanq', 'gooday', 'ff', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['agre', ':)', 'phone', 'wifi', 'lifestyl', 'qatarday']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"that'\", 'one', 'mine', 'button', 'pusher', 'soon', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ye', 'bro', '..', 'lost', 'mani', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fuck', 'hate', 'wake', 'like', 'time', \"can't\", 'fall', 'back', 'asleep', 'ugh', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'mosqu', 'w', 'sit', 'outsid', 'whole', 'time', 'smelli', \"i'm\", 'annoy']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeahh', ':(', 'hard', 'esp', \"they'r\", 'well', 'digit', 'chart', 'cri']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sunni', ':(', 'feel', 'bad', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sometim', \"be'\", 'like', 'yo', 'follow', 'someon', 'day', 'later', 'realis', \"they'r\", 'problemat', 'fuck', 'life', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['know', ':(', 'happi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['long', 'us', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['okay', 'start', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['guy', 'great', 'weekend', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['great', 'hear', 'let', 'us', 'know', 'recip', 'enjoy', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['awesom', ':)', \"i'll\", 'wait']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sweet', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['keep', 'better', 'better', 'everi', 'updat', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['snapchat', 'jennyjean', '22', 'snapchat', 'kikmeboy', 'model', 'french', 'kikchat', 'sabadodeganarseguidor', 'sexysasunday', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['woah', 'nice', 'nice', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tomorrow', '7-3', 'sunday', '7.30-', '4', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['clean', 'room', 'yay', 'pana', 'af', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['want', 'go', 'back', 'apb', 'colleg', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['also', '...', \"kath'd\", 'earlier', 'never', 'respond', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow', 'thank', 'pleas', 'follow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['live', 'trivia', 'crack', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['dad', 'said', 'sorri', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['<3', '<3', 'awsm', 'song', '<3', ':-(', ':-(', \":'(\"]\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hmm', 'gold', '...', ':p']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'd\", 'plan', 'nice', 'scamper', 'slept', 'alarm', 'two', 'hour', 'suppos', 'need', 'sleep', \"ain't\", 'mad', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'iceland', 'amaz', 'crazi', 'landscap', 'hope', 'get', 'soon', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'follow', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['heard', 'lot', 'good', 'thing', 'movi', '..', 'definit', 'watch', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hand', 'incorrect', 'spell', \"i'm\", 'apologist', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'thank', 'connect', 'specialis', 'thermal', 'imag', 'survey', '‚Äì', 'need', 'us', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['im', 'jealou', '..', ':(', 'follow', 'jacob', '..', 'follow', 'jacob', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['greekcrisi', 'greek', 'leav', 'work', 'abroad', 'gave', 'light', 'word', 'remain', 'dark', '...', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['would', 'fair', 'say', 'privileg', 'club', 'joke', 'matter', 'ask', 'noth', 'ever', 'avail', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['end', 'would', 'leav', 'work', 'therefor', 'miss', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"can't\", 'announc', 'cool', 'show', 'today', 'wait', 'monday', 'thank', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['alreadi', 'left', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['woman', 'gave', 'warmest', 'greet', 'w', 'chin', 'tickl', 'even', 'think', 'wrong', 'girl', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['kind', 'eye', 'tear', 'tear', 'hard', 'care', ':(', 'ily.melani']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['twitter', 'meni', 'tebrik', 'etdi', ':)', 'congratul', '700', 'follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tessnzach', 'cute', 'account', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fnaf', '4', 'drop', '...', 'look', 'like', 'sleep', '4']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'follow', 'great', 'connect', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nice', 'tweet', ':)', 'follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':)', 'alway', 'smile']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['rememb', 'oh', 'god', 'frank', 'iero', 'phase', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['make', 'u', 'feel', 'better', 'never', 'see', 'anyon', 'kpop', 'flesh', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cours', 'alway', 'miss', 'tweet', 'spree', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['weather', 'scroll', 'contact', 'peopl', \"i'v\", 'curv', 'hey', 'üòâ', 'sigh', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ifeely', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['knew', 'u', 'would', 'sad', ':(', 'funer', 'gonna', 'text', 'u', 'phone']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sadli', 'show', 'masaan', ':(', 'watch', 'muv', 'come', 'hd']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['imysm', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['heck', ':(', 'follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'zokay', 'russian', 'accent']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['appreci', 'follow', 'like', 'tweet', ':)', 'dorset', 'beauti', 'place']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['msged', 'said', '3', '5', 'day', 'respons', 'im', 'stick', 'chiquito', ':)', 'xx']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cant', 'wait', 'get', 'baller', 'g', 'lil', 'fade', 'chain', ':)', 'everybodi', 'ought', 'know']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'share', 'wish', 'wick', 'weekend', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['perfect', 'alreadi', 'know', \"what'\", 'wait', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['‚Äô', 'exactli', 'look', '‚Äô', 'go', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['quot', 'tweet', 'dm', 'realli', 'want', \"i'll\", 'surpris', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['number', 'two', 'week', 'done', 'gorefiend', 'mythic', 'live', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['scari', 'dream', \"can't\", 'go', 'back', 'sleep', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['done', 'ya', 'gue', 'report', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['la', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['aw', 'miss', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeahh', \"they'r\", 'actual', 'floor', 'closet', \"i'm\", 'pretti', 'sure', \"they'r\", 'high', 'water', ':(', 'ugli', 'back']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bore', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"can't\", 'ever', 'listen', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['suck', 'mean', 'watch', 'play', 'around', '1', 'p', 'time', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step     20: train CrossEntropyLoss |  0.34137666\n",
      "List of words from the processed tweet:\n",
      "['thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hallo', 'twitter', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'amsterdam', 'guy', 'wooo', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stand', 'cool', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cheer', 'mention', '...', 'even', 'wrong', 'section', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['chri', \"that'\", 'great', 'hear', ':)', 'due', 'time', 'remind', 'inde', 'plan', 'avail', 'distant', 'futur']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['imma', 'use', 'next', 'time', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['littl', 'finger', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['deactiv', 'acc', ':(', 'otwolgrandtrail']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['u', \"gray'\", 'followk', 'omg', 'hard', 'day', 'u', 'know', ':(', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ya', 'sorri', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['omg', 'realli', \"i'm\", 'sorri', 'babe', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nooo', 'last', 'day', 'today', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nope', ':(', 'best', 'place', 'get', 'first', 'pre-ord', 'amazon']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'inact', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['told', 'suicid', 'kill', 'ran', 'away', 'forev', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step     20: eval  CrossEntropyLoss |  0.20654774\n",
      "Step     20: eval          Accuracy |  1.00000000\n",
      "List of words from the processed tweet:\n",
      "[]\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['love', 'use', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['love', 'mix', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['like', '...', ':-)', 'new', 'blue', 'point', 'snap', 'socket', 'set', 'spanner', 'ebay', 'ad', '-->']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happi', 'friday', ':-)', 'catch', 'us', '01282', '452096', 'want', 'chat', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ugli', 'daniel', 'good', 'ape', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['mister', 'best', 'mister', ':)', 'support', 'kind', 'absolut', 'amaz', '3']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cheer', ':)', 'xx']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['one', 'favourit', 'mutual', 'unfollow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['glue', 'factori', 'kuchar', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oop', 'meant', 'tini', 'chat', 'day', 'ooop', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['heyyy', 'want', 'see', 'yeol', 'solo', 'dancee', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['believ', 'actual', 'went', 'home', 'sat', 'half', 'hour', 'went', 'work', 'lost', 'number', 'ring', 'anyway', 'bbz', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['could', 'pleas', 'te', 'member', 'auto-followback', ':(', 'thank']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['alreadi', 'miss', 'old', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['absolut', 'world', 'close', 'see', 'show', 'manila', ':(', 'cri', 'fuck', 'hard', 'time']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hey', 'jame', 'odd', ':/', 'pleas', 'call', 'contact', 'centr', '02392441234', 'abl', 'assist', ':)', 'mani', 'thank']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['someon', 'matur', 'taught', 'hide', 'privaci', 'wise', ':)', 'time', 'grow', 'way']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nice', 'talk', 'someon', 'use', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'propos', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['awww', 'thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oh', 'still', 'stuck', 'island', '...', ':D', 'need', 'learn', 'sail', 'ship', 'faster', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['jd', 'laro', 'tayo', ':)', 'tag', 'answer', 'us', 'ht', ':)', 'angelica', 'anghel', 'otwolgrandtrail', 'aa']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"there'\", 'la', 'vega', 'snapchat', 'stori', \"i'm\", 'sad', 'want', 'go', 'back', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['realli', 'want', 'see', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'sorri', 'loss', 'may', 'allah', 'give', 'jannatul', 'ferdou']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"what'\", 'mean', 'understand', 'noth', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['billionair', '1645', 'world', 'island', '1190', 'maldiv', 'dheena', 'fasgadah', 'alvadhaau', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['reign', 'error', 'exampl', 'inflat', 'rate', 'sic', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['good', 'thing', 'got', 'free', 'ear', \"i'v\", 'somehow', 'lost', 'one', 'tini', 'screw', 'ball', 'thing', 'one', 'barbel', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['steal', 'narco', 'grow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['die', 'whilst', 'hug', 'aki', 'yaaay', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tomorrow', 'morn', 'mate', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sweet', ':)', 'u', 'knw', 'tri', 'open', 'block', 'dikha', 'reh', 'h', '..']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['live', 'good', 'memori', 'alway', 'cherish', 'well', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tri', 'come', 'decis', 'favor', 'side', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['someon', 'mention', 'usagi', 'looov', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['onlin', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['inde', 'may', 'email', 'bit', 'info', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['weather', 'aw', 'today', 'mean', 'stuck', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wish', ':(', 'zayniscomingbackonjuli', '26']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['done', 'first', 'week', 'work', 'alreadi', 'got', 'work', 'weekend', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ummm', 'found', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['1000', 'batteri', 'life', 'pleas', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fineandyu', 'take', 'gd', 'pic', 'like', 'tri', 'hurt', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fricken', 'cold', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['‚Äô', 'realli', 'like', 'handl', 'muriel', 'ahour√©', '‚Äô', 'pr', 'brand', 'imag', '‚Äô', 'miss', 'mani', 'opportun', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ghost', 'bae', ':D', 'love', 'tagsforlikesapp', 'instagood', 'smile', 'follow', 'cute', 'photooftheday', 'tbt', '‚Ä¶']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['worri', 'r', 'still', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['need', 'manual', 'tweak', 'php', 'yeah', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['prob', 'still', 'wonder', 'around', \"callie'\", 'bc', 'poor', 'puppi', 'terribl', 'owner', ':-)', 'callinganimalabusehotlineasap']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['worri', 'aapk', 'benchmark', 'ke', 'hisaab', 'se', 'alreadi', 'hit', 'ho', 'gaya', 'hai', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['come', 'bergerac', 'first', 'team', 'left', 'u21', \"i'd\", 'know', 'check', 'chant', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['well', 'get', 'recruit', 'team', 'doom', 'nasti', 'crew', 'go', 'affili', '>:)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['üòÇ', 'üòÇ', 'thank', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['alway', 'wake', 'around', '3', \"can't\", 'fall', 'back', 'asleep', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['problem', '1', \"i'm\", 'uk', '¬£', '2', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['heavi', 'duti', 'benadryl', 'lmao', 'dat', 'shit', 'work', 'forgot', 'take', 'earlier', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bcz', 'kat', 'reject', 'like', 'chem', 'onscreen', 'tho', '...', 'ofscreen', 'kinda', 'ignor', ':(', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sorri', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['today', 'rma', 'jersey', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':-(', \"i'm\", 'w', 'one', 'talk', 'bye']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hand', 'hurt', ':(', 'got', 'sick', 'movement', 'lmao']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'stock', 'marmit', 'chees', 'spread', 'thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"we'r\", 'count', ':)', 'mm']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['great', 'hear', 'glad', 'could', 'help', ':)', 'localgaragederbi']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['best', 'want', 'bodi', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sore', 'yaa', 'slr', 'som', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['good', 'idea', 'free', 'stress', 'specul', '...', 'allow', 'work', 'consist', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stat', 'week', 'arriv', '1', 'new', 'follow', 'unfollow', ':)', 'via']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['6pm', 'door', 'open', 'everyon', 'vip', 'gener', 'ticket', 'get', 'seat', 'earli', 'watch', 'support', 'act', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['teas', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['got', 'stung', 'wasp', 'im', 'cri', 'hard', 'tweak', 'keep', 'get', 'bigger', 'ive', 'never', 'stung', 'one', 'hope', 'im', 'allerg', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['score', 'first', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sent', 'home', \"i'm\", 'allow', 'work', 'broken', 'toe', ':(', 'manag', 'fuck', 'horrend', 'jump', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', ':(', 'pl', 'follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ahhh', 'get', 'well', 'soon', 'amber', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['na', 'gi-guilti', 'akooo', ':(', \":'(\", 'imveryverysorri', 'mistak']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['make', 'realli', 'distress', 'read', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['also', 'sometim', 'nice', 'break', 'see', 'constant', 'tweet', 'one', 'subject', 'welcom', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'omg', 'realiz', 'video', 'awww', 'cute', 'congrat', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'ash', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['woah', 'big', 'plan', 'good', 'luck', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'bam', 'follow', 'bestfriend', 'love', 'lot', ':)', 'see', 'warsaw', '<3', 'love', '<3', 'x27']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'saw', 'u', 'follow', 'thought', 'u', 'might', 'like', 'dark']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['love', 'miss', 'hammi', 'thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'like', 'omg', 'saw', 'misspelt']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['babi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['waduh', '..', 'pant', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['dylan', 'pl', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['citi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['alreadi', 'miss', 'much', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'work', 'hoe', 'xd', \"can't\", 'even', 'look', 'bbi', 'peac', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['doushit', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wowww', 'thank', 'lot', 'team', ':)', '‚ù§', 'üíô', 'üíö', 'üíï', '‚ù§', 'üíô', 'üíö', 'üíï', '‚ù§', 'üíô', 'üíö', 'üíï', '‚ù§', 'üíô', 'üíö', 'üíï']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['steph', 'come', 'back', 'nj', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'alway', 'want', 'see', 'angle.nelson', 'would', 'proud', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['clash', ':)', 'milano', 'call', 'üëå']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['id', 'like', 'think', 'peopl', 'know', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sorri', 'mani', 'updat', 'usual', 'told', 'peopl', 'miss', 'photo', 'lol', ':)', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['awesom', 'look', 'lot', 'like', 'hoxton', 'holborn', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wtf', 'never', 'happen', 'moment', 'may', 'may', 'like', \"there'\", 'surpris', 'thrown', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['unfriend', ':(', '3', 'greysonch']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['get', 'vanilla', 'latt', 'though', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['want', 'armor', 'piec', 'better', 'legendari', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['mom', 'far', 'away', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['see', 'oh', 'hunni', ':(', 'u', 'know', \"i'm\", 'end', 'line', 'u', 'need', 'friendli', 'ear', 'know', \"i'v\", 'said', 'b4', 'still', 'stand']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ugh', 'well', 'could', 'still', 'say', \"i'm\", 'lucki', 'sapiosexu', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wru', 'scissor', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bore', 'sa', 'dorm', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['masaantoday', 'masaantoday', 'a4', '...', 'shweta', 'tripathi', 'masaantoday', 'masaantoday', ':-)', '..']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['love', 'u', '2', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hello', 'babe', 'got', 'niam', 'access', 'could', 'give', 'someth', 'ship', 'dm', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'kalin', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fback', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['haha', 'thank', ':)', 'üç∞']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happi', 'friday', 'love', 'great', 'day', 'love', 'weekend', ':)', 'friday', 'smile', 'weekend', 'fridayfeel']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wow', 'like', 'ur', 'tattoo', '‚Äî', 'thank', ':)', 'love']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['mom', 'call', ':(', 'got', 'distract']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['haha', 'aw', 'miss', 'seen', 'agesss', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['dude', \"i'm\", 'fckin', 'okay', 'avoid', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['look', 'promis', 'even', 'cricket', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['woke', 'pee', 'sun', 'come', \"can't\", 'go', 'back', 'sleep', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'keep', 'bug', 'spray', 'industri', 'busi', 'someth', 'bit', 'foot', 'swollen', 'half', 'way', 'leg', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['come', 'back', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['daddi', 'chang', 'channel', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'get', 'super', 'slow', 'move', 'definit', 'want', 'mono', 'life', 'remix', 'done', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'omar', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['goodby', 'twitter', 'long', 'time', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['amaz', 'deserv', 'siddi', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['7th', 'cinepambata', 'video', 'festiv', 'complet', 'mechan', 'offici', 'entri', 'form', 'may', 'download']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['morn', 'everyon', \"we'r\", 'open', '12pm', 'everi', 'day', 'sunday', ':)', 'hello', 'weekend', 'come', 'say', 'x']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wait', 'foundri', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"here'\", 'selfi', 'tara', 'thank', 'anoth', 'great', 'event', '. . .', 'happyfriday', 'ff', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['eh', 'guy', 'gana', 'face', 'ah', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['charli', 'hot', 'everi', 'time', 'finland', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['still', 'go', 'braid', 'hair', 'damag', 'initi', 'braid', ':(', 'ruth', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['exactli', 'unfortun', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['back', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'sorri', ':(', 'hypercholesteloremia', 'ok']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['treat', 'horribl', 'ugh', ':(', 'im', 'v', 'happi', 'u']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['taken', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':D', 'u', 'kno', 'whet']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nex', 'vex', 'design', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['paint', 'nail', 'pink', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['gamedev', 'hobbi', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['see', 'monday', '2emt', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['got', 'ticket', 'amsterdam', \"can't\", 'wait', 'see', 'guy', 'perform', '3rd', 'time', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['urquhart', 'castl']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'retweet', 'friend', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ye', '..', 'cant', 'see', 'rider', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'robyn', 'realli', 'sorri', 'one', 'necklac', ':(', 'rachel', 'x']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sc', 'long', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fandom', 'bore', ':(', 'hurt', 'zayniscomingbackonjuli', '26']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pleass', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cheer', 'ella', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['jahat', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step     30: train CrossEntropyLoss |  0.20208922\n",
      "List of words from the processed tweet:\n",
      "['love', 'write', ':)', 'absolut', 'ador', 'nva', 'lucki', 'enough', 'live', '5', 'minuet', 'away', 'roll', 'gamec']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['parti', 'cancel', ':p', 'bajrangibhaijaanhighestweek', '1']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thought', 'ear', 'malfunct', 'thank', 'good', 'clear', 'one', 'apolog', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['combin', 'three', 'add', 'bit', 'asian', 'u', 'got', 'truli', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['realli', 'cuti', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['second', 'well', 'done', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['morn', 'u', 'today', 'u', 'got', 'plan', 'lazi', \"i'm\", 'still', 'bed', 'toast', 'fresh', 'coffe', '2', ':)', 'great', 'fridayfunday']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['heyi', ':)', 'u', 'rt', 'link']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['omg', ':(', 'gonna', 'happen', 'yeah', 'wish', '..']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['last', 'night', 'good', ':(', 'üò∫', 'üíí', 'üíé', 'üéâ']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hate', 'hot', 'weather', ':(', 'graduat', 'ceremoni', 'later', 'boy', 'gonna', 'wear', 'suit', 'everyon', 'go', 'die']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['aur', 'ap', 'bhi', 'shamil', 'ho', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['summer', 'holiday', 'great', \"i'm\", 'bore', 'alreadi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['head', 'kill', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pleas', 'follow', 'conno', \"i'm\", 'late', 'time', 'zone', 'could', 'freak', 'never', 'see', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['old', 'one', 'day', '...', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step     30: eval  CrossEntropyLoss |  0.21594886\n",
      "Step     30: eval          Accuracy |  0.93750000\n",
      "List of words from the processed tweet:\n",
      "['happi', 'birthday', 'malik', 'umair', 'big', 'celebr', 'canon', 'gang', 'stay', 'bless', 'bro', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sorri', 'nichola', 'glad', 'back', 'work', ':)', 'alway', 'need', 'us', 'mka']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['okay', \"let'\", 'start', 'littl', 'featur', 'today', ':)', 'y≈´j≈ç-cosplay', 'cosplay', 'luxembourg', 'start', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['killin', 'meme', 'worthi', ':)', 'cool', 'shot']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wonder', 'thing', 'made', 'feel', 'fluffi', 'insid', 'great', 'pirouett', 'moos', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ima', 'leav', 'right', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['photograph', 'took', 'photo', 'us', 'last', 'night', 'find', 'photo', 'event', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pass', ':)', 'üöÇ', 'dewsburi', 'railway', 'station', 'dew', 'dewsburi', 'west', 'yorkshir']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeah', 'true', 'fuck', 'one', 'signatur', 'dish', 'last', 'night', 'unfamiliar', 'kitchen', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['want', 'stuck', 'insid', 'day', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['u', 'make', 'diari', 'pleas', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['aaron', 'said', 'shit', 'ago', 'said', 'worth', 'take', \"that'\", 'bang', 'well', 'jelous', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['domesticviol', ':(', 'propos', 'new', 'law', 'help', 'domest', 'violenc', 'victim', '98fm', 'victim', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['acc', \"can't\", 'look', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['twurkin', 'razzist', '1', 'block', 'report', 'u', 'write', 'tumblr', 'post', 'u', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['gig', 'includ', '..', 'tag', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['welcom', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'realli', 'appreci', 'take', 'time', 'watch', 'video', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['anoth', 'amaz', 'video', 'kev', ':)', 'proud', 'wonder', 'support', 'wonder', 'person', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['goosnight', 'everyon', 'love', 'u', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bank', 'credit', 'depart', 'bad', 'enough', 'anz', 'gone', 'extrem', 'offshor', 'absolut', 'classic', ':-)', 'gottolovebank']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeeeah', ':D', '‚Ä¶']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['morn', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['glad', 'bother', 'make', 'work', 'outsid', 'morn', 'face', 'would', 'gone', 'drain', 'bloodi', 'rain', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sorri', 'say', 'leg', 'black', 'venom', 'way', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['r', 'u', 'angri', '...', 'feel', 'disappoint', '..', 'mm', '...', 'srri', '..', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['went', '10:59', 'stay', '11.00', 'never', 'ubericecream', 'vehicl', 'avail', 'sham', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', '‚Äú', 'vandag', 'long', 'ass', 'month', '...', 'kgola', 'neng', 'eintlik', 'üò¢']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['crave', 'pizza', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow', 'alreadi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['im', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['found', 'thoracicbridg', '5minut', 'flow', 'thank', 'share', ':-)', 'alreadi', 'pass', 'famili', 'like', 'nonscript']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oh', 'awesom', 'thank', 'much', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['well', 'travel', 'around', 'new', 'zealand', 'holiday', 'hope', 'could', 'meet', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ye', 'dad', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happiest', 'birthday', 'hope', 'wonder', 'day', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['gotcha', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['someth', 'get', 'friday', 'great', 'start', ':)', 'great', 'day', 'mclaren', 'fridayfeel', 'tgif']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['say', 'man', 'u', 'fan', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['jouch', 'por', 'que', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['well', 'broke', 'anoth', 'raspberri', 'pi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sometim', 'wonder', 'went', 'wrong', ':(', 'guess', \"i'll\", 'never', 'know']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['god', 'im', 'sorri', 'hope', 'better', ':-(', 'wah']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['foot', 'pain', 'woke', 'nooo', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"can't\", 'see', \"i'm\", 'sorri', 'call', 'manbearpig', 'cannot', 'see', 'sorri', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hope', 'miss', 'na', 'kita', 'cri', 'ia', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tgif', 'forecast', 'rain', '10am', '5am', ':D', 'weird', 'way', 'mind', \"i'm\", 'work', 'anyway', 'rain', 'kinda', 'sooth', 'summer']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oh', 'bed', 'haha', \"can't\", 'sleep', 'though', 'day', 'excit', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['clicksco', 'thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ask', 'made', 'year', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['u', 'wake', '4', 'suit', 'life', 'zach', 'codi', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nba', '2k15', 'mypark', '‚Äì', 'chronicl', 'gryph', 'volum', '3']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['never', 'ever', 'meet', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thankyou', 'greet', 'australia', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['im', 'still', 'giddi', 'd1', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['someon', 'talk', 'miss', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nathann', 'üíï', 'never', 'got', 'chanc', 'take', 'pic', ':(', 'got', 'hug', 'tho', 'pic', 'next', 'time', 'see', 'üòä']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thaank', 'jhezz', 'omg', 'stay', 'awesom', 'damn', 'sneak', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeah', 'next', 'prey', ':(', 'poor', 'girl']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['snapchat', 'shoshannavassil', 'snapchat', 'hornykik', 'selfi', 'gay', 'porn', 'quot', 'camsex', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cant', 'believ', 'final', 'egypt', 'still', 'wont', 'abl', 'meet', ':(', 'gut']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['annnd', 'go', 'winchest', '{:']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"why'd\", 'program', 'acorn', 'origin', 'way', 'sinc', 'c64', 'apart', 'cpu', 'consider', 'advanc', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['listen', 'jazz', 'truck', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['snapchat', 'guy', 'ivypowel', '19', 'snapchat', 'kik', 'chat', 'xxx', 'kikmeboy', 'newmus', 'sexchat', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['experi', 'gastronomi', '...', ':)', 'newblogpost', 'foodiefriday', 'foodi', 'friday', 'yoghurt', 'pancak', '‚Ä¶']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hahahaaah', 'owli', 'said', 'knock', 'momma', 'said', '..', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['coconut', 'sambal', 'mussel', 'recip', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['awesom', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oldschool', 'runescap', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['never', 'knew', 'cousin', 'share', 'love', '1d', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['even', 'know', 'k3g', 'smh', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"can't\", 'abl', 'view', 'pic', 'da', '..', ':(', 'wat', 'pic']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['smile', 'beauti', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['omg', 'wanna', 'hear', 'ghost', 'stori', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wors', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lol', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"they'r\", 'realli', 'uniqu', 'refresh', ':(', 'popular', 'great', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['definit', 'love', 'make', 'peopl', 'smile', 'caus', 'swear', 'sinc', 'met', 'earlier', 'stop', 'smile', ':)', 'love', 'cam', 'üò≠', '‚ù§', 'Ô∏è']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['welcom', 'michael', 'favour', 'dancetast', 'day', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['shout', 'grassi', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['visit', 'blog']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'll\", 'never', 'get', 'use', 'thank', 'much', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'great', 'connect', 'specialis', 'thermal', 'imag', 'survey', '‚Äì', 'need', 'us', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['awwhh', 'cute', 'bulldog', 'friday', 'happi', 'good', 'friday', 'everyon', ':)', 'x']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['done', 'afang', 'soup', 'make', 'semo', 'next', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['carmen', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'cat', 'cafe', 'melbourn']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['babi', 'angel', 'die', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', 'apink', 'perform', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['iam', 'fat', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"can't\", 'smile', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['senpai', '>:(', 'look', 'buttsex']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'sorri', 'alreadi', 'shawn', 'mend', 'sponsor', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':)', 'ty', 'mne', 'pro', 'pchelok', 'ja', '...', ':)', 'thank']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['üç∏', '‚ï≤', '‚îÄ', '‚îÄ', '‚ï±', 'üç§', '‚ï≠', '‚ïÆ', '‚ï≠', '‚ïÆ', '‚îì', '‚îì', '‚ï≠', '‚ïÆ', '‚ïÆ', '‚î≥', '‚ï≠', 'üç∏', '‚ï≤', '‚îÄ', '‚îÄ', '‚ï±', 'üç§', 'üç§', '‚îÄ', '‚ï≤', '‚ï±', '‚îÄ', 'üç∏', '‚î£', '‚ï±', '‚ï∞', '‚ïØ', '‚îó', '‚îó', '‚ï∞', '‚ïØ', '‚ï∞', '‚îª', '‚ïØ', 'üç§', '‚îÄ', '‚ï≤', '‚ï±', '‚îÄ', 'üç∏', 'big', 'love', 'hug', 'babe', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'bam', 'follow', 'bestfriend', 'love', 'lot', ':)', 'see', 'warsaw', '<3', 'love', '<3', 'x28']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':)', 'babi', 'üòò', 'üòò', 'üòò']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['contradict', 'continu', 'creat', 'chao', 'contain', 'cranium', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'follow', 'us', 'wonder', 'friday', ':)', 'ff']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['first', 'day', 'back', 'gym', ':D', 'feel', 'good', ':D', 'gym', 'england', 'feelinggood']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sorri', ':-(', 'hope', 'get', 'better']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['want', 'froze', 'chocol', 'cover', 'banana', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['badli', 'second', 'semest', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['glo', 'yet', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['omg', 'love', 'ty', 'zap', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['idk', 'wanna', 'watch', 'episod', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['noooo', \"i'v\", 'heard', 'news', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'true']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lol', \"look'\", 'like', 'scari', 'room', 'ghost', 'stori', 'murder', 'mysteri', 'either', 'way', 'tri', 'get', 'done', 'asap', ':p']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['jump', 'time', ':p']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'll\", 'regret', 'rn', 'tomorrow', 'morn', 'whatev', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['first', 'meet', 'three', 'today', 'ace', 'way', 'end', 'week', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['think', 'design', 'industri', 'present', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['get', 'readi', 'choir', 'member', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['check', 'blog']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ur', 'mom', 'get', 'u', 'swisher', 'ur', 'cop', 'monday', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['summer', 'sleep', 'schedul', 'back', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fresh', 'news', 'still', 'imposs', 'play', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bad', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', 'u', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['true', 'life', 'cant', 'hide', 'anyth', '...', ':(', '.\\n.', 'love', '...', '‚ù§', '.\\n.', '...', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['low', 'compar', 'rank', 'mcountdown', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['zayniscomingbackonjuli', '26', 'see', 'trend', 'radio', 'next', 'sing', 'see', '...', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['exam', 'today', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['student', 'around', 'globe', 'miss', 'enjoy', 'summer', \"we'll\", 'see', 'soon', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bet', 'plan', 'secret', 'babi', 'shower', 'chloe', 'say', 'bare', 'know', 'comput', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', ':)', 'may', 'interest', 'blog']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['check']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['welcom', 'back', 'teamr', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['talk', 'radio', 'alreadi', 'happen', ':)', 'tweet', 'made', 'like', 'week', 'ago', 'fuck', 'freak']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['great', 'way', 'better', 'last', 'night', \"tonight'\", 'shift', 'work', 'hope', 'better', 'shorter', 'thank', 'tweetin', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'v\", 'want', 'panda', 'like', '3', 'day', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['doubt', \"i'm\", 'go', 'bed', 'soon', 'though', ':(', 'wake', 'like', '9']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hey', 'take', 'care', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hahah', 'dunwan', 'keep', 'also', 'suan', 'le', 'ba', 'haiz', 'see', 'everyon', 'otw', 'rn', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', 'king', 'futur', 'boss', 'rain', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['heart', 'breath', 'moment', 'time', \"i'll\", 'find', 'word', 'say', 'leav', 'today', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'mean', ':(', 'sazbal', '‚ù§']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'least', 'freez', 'outsid', 'right']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'bam', 'follow', 'bestfriend', 'love', 'lot', ':)', 'see', 'warsaw', '<3', 'love', '<3', 'x7']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['kill', ':)', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['6:15', 'pm', 'rn', 'current', '70', 'degre', 'fahrenheit', 'realli', 'warm', 'good', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['morn', 'mine', '1100d', 'usual', 'auto-focu', 'use', 'manual', 'realli', 'close', 'like', 'shoot', 'vein', 'crackl', 'glaze', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pain', 'workout', 'love', 'pain', 'impair', 'get', 'uggghhh', 'dam', 'fuck', 'day', 'shit', 'get', 'dun', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['let', 'us', 'know', 'best', 'address', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['‡§¨', 'matter', 'jungl', 'accommod', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['suck', 'everyon', 'knock', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pleas', 'bring', 'back', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['secret', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nk', 'gi', 'uss', 'birthday', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['may', 'look', 'strong', 'hey', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['someon', 'sc', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['music', 'keep', 'paus', 'randomli', 'app', 'open', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['u', 'alreadi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step     40: train CrossEntropyLoss |  0.19611198\n",
      "List of words from the processed tweet:\n",
      "['aww', 'thankyou', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tack', '<3', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['back', 'thnx', 'god', \"i'm\", 'happi', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bad', 'thing', 'think', 'weird', 'face', 'rad', ':-)', 'thank', 'though']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stat', 'day', 'arriv', '6', 'new', 'follow', 'unfollow', ':)', 'via']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':)', 'masaantoday']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hp', 'curs', 'child', 'book', 'play', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['number', 'one', 'america', 'üò≠', 'üá∫', 'üá∏', 'wish', 'could', 'buy', 'money', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sharknado', 'one', 'hour', 'never', 'get', 'back', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['realli', 'sorri', 'cant', 'find', 'imag', 'mention', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ehem', 'üòè', 'üòè', 'haha', 'ala', 'yeke', ':(', 'okay', 'fun', 'kk', 'jumpa', 'next', 'time', '‚úà', 'Ô∏è', 'üá∫', 'üá∏', 'mayb', 'üòã']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['...', 'havent', 'finish', 'yet', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['think', 'need', '2', 'year', 'train', 'beat', 'record', 'sia', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['im', 'hungri', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step     40: eval  CrossEntropyLoss |  0.17582777\n",
      "Step     40: eval          Accuracy |  1.00000000\n",
      "List of words from the processed tweet:\n",
      "['that', 'lord', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':-)', 'caro']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['everi', 'day', 'shud', 'begin', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['see', 'tonight', 'tgp', '1', 'test', 'race', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['moar', 'kitteh', 'incom', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['appreci', 'follow', ':)', 'love', 'weekend']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oh', '...', 'hope', \"he'\", 'okay', 'karen', \"he'\", 'like', 'old', 'dog', \"he'll\", 'start', 'wag', 'bum', 'instead', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'contribut', 'kind', 'comment', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow', 'back', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'may', 'request', 'promo', 'code', 'pl', 'tri', 'use', 'iwantin', 'got', 'reject', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'tan', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fk', 'freak', 'word', 'describ', '...', 'eric', 'prydz', 'like', ':(', 'world']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['even', 'awak', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['saw', 'chri', 'brown', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hot', 'cornwal', 'verit', 'cold', ':(', 'great', 'prize', 'xx']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cont', 'line', 'l', 'onlin', 'repli', 'mention', 'watch', 'tl', 'x', 'sorri', 'late', 'repli', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wanna', 'see', 'beauti', 'rotat', 'photo', 'work', '...', ':p', 'v']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['beauti', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['resort', 'right', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['make', 'sure', 'alway', 'tissu', 'chocol', 'tea', 'everi', 'hannib', 'episod', ':)', 'work', 'way', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['true', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['come', 'back', 'next', 'thursday', 'still', 'hang', ':-)', 'hehe', 'said', 'hi', 'love', 'u']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['30', '¬∞', 'c', 'üòú', 'chang', 'day', 'good', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['get', 'fuck', 'outta', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['unblock', 'instagram', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stock', 'sa', '1', 'hour', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['im', 'sed', '..', ':-(', 'person', 'im', 'meet', 'late', 'im', 'end', 'sg', 'havent', 'taken', 'dinner', 'yet', 'bless']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ihhh', 'stackar', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', 'lil', 'shawti', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"that'\", 'goat', 'mino']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"there'\", 'milk', 'left', 'cereal', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['jeremi', 'kyle', 'close', 'home', 'morn', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'follow', 'gemma', 'enjoy', 'new', 'orlean', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hey', 'thank', 'follow', ':)', 'autofollow', 'teamfollowback']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['msg', 'play', 'fantasi', '...', 'mayb', 'get', 'leagu', ':)', 'bowwowchicawowwow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wait', \"we'r\", 'youtub', 'comment', 'section', 'video', 'slightli', 'mention', 'scienc', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wow', 'thank', 'glad', 'notic', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['need', 'shower', 'food', 'first', 'though', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['üò≠', 'think', 'well', 'offens', 'laugh', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['anyon', 'give', 'money', 'train', 'see', 'today', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hell', 'insta', 'work', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nobodi', 'told', 'side', 'twitter', 'cs', 'own', 'sorri', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'sorri', ':-(', ':-(', ':-(', 'block']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['im', 'tire', 'af', 'need', 'ignor', 'bc', 'schoolwork', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'certain', 'unfortun', ':(', \"i'm\", 'okay-ish', 'html', \"i'd\", 'quit', 'like', 'better', 'languag', 'work', 'c', 'cours']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bewar', 'agonis', 'thought', 'experi', 'ahead', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'kind', 'good', 'look', 'power', 'kinda', 'meaningless', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'follow', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['annoy', 'selfi', 'friday', 'walangmakakapigilsakin', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'would', 'like', 'concert', 'let', 'know', 'citi', 'countri', \"i'll\", 'start', 'work', 'thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['mani', 'thank', ':)', 'via']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stat', 'day', 'arriv', '1', 'new', 'follow', 'unfollow', ':)', 'via']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thug', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['got', 'half', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['climatechang', 'cc', \"ain't\", 'easi', 'green', 'golf', 'cours', 'california', 'ulti', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stay', 'lahhh', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['paid', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oh', ':-(', 'hope', 'love', 'time', 'though', 'danni']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['much', ':(', 'especi', 'tv', 'network', 'cancel', 'time', \":'(\"]\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow', 'forgot', 'followback', 'unfollow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lol', ':D', 'hilari', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['men', 'like', 'power', 'tool', 'make', 'lot', 'nois', 'hard', 'get', 'work', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'babi', 'love', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stat', 'week', 'arriv', '1', 'new', 'follow', 'unfollow', ':)', 'via']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['advaddict', '_15', 'follback', 'ya', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"insha'allah\", '..', 'wil', '..', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wait', 'comeback', 'gross', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sorri', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sorna', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'sa', 'bracelet', 'ang', 'ganda', ':(', 'üíó', 'üíó', 'üíó']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['match', 'histori', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['31', 'unit', 'first', 'sem', 'second', 'year', 'pleas', 'tell', 'us', 'kid', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':-(', \"i'm\", 'alreadi', 'plan', 'lfccw', 'hope', 'meet', 'guest']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['til', '19th', 'sept', 'co', \"that'\", 'bill', 'start', 'full', 'rent', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeah', \"i'v\", 'heard', ':(', 'think', 'learn', 'littl', 'tradit', \"people'\", 'beheaviour', 'visit', 'countri', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fav', 'awak', 'fam', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'macauley', 'mexico', 'sound', 'love', ':-)', 'plan', 'add', 'new', 'countri', 'moment', 'howev', \"we'r\", '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happi', 'birthday', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fab', 'hope', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['..', 'ye', 'sometim', 'pass', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['beyond', 'excit', 'la', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['dinner', 'love', 'love', 'babe', ':)', 'x']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['love', 'thank', 'share', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lucki', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['9/11', 'kinda', 'ruin', 'sound', 'make', 'brace', 'hear', 'plane', 'crash', 'everi', 'time', 'bueno', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['know', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ignor', 'pleas', ':(', 'birthday', '12th', 'juli', 'could', 'follow', 'present', 'üòä', 'üòä', 'üòä', '‚ù§', 'Ô∏è']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wish', 'could', 'join', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['goat', 'got', 'scare']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ah', 'bad', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['co', '‚Äú', 'would', 'entertain', 'tbh', 'hey', 'rememb', 'typa', 'weather', ':(', '‚Äù']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'also', 'move', 'tomorrow', 'good', 'luck', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['feel', 'smthng', 'smthng', 'fr', 'u', '...', 'lol', '...', 'wanna', 'hug', 'u', '...', 'xoxo', '...', 'love', 'uu', 'jaann', 'take', 'care', ':)', '<3']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'shout', 'wonder', 'weekend', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['gotham', 'sneak', 'peek', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['congraaat', 'guysss', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['appreci', 'follow', ':)', 'heard', 'botan', 'soap', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['smart', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stat', 'day', 'arriv', '1', 'new', 'follow', 'unfollow', ':)', 'via']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'sorri', 'sweeti', ':(', 'one', 'deserv']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['teribl', 'headach', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['naaan', 'pa', 'goodby', 'stage', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['goodluck', 'pocket', \"i'm\", 'move', 'next', 'week', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bosen', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ah', 'honestli', 'love', 'dad', 'much', \"i'm\", 'slightli', 'upset', \"i'm\", 'see', 'birthday', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['go', 'indian', 'polit', 'everi', 'one', 'blame', '68', 'year', 'still', 'repres', 'school', 'life', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['guy', 'buy', 'u', 'water', 'u', \"can't\", 'drink', 'u', 'watch', 'cup', 'like', 'hawk', 'transfer', 'bartend', 'u', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'bam', 'follow', 'bestfriend', 'love', 'lot', ':)', 'see', 'warsaw', '<3', 'love', '<3', 'x12']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nobodi', ':)', 'like', ':)', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'wonder', 'reader', 'like', 'rose', 'make', 'worthwhil', 'love', 'weekend', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['must', 'hahaha', ':p']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', ':)', 'cjradacomateada']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['view', 'offic', 'pretti', 'awesom', 'ugli', 'side', 'sea', 'side', 'even', 'prettier', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"can't\", 'go', 'great', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['enjoy', 'londondairi', 'icecream', 'share', 'us', 'experi', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['understand', ':(', 'sad', 'depress', 'make', 'extrem', 'anxiou', 'poc', ':/']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['song', 'freak', 'saaad', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['love', 'much', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['want', 'jack', 'box', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ive', 'got', 'much', 'thing', '3', 'day', ':(', 'syawal']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['suppos', 'august', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"party'\", 'goodby', 'stage', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hate', 'insecur', 'go', 'away', 'pl', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['aha', 'yesss', 'look', 'forward', 'see', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['luckili', 'one', 'mani', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['welcom', 'famili', '..', ':)', 'send', 'laughter', 'giggl', 'way']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fun', ':p']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['weltum', 'teh', 'deti', ':p', 'huh', '=D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['look', 'pretti', 'nice', ':)', 'love', 'flat', 'look']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['alway', 'seem', 'like', 'sloppi', \"second'\"]\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['kind', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['know', 'other', 'gonna', 'miss', 'classic', 'duo', 'ti5', 'cast', 'stream', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pleas', 'come', 'back', 'love', 'much', '<3', ':(', 'zayncomebackto', '1d', 'zayniscomingbackonjuli', '26', 'zayniscomingback']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['feel', 'like', \"i'm\", 'person', 'ireland', 'go', 'see', 'croke', 'park', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['girl', 'came', 'like', 'beauti', ':-(', 'want', 'dieididieieiei', 'cute']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['im', 'gonna', 'miss', 'u', 'sexi', 'prexi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sheriff', 'seen', 'aaa', 'aaag', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', 'chri', 'voic', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'v\", 'go', 'sleep', 'like', '5am', 'everday', ':(', 'would', 'cool', 'peopl', 'like', 'would', 'hangout', 'time']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['weekend', 'gonna', 'shoot', 'love', '<3', 'pictur', 'made', 'last', 'time', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['haha', 'ye', '24', 'hr', 'time', 'come', 'touch', 'kepler', '452b', 'chalna', 'hai', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['great', 'watch', 'fellow', 'fan', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['followback', 'po', ':-)', ':-d']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hope', \"u'll\", 'njoy', 'followingg', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['interview', 'newsround', 'news', 'hound', '7:40', 'today', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'enter', 'purchas', 'specially-mark', 'first', 'choic', 'custard', 'pack', 'enter', 'via', 'sm', 'on-pack', 'instruct', ':)', 'good', 'luck']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['need', 'sleep', '...', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['got', 'home', '..', 'heavi', 'rain', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lost', '3.4', 'k', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['need', 'get', 'motiv', 'mean', 'walk', 'way', \"q'don\", 'think', 'energi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sold', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ao', 'want', 'stick', '8th', 'either', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sorri', 'cant', 'hope', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['didnt', 'expect', 'omg', 'face', ':(', 'im', 'proud', 'infinit']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step     50: train CrossEntropyLoss |  0.11203773\n",
      "List of words from the processed tweet:\n",
      "['thank', 'refollow', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['life', 'smile', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['morn', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['far', 'good', 'ikaw', 'musta', 'cheatmat', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['1', 'day', 'go', '1', 'day', 'left', 'sponsor', 'us', 'matter', 'small', 'donat', 'differ', 'make', 'huge', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['u', 'like', 'uta', 'read', 'manga', ':-)', 'love', 'even', ':-)', 'haha', 'yeah']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lol', 'meant', 'lucki', 'eagl', 'thank', 'servic', 'oceana', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['read', 'newspap', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['missin', 'homeslic', 'bday', ':(', '333']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['best', 'get', 'littl', 'ladi', \"pj'\", 'need', 'go', 'wallpap', 'mom', ':(', 'realli', 'cba', 'today']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['whatev', 'stil', 'l', 'young', '>:-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['home', 'alon', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bull', 'shark', ':(', 'late', 'trafficcc']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['deth', 'deth', 'dont', 'leav', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['want', 'bandana', 'bottl', 'set', 'huhuh', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['develop', 'releas', 'good', 'game', 'school', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step     50: eval  CrossEntropyLoss |  0.07589275\n",
      "Step     50: eval          Accuracy |  1.00000000\n",
      "List of words from the processed tweet:\n",
      "['come', 'tomorrow', 'team', ';)', 'count', ':)', 'gohf']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['finish', 'develop', 'test', 'spain', 'next', 'offici', 'test', 'excit', 'season', '2', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['anyon', 'need', 'ride', 'educampakl', 'tomorrow', 'leav', 'rotorua', 'earli', 'morn', 'pick', 'peopl', 'way', 'like', 'come', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['u', 'look', 'angel', 'one', ':)', 'flipkartfashionfriday']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'activ', \"i'm\", 'near', 'make', 'account', 'privat', '...', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'much', 'help', 'spread', 'word', 'us', 'finalist', 'nation', 'lotteri', 'award', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeey', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['favourit', 'peopl', 'deactiv', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thankyou', 'sa', 'treat', 'kanina', 'sorri', 'sa', 'breakdown', 'bc', 'sir', 'mag', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['unfair', 'didnt', 'get', 'follow', 'other', 'get', 'spam', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', 'half', 'music', 'bank', 'bc', 'groceri', 'shop', ':(', 'good', 'thing', 'final', 'hubba', 'bubba', 'bubbl', 'gum', '<3']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['noon', 'live', 'drag', 'draw', 'ok', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['left', 'ear', 'swell', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['call', 'babi', 'know', 'im', 'one', ':(', '‚Äî', 'eat', \"zach'\", 'burger', 'xpress']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['one', 'thank', 'bc', '1d', ':(', 'h', 'e', 'e', 'kbye']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hah', 'given', 'hawk', 'fan', 'say', 'greatest', 'side', 'histori', 'along', 'team', '80', 'talkback', 'fun', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['omg', 'math', 'channel', 'continu', 'like', 'doubt', 'get', '2', '300', 'sub', '200', 'sub', 'video', 'come', ':)', ':/']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['someon', 'plz', 'convo', 'w', 'bc', 'never', 'sleep', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'love', 'ff', 'right', 'back', 'love', 'day', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['like', 'wonder', 'origin', 'recip', 'share', 'foiegra', 'lover', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['complet', 'youtub', 'intro', ':)', 'excit', 'intro', 'full', 'graphic', 'graphicdesign', 'videograph', 'youtub']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['like', 'coca', 'cola', 'love', 'fanta', 'dream', 'pepsi', 'die', 'sprite', 'all', 'sweeeti', '‚ô°', '‚ô•', '‚ô°', '‚ô•', ':-)', ';-)', ':-d']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['say', 'want', 'luci', 'gone', 'soon', 'lisa', 'oh', \"that'\", 'horribl', 'sorri', ':-)', 'ye', 'back', 'soon', 'pleas', 'x']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['school', 'start', 'pretti', 'soon', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fuck', 'sake', 'hamstr', 'injuri', 'delph', ':-(', 'anoth', 'rodwel']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'daniel', 'oh', ':(', 'latest', 'softwar', 'instal', 'tri', 'back', 'restor']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', 'venic', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['final', 'hate', 'switzerland', 'kfc', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['know', 'vine', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['gave', 'away', 'last', 'night', 'didnt', 'hear', 'anybodi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['obyun', 'unni', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['1000', 'view', 'thank', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['keep', 'smile', 'life', 'beauti', 'thing', 'much', 'smile', ':)', 'mani', '‚Ä¶']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'much', '..', 'glad', 'like', '..', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['look', 'forward', 'review', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['text', 'hit', 'dm', '...', 'lol', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"friday'\", 'give', 'us', 'call', 'today', 'commerci', 'farm', 'insur', 'requir', ':)', 'happyfriday', 'lookfortheo']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sept', '4th', 'rudramadevi', 'anushka', 'gunashekar', 'sir', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['finish', 'download', '10', 'movi', 'one', 'complet', 'tv', 'seri', 'last', 'night', '‚Ä¶', 'where.do.i.start', ':)', 'moviemarathon']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'pl', 'think', 'good', 'enough', '___', 'let', \"he'\", 'miss', 'chanc', 'fault']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['everyon', 'happi', 'bc', 'amaz', 'friend', 'bc', '1d', 'still', 'wait', 'internet', 'bestfriend', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['want', 'chicken', 'nugget', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'watch', \"i'm\", 'work', ':(', \"i'm\", 'interest', 'man', 'citi', 'anyway', 'üòÇ', 'hope', 'lose']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yach', '..', 'telat', '..', 'huvvft', '...', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['complet', 'forgot', 'final', 'symphoni', ':(', 'need', 'meet', 'sometim', 'soon', 'joe', 'co']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['your', 'get', 'famou', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['muller', 'sold', 'quit', 'obviou', 'though', 'still', 'hope', ':(', 'mufc']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happi', '30th', 'wed', 'anniversari', 'folk', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['earth', '. . .', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['goodmorn', \"what'\", 'come', 'next', '=:', '=:']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['contact', 'us', 'today', \"we'll\", 'get', 'citi', 'hotb', 'bush', 'somewher', ':)', 'classicassur']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['made', 'happi', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['favor', 'tridon', 'caus', \"he'\", 'tank', ':p']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['emon']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hang-copi', 'translat', \"dostoevsky'\", 'note', '..', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['misundersrand', 'day', 'chal', 'raha', 'hai', 'yhm', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happi', 'trip', 'keep', 'safe', 'see', 'soon', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fri', 'pleas', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['‚ôõ', '‚ôõ', '‚ôõ', '„Äã', '„Äã', '„Äã', 'love', 'much', 'beliÃáev', 'wiÃáll', 'follow', 'pleas', 'follow', 'pleas', 'justiÃán', ':(', 'x15', '340', '„Äã', '„Äã', '„Äã', 'ÔΩìÔΩÖÔΩÖ', 'ÔΩçÔΩÖ', '‚ôõ', '‚ôõ', '‚ôõ']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wast', 'good', 'outfit', 'today', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hey', 'bud', 'christi', 'realli', 'sick', \"can't\", 'leav', '2', 'home', ':(', 'xo', 'thank', 'man']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'okay', 'x']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['futur', 'show', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'babi', 'girl', ':)', 'xx']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'share', 'enjoy', 'app', 'wish', 'wick', 'weekend', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ok', 'remind', 'set', '9:00', 'repli', 'want', 'chang', 'time', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'emili', \"i'm\", 'fan', 'wish', 'much', 'happi', 'life', 'funni', 'beauti', 'happi', 'birthday', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', ':)', 'üç∞']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['love', ':)', 'wow', 'pick', 'cala', 'brush', 'day']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['parent', 'proud', '‚Äî', 'lol', 'least', ':D', 'mayb', 'sometim', 'get', 'happi', 'grade', 'al', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['low', 'qualiti', 'pic', 'high', 'qualiti', 'girl', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', 'edel', 'aww', ':(', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'fuck', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['feelgoodfriday', 'way', 'camp', 'new', 'forest', '...', 'rain', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['start', 'pictur', ':(', 'serious', 'zayniscomingbackonjuli', '26']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['either', 'would', 'prefer', 'benzema', 'get', 'though', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'use', 'foot', 'injuri', 'lol', '..', 'appendix', 'burst', 'ago', 'append', '..', 'told', 'hernia', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['snapchat', 'leannerin', '19', 'snapchat', 'hornykik', 'loveofmylif', 'dmme', 'pussi', 'newmus', 'sexo', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"what'\", 'wrong', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['im', 'hope', 'home', 'april', 'week', ':)', ':)', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"pixar'\", 'insid', 'today', 'ask', '5', 'member', \"what'\", 'favourit', 'subject', 'brain', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['aww', 'yay', \"i'm\", 'hate', 'group', 'promis', 'u', 'u', 'ignor', 'hater', 'retali', \"it'll\", 'get', 'better', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ye', 'work', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bitter', 'wait', 'till', 's02e04', 'air', 'dominion', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['definit', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"here'\", 'wish', 'happi', 'birthday', 'doug', 'bolling', 'turn', '34', 'today', '..', ':)', 'play']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cleans', 'scam', 'ridden', 'bodi', 'vyapam', 'go', 'renam', 'idea', ':D', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ugh', 'want', 'right', 'right', 'need', 'ya', 'tay', 'one', 'understand', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pleas', 'help', 'find', 'babi', 'goat', 'stolen', 'chilton', 'mother', 'cri', 'feed', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ask', 'sister', 'said', 'neither', 'parent', 'like', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['babi', 'boy', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['rain', 'today', 'boo', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['want', 'big', 'hug', 'want', 'sweet', 'smile', 'want', 'warm', 'eye', 'sinc', 'moment', \"can't\", ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['got', '7', 'outfit', 'music', 'bank', 'messi', ':(', 'colour', 'could', 'better']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ahh', 'wonder', \"they'r\", 'realli', 'support', 'pg', 'shame', 'msc', 'like', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'would', 'like', 'concert', 'let', 'know', 'citi', 'countri', \"i'll\", 'start', 'work', 'thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cant', 'see', 'feet', ':p']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['app', 'look', 'influenc', ':)', 'interest', \"here'\", 'invit']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'alic', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['love', 'everi', 'step', 'way', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['gail', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', ':)', 'üç∞']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ye', 'pleas', 'would', 'great', 'work', ':D', 'peachyloan', 'fridayfreebi', 'xx']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nose', 'forehead', 'peel', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['rain', ':(', 'make', 'sure', 'get', 'pictur', 'soon']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['realli', 'sorri', 'receiv', 'wrong', 'order', ':(', 'would', 'like', 'sort', 'straight', 'away', 'email', 'help@veryhq.co.uk']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sweat', 'look', 'omg', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['realli', 'want', 'pierc', \"cupid'\", 'bow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['know', 'song', 'disappear', 'phone', 'itun', \"i'v\", 'lost', 'daze', 'confus', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tbh', 'bestfriend', 'breakup', 'even', 'wors', 'relationship', 'breakup', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['realli', 'realli', 'realli', 'realli', 'realli', 'realli', 'like', ':)', ':)', 'psygustokita']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['great', 'enough', 'reason', 'listen', 'one', 'epic', 'soundtrack', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['look', 'forward', '12', 'hour', 'shift', 'today', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['favourit', ':)', 'cream', 'soda', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ack', 'think', 'read', ':)', 'heard', 'good', 'thing', 'far', 'seem', '..', 'awesom', 'hih', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['got', 'back', 'walmart', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['would', 'know', 'huh', 'look', 'mirror', 'everi', 'day', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['see', 'thing', 'there', 'one', 'way', 'teach', 'someon', 'rais', 'awar', 'anyway', '..', 'time', 'real', 'work', '::']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nobodi', 'go', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['corbyn', 'must', 'understand', \"labour'\", 'new', 'member', 'chang', \"party'\", 'fortun']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ever', 'hear', 'mari', 'know', 'live', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cri', 'real', 'coz', 'bandana', 'cute', 'match', 'dress', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['mum', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['smh', 'know', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['taxi', 'emptier', \"ciara'\", 'concert', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['need', 'massag', ':(', 'asap']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['great', 'weekend', 'friend', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['see', 'yaaah', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['need', 'someon', 'talk', 'alway', ':)', 'keep', 'head']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'geoff', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['well', 'haha', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fridaydownpour', 'hope', 'clear', 'wkend', 'good', 'one', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['well', 'say', 'europ', 'rise', 'find', 'hard', 'believ', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wake', 'smile', 'face', 'thank', 'heart', 'god', 'give', 'anoth', 'brand', 'new', 'start', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['niaaa', 'pl', 'follow', '2/4', 'miss', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hope', 'sj', 'nomin', 'soon', 'sj', 'vs', 'infinit', 'pl', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hello', '3', 'geez', '..', 'badli', 'miss', '12', 'preciou', 'star', ':(', 'exo', '<3']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['would', '1.300', 'credit', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['awww', 'shit', 'net', 'low', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['god', 'pleas', 'help', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['one', 'person', 'still', 'import', '...', 'want', 'gone', 'longer', '...', 'yet', 'sit', 'scare', 'mind', ':(', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['big', 'thank', 'everyon', 'done', 'without', 'support', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['5', 'year', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['gud', 'night', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pleas', 'comment', 'think', 'wrote', 'far', 'twitlong', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wet', 'feet', 'welcom', 'carpet', 'worri', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['mute', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'share', 'articl', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tomorrow', 'lmao', 'hehe', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ce', 'say', 'gw', 'pleasee', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"can't\", 'open', 'door', 'hard', 'notic', 'alreadi', 'start', 'song', 'love', 'live', 'fail', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['take', 'back', ':(', 'hsm', 'canario', 'park', 'hotel']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'merlin', 'current', 'wait', 'roof', 'repair', 'kitchen', 'let', 'water', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pleas', 'come', 'san', 'antonio', 'soon', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['need', 'holiday', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['love', 'hozier', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"we'r\", 'get', 'older', 'babi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step     60: train CrossEntropyLoss |  0.09375446\n",
      "List of words from the processed tweet:\n",
      "[\"can't\", 'wait', 'week', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['t-shirt', ':D', 'awesom']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['look', 'fun', 'kik', 'thencerest', '547', 'kik', 'kiksext', 'sex', 'followback', 'l4l', 'indiemus', 'kikgirl', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['famili', 'hang', ':)', '‚Äî', 'feel', 'festiv', 'jamuna', 'futur', 'park']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['morn', 'em', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['haha', 'kyle', \"gf'\", 'babi', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['could', 'say', 'egg', 'face', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"he'\", 'gona', 'miss', 'sheff', 'utd', 'york', 'probabl', 'wait', 'visa', 'leav', 'gim', '1', 'friendli', 'bet', 'start', 'season', 'bench', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oh', 'gfriend', 'perform', 'rn', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bore', 'time', ':(', 'know', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tya', 'mind', 'refollow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['joke', 'man', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sorri', 'hear', 'jess', ':(', 'quickest', 'way', 'call', '44', '7782', '333', '333', 'add', 'account', 'within', '72', 'hour']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['drop', 'dead', 'fred', 'use', 'favorit', 'movi', 'wish', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['think', 'someon', 'hack', 'acc', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step     60: eval  CrossEntropyLoss |  0.09290724\n",
      "Step     60: eval          Accuracy |  1.00000000\n",
      "List of words from the processed tweet:\n",
      "['hi', 'bam', 'follow', 'bestfriend', 'love', 'lot', ':)', 'see', 'warsaw', '<3', 'love', '<3', 'x38']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['brief', 'introduct', '2', 'earliest', 'histori', 'indian', 'subcontin', 'even', 'bfr', 'maurya']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['messag', 'us', 'right', 'nicola', 'forward', ':)', 'brighten', 'day', ':D', 'helen']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['singl', 'man', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sweet', 'tooth', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fback', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['love', 'baekhyun', 'love', 'eyesmil', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['adolf', 'hitler', '...', 'obstacl', 'exist', 'surrend', 'broken', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', 'senior', 'much', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[]\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sorri', 'closest', 'door', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['dd', 'leav', 'lagi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wanna', 'hang', 'fellow', 'tweep', 'roar', 'lion', 'victori', 'tweepsmatchout', 'want', 'hang', 'twitter', 'famili', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow', 'thank', 'pleas', 'follow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['feel', 'nose', 'pool', 'water', 'goe', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nah', '1', 'jordan', ':(', 'tournament', ':(', 'im', 'goin', 'ps4']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['let', 'friend', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['famili', 'friend', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['alway', 'amaz', 'peopl', 'forget', 'drink', 'water', 'drink', 'water', 'peopl', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cuuut', ':)', '‚ô°']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'appreci', 'bodi', 'feed', 'need', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', \"we'll\", 'get', 'ad', 'student', 'research', 'job', 'board', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lmaoo', 'best', 'song', 'ever', '<3', 'shall', 'littl', 'throwback', 'week', ':p']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'brian', \"i'll\", 'get', 'ship', 'today', 'take', '2-3', 'week', 'australia', 'mani', 'thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['snapchat', 'jaclintil', 'snapchat', 'snapchat', 'teen', 'model', 'likeforlik', 'mpoint', 'hotfmnoaidilforariana', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cute', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['realli', 'first', 'ever', 'edit', 'haha', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cheatday', \"i'v\", 'ran', 'protein', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['5h', 'kid', 'make', ':/']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wtf', 'wrong', 'peopl', '...', 'shoot', 'movi', 'theater', 'smh', '...', 'damn', 'peopl', 'cant', 'even', 'live', 'good', 'life', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['chanc', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tire', 'work', 'morn', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happi', 'friday', 'thank', 'recent', 'rt', 'fave', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['great', 'start', 'se100lead', 'shine', 'light', 'good', 'busi', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nope', 'definit', 'still', 'appli', 'year', 'want', ':D', 'iamca']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"can't\", 'wait', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nice', 'see', 'spanish', '...', 'well', 'latin', 'american', 'spanish', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':)', 'day', 'tommorow', 'ok', 'chill']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['enemi', 'simpli', 'go', 'around', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pick', 'pleas', 'employ', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cold', 'abi', 'hous', 'dey', 'close', 'arctic', 'ocean', 'man', 'die', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['even', 'nate', 'think', ':(', 'drop', 'place', 'alreadi']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wanna', 'go', 'hous', 'rn', 'give', 'big', 'big', 'hug', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['‚ôõ', '‚ôõ', '‚ôõ', '„Äã', '„Äã', '„Äã', 'love', 'much', 'beliÃáev', 'wiÃáll', 'follow', 'pleas', 'follow', 'pleas', 'justiÃán', ':(', 'x15', '319', '„Äã', '„Äã', '„Äã', 'ÔΩìÔΩÖÔΩÖ', 'ÔΩçÔΩÖ', '‚ôõ', '‚ôõ', '‚ôõ']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['poor', 'kid', 'momma', 'die', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tuesday', 'daw', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['none', 'friend', 'came', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['come', 'belgium', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeah', 'right', 'perfect', \"let'\", 'pretend', 'video', 'song', 'anyth', 'els', 'make', 'suck', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pick', 'pleas', 'employ', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ye', 'thank', 'much', 'care', 'send', 'link', ':)', ':)', '‚ô•', 'promo', 'song', 'beauti']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['gotta', 'love', 'timezon', ':p']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['learn', 'spell', '::']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ye', 'cass', 'think', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lmaoo', 'mca', 'team', 'gettin', 'hella', 'money', 'dm', 'u', 'wana', 'make', 'fast', 'bandz', 'asap', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['great', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'vine', 'luke', 'clip']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"we'r\", 'truli', 'sorri', ':(', 'safe', 'flight']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['vidcon', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['imi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lloyd', 'r', 'asshol', 'clearli', 'one', 'anyth', 'n', 'wait', 'till', 'tuesday', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['snapchat', 'guy', 'sexyjan', '19', 'snapchat', 'kik', 'horni', 'bestoftheday', 'photo', 'goodmus', 'sexual', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['a9', 'would', 'ice', 'cream', 'without', 'ice', ':(', 'wsalelov']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['leed', 'wanna', 'see', 'live', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['made', 'cajun', 'chicken', 'spice', 'couscou', 'dinner', 'yum', ':)', 'cajunchicken', 'couscou', 'food', 'dinner', '‚Ä¶']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeah', 'sure', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['credit', 'rocro', '13glodyysbro', ':p']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['realli', 'glad', 'hear', ':)', 'anymor', 'problem', 'need', 'us', \"we'r\", 'tweet', 'away', 'lb']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'go', 'cabincrew', 'interview', 'langkawi', '1st', 'august', ':)', 'pleas', 'wish', 'luck']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['want', 'sleep', 'time', 'guess', 'got', '...', 'morn', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['plan', 'trip', 'kuch', 'enjoy', 'special', 'room', 'rate', 'merdeka', 'palac', 'hotel', 'suit', 'plusmil', 'card', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['omg', '...', 'beauti', '...', 'miss', 'sooo', 'much', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow', 'thank', 'pleas', 'follow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sorri', 'busi', 'arini', 'je', 'tak', 'busi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['guess', \"who'\", 'miss', \"selena'\", 'tweet', 'fan', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['know', 'said', 'michael', 'jackson', 'obvious', 'littl', 'boy', 'littl', 'brother', 'listen', 'jackson', '5', 'song', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'bright', 'man', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['said', 'got', '7', 'back', 'vietnam', 'decemb', 'dunt', 'think', 'awww', ':(', 'kid']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['feel', 'well', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['twine', 'look', 'great', \"diane'\", 'place', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['alright', 'last', 'winner', ':)', 'dm', 'ur', 'email']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'follow', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hour', 'beauti', 'sleep', 'readi', 'hour', 'pure', 'organis', 'offic', 'weekend', ':-)', 'oh', '...', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['caught', 'first', 'salmon', 'today', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['look', 'offlin', 'thesim', '4', 'game', 'guid', \"we'v\", 'got', 'cover', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['talk', 'help', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['settl', 'main', 'air', 'dkt', 'porch', ':p']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nooo', 'broke', 'soo', 'amaz', 'toghet', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yet', ':(', '...', 'hope', 'soon', 'finger', 'crosss', 'come', 'clapham']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['go', 'x-ray', 'dental', 'monday', 'check', 'spine', '...', 'noth', 'though', 'apart', 'pain', 'relief', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['movie', 'buddyyi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"everyone'\", 'holiday', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hell', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pack', 'sock', 'camp', 'wear', 'odd', 'sock', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'b', 'rude']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['go', 'sleep', 'u', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nextweek', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['jisung', 'kid', 'sunshin', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['chalut', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"we'r\", 'radio', 'delhi', 'hit', '95', 'fm', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['beauti', ':)', 'got', 'blackfli', 'courgett', 'flower', 'year', '..', 'idea', 'hope', 'wont', 'affect', 'fruit']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"can't\", 'sleep', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['..', ':(', 'ohh', 'need']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['catch', 'onlin', 'long', 'enough', 'haha', \"he'\", 'annoy', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['saaad', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['gorgeou', 'sexi', 'hot', 'amaz', 'angel', '‚Äô', 'still', 'shoot', 'rome', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['send', 'sunshin', 'rain', 'much', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ore', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['win', 'ticket', 'codi', 'simpson', 'concert', 'singapor', 'august', '10']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['get', 'free', 'gold', 'w', '‚Äì', 'download', 'world', 'tank', '‚Äì', 'easi']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['collect', '..', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['flipkartfashionfriday', 'love', '...', 'hope', 'u', 'love', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"we'v\", 'receiv', 'payment', 'order', 'process', ':)', 'thank', 'shop', 'spree']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['welcom', 'thank', 'follow', 'back', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yay', \"we'r\", 'mega', 'excit', 'retail', 'launch', 'great', 'know', 'feel', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':D', 'welcom']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stomach', 'hurt', 'bad', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hear', 'sob', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lol', 'love', 'sweet', 'potato', 'fri', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['kik', 'gion', '886', 'kik', 'kikhorni', 'teen', 'talk', 'nsfw', 'kidschoiceaward', '2015', 'hotfmnoaidilforariana', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['anyon', \"else'\", 'show', 'box', 'work', ':(', ':(', ':(', ':(', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['still', 'dosent', 'work', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['idk', 'mani', 'us', 'doctor', 'toxin', 'freedom', 'go', 'organ', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'doug', 'today', 'thank', 'lot', 'follow', 'look', 'forward', 'tweet', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['good', 'morn', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['idea', ':)', 'cc', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['flash', '...', 'day', 'long', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happi', '...', 'see', 'photo', 'rescu', 'mutt', 'twitter', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['help', 'advic', 'info', 'need', 'yell', ':)', 'normal', 'catch', 'us', 'erm', 'time', ';)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['actual', '1', 'hehe', ':)', 'live', 'strang', 'imagin', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['verg', 'owl', 'citi', 'thank', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['leav', 'pcb', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['‚Äô', 'rain', 'cold', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['last', 'though', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['done', 'pleas', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'got', 'ground', 'bc', 'think', 'sneak', 'peopl']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', 'youuu', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stori', 'life', '...', 'love', 'london', \"can't\", 'imagin', 'live', 'elsewher', 'gonna', 'happen', 'someday', ':(', 'space']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['love', 'nicknam', 'babygirl', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sport', 'club', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happi', 'weekend', 'everyon', 'smile', 'lot', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['famili', 'time', ':D', 'dj', 'resto', '‚Äî']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nighti', 'night', 'let', 'bed', 'bug', 'bite', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ye', 'big', 'ye', 'final', 'got', 'home', 'sever', 'rain', 'thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['case', 'cat', 'video', 'mood', 'swear', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ÿØÿπŸÖŸÖ', 'ŸÑŸÑÿπŸàÿØÿ©', 'ŸÑŸÑÿ≠Ÿäÿßÿ©', 'heiyo', 'visit', 'websit', 'free', '50.000', 'coin', '8', 'ball', 'pool', 'thank', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'everyon', '..', 'good', 'aftenoon', 'feel', 'booor', ':(', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sir', 'pleas', 'announc', '3', 'badli', 'wait', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['dnt', 'stab', 'meh', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['unfortun', \"today'\", 'perform', 'pier', 'approach', 'go', 'ahead', 'due', 'weather', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['11:11', 'boyfriend', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['need', 'get', 'photo', 'taken', 'licenc', 'weekend', 'rli', 'bad', 'acn', 'along', 'chin', 'forehead', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow', 'thank', 'pleas', 'follow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['drop', 'home', 'today', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step     70: train CrossEntropyLoss |  0.08785903\n",
      "List of words from the processed tweet:\n",
      "['love', ':)', 'want', 'corn', 'chip', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['snapchat', 'sexyjudi', '19', 'snapchat', 'kikmeboy', 'tagsforlik', 'pussi', 'gay', 'indiemus', 'sexo', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hiya', \"i'm\", 'afraid', \"we'v\", 'alreadi', 'pick', 'rider', 'year', \"we'll\", 'look', 'next', 'year', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nice', 'pic', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bluesidemenxix', \"i'd\", 'love', 'win', 'one', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['make', 'use', 'masquerad', 'mask', 'lol', 'zorroreturm', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['aitor', 'sn', 'would', 'mind', 'check', 'bio', 'particip', 'ps4', 'giveaway', 'could', 'winner', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['could', 'make', \"foxy'\", 'song', 'version', 'instrument', 'form', 'sound', 'epic', 'beat', 'alon', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['weather', 'set', 'sleep', 'respons', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['agh', 'sorri', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['never', 'give', 'best', 'thing', 'take', 'time', 'weh', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['everyon', 'asleep', 'alreadi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bade', 'fursat', 'se', 'usey', 'banaya', 'hai', 'usey', 'uppar', 'waal', 'ney', 'afso', 'hums', 'dur', 'hai', 'wo', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['add', 'snapchat', 'jannygreen', '22', 'snapchat', 'kiksex', 'xxx', 'seduc', 'tagsforlik', 'nakamaforev', 'hotel', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['la', 'time', 'right', 'ye', 'yunni', 'im', 'seriou', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['last', 'full', 'night', 'greec', ':(', 'opu', 'inner', 'pleasur']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step     70: eval  CrossEntropyLoss |  0.09610598\n",
      "Step     70: eval          Accuracy |  1.00000000\n",
      "List of words from the processed tweet:\n",
      "['get']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['finger', 'cross', ':-)', 'bajrangibhaijaanhighestweek', '1']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['late', 'night', 'storm', 'thunderstorm', 'could', 'put', 'sleep', 'everi', 'night', '..', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['matter', 'pay', 'bill', 'alway', '..', 'foreveralon', ':D', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['rememb', 'one', 'time', 'go', 'flume', 'kaytranada', 'alunageorg', 'even', 'though', 'ticket', 'still', 'want', 'km']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeah', 'realli', 'shoulda', 'hippo', 'one', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['seem', 'like', 'peopl', 'use', 'men', 'sourc', 'noth', 'new', 'guess', 'gossip', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['got', 'home', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', 'mcg', 'rn', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['would', 'entertain', 'tbh', 'hey', 'rememb', 'typa', 'weather', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'disgust', 'piec', 'shit', '..', 'amaz', 'mother', 'mateo', 'brooklyn', 'lucki', 'üíó', 'deserv']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"can't\", 'see', 'tweet', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['feel', 'yah', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeah', ':(', 'okay', \"i'll\", 'see', 'time', 'next', 'year']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bad', 'shoulder', 'start', 'hurt', 'like', 'bitch', ':(', \"we'r\", 'day', 'away', 'competit', 'hope', \"it'll\", 'fine', 'time']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['biodivers', 'taxonom', 'infrastructur', 'intern', 'collabor', 'new', 'speci', 'discoveri']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"that'\", 'fair', 'enough', ':)', 'rush', 'back', 'either']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['üíÖ', 'üèΩ', 'üíã', ':)', 'seen', 'year']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['glad', 'like', 'check', 'nice', 'stuff', 'wsalelov', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'ga', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['basic', 'piss', 'cup', 'test', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['final', 'day', 'hale', 'cheshir', 'got', 'love', 'shot', \"i'll\", 'share', 'tomorrow', 'call', 'book', 'decor', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yup', 'soul', 'good', ':-)', 'guitar', 'soul', 'featur', 'cocoar']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['okay', 'thank', 'mak', '‚àó', 'lega', '‚àó', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['mum', 'stop', 'pick', 'cloth', 'pleas', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['buff', 'leg', 'yet', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['suppos', 'rain', 'weekend', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['go', 'home', 'soon', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['swifti', 'outsid', 'u', 'suppos', 'vote', ':(', 'realli', 'realli', 'realli', 'want', 'help', 'win']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happen', 'eyesight', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wanna', 'go', 'back', 'dream', 'üéÄ', 'buy', 'load', 'cloth', 'woke', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['know', 'help', 'get', 'awak', '...', 'shock', 'spill', 'coff', 'rest', 'coff', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['free', 'slice', 'toast', 'everi', 'larg', 'coffee-weekday', '9-11', '30', ':)', 'ask', 'us', 'offer', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['check', 'bro', '2nd', 'blogiversari', 'mani', 'awesom', 'review', 'come', ':D', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fback', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['read', 'beauti', 'like', 'read', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['good', 'morn', 'steve', '...', 'gooday', 'ff', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hahaha', 'tell', 'anytim', 'wanna', 'tast', 'oh', 'yer', 'address', 'send', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['‚Äô', 'muslim', 'love', 'prophet', ':)', 'ÿ∫ÿ±ÿØŸÑŸä']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['need', 'special', 'medicin', \"son'\", 'kidney', \"can't\", 'afford', 'bought', 'printer', 'ink', 'last', 'week', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['woaah', 'chri', 'brown', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['feel', 'sick', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oh', ':(', 'insur', 'loss', 'stolen', 'accident', 'damag', 'devic', 'warranti', 'repair', 'sent', 'repair', 'centr']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['aw', 'know', \"i'm\", 'sorri', ':(', 'hashbrown', 'üòÇ', 'üòä', 'üíò', 'üíò']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ugh', 'think', \"i'm\", 'get', 'sick', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fan', 'dog', 'keeno', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['jokingli', 'said', 'serious', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'follow', 'telecom', 'happi', 'friday', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['therealgolden', '47', 'thank', 'follow', 'hope', 'enjoy', 'work', ':-)', ':-)', ':-)', ':-)', ':-)', 'girlfriendsmya', '‚Ä¶']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'bam', 'follow', 'bestfriend', 'love', 'lot', ':)', 'see', 'warsaw', '<3', 'love', '<3', 'x19']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['...', 'special', 'offer', 'rrp', '¬£', '18.99', 'price', '¬£', '13.99', '...', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'love', 'specialist', 'breath', 'peopl', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['plant', 'peopl', 'allow', 'thing', 'everi', 'thing', ':p']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['mayb', 'issu', 'end', 'sure', \"i'll\", 'shoot', 'email', 'right', 'away', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nice', 'one', 'baz', 'lucki', 'work', 'wonder', 'compani', 'peopl', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['im', 'listen', 'everyth', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['kik', 'himseek', '8', 'kik', 'kiksex', 'kikmsn', 'like', '4like', 'kissm', 'akua', 'hotel', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['weather', 'make', 'want', 'go', 'gym', '...', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['twitter', 'help', 'center', '39', 'follow', 'peopl']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['one', 'help', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thought', 'find', 'someon', 'hard', 'find', 'tri', 'make', 'stay', 'harder', ':(', 'imintoh']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', \"i'm\", 'help', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pleas', 'pleas', 'come', 'back', 'wwe', ':(', 'ÿ™ŸÉŸÅŸâ']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['friday', 'night', 'hong', 'kong', '...', 'ferri', 'central', 'meet', 'new', 'girlfriend', 'after-work', 'drink', ':-)', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oh', 'okay', 'thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oh', 'yeah', 'definit', 'go', 'tri', 'tonight', ':)', 'took', 'advic', 'treviso']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sleep', 'pill', \"i'v\", 'nocturn', 'past', 'coupl', 'day', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['solo', '7', 'chees', 'overload', 'pizza', '‚Ä¢', 'mcfloat', '‚Ä¢', 'hot', 'fudg', 'sanda', 'munchkin', \"d'd\", '‚ô•', 'atm', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"here'\", 'invit', 'becom', 'scope', 'influenc', ':)', 'detail']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['send', 'booti', 'pic', '2', 'bestfriend', 'bcuz', 'bae', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pictur', \"i'd\", 'like', 'see', 'obituari', 'advert', ':)', 'goofingaround', 'mad', 'mad', 'bollywood', '‚Ä¶']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow', 'thank', 'pleas', 'follow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"ain't\", 'gonna', 'follow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tri', 'think', 'return', 'spain', 'leav', 'eldest', 'behind', 'alreadi', 'one', 'cri', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ugh', 'cant', 'stream', 'tmr', 'duti', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pretti', 'sure', ':-(', 'least']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['‚ôõ', '‚ôõ', '‚ôõ', '„Äã', '„Äã', '„Äã', 'love', 'much', 'beliÃáev', 'wiÃáll', 'follow', 'pleas', 'follow', 'pleas', 'justiÃán', ':(', 'x15', '350', '„Äã', '„Äã', '„Äã', 'ÔΩìÔΩÖÔΩÖ', 'ÔΩçÔΩÖ', '‚ôõ', '‚ôõ', '‚ôõ']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tri', 'way', 'brain', 'hard-wir', 'alway', 'feel', 'like', 'enough', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['awww', ':(', 'kaya', 'yan', 'think', 'posit']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['forgot', 'u', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'time', 'thing', 'like', 'wrap', 'tonight', 'gotta', 'figur', 'get', 'back', 'surrey', 'afterward', 'thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hello', 'üëä', 'üèª', '‚Äú', 'hi', ':)', 'trmdhesit', '‚Äù']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'fave', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pegel', 'lemess', 'n', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hey', 'liam', 'know', 'sweet', \"spot'\", 'around', 'barnstapl', 'bideford', 'stay', 'day', 'plan', 'abit', 'road', 'trip', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['effect', 'spanish', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['mash', 'phone', 'lost', 'number', '..', ':(', 'whattsap', 'avail']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['want', 'new', 'stuff', ':(', 'use', 'love', \"friday'\", 'xur', 'month', 'excit', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'hope', 'someth', 'hold', 'interest', 'soon']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['suuuper', 'sick', ':(', 'fever', 'got', 'way', 'wors', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['name', 'tho', ':-(', 'miss']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['back', 'fah', 'mine', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['heed', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'would', 'like', 'concert', 'let', 'know', 'citi', 'countri', \"i'll\", 'start', 'work', 'thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['caus', 'problem', 'discuss', 'thing', 'know', ':p']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['leg', 'hurt', 'bad', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nice', 'use', 'non-mapbox', 'map', 'still', 'restrict', 'use', 'mapbox', 'basemap', 'contractu', 'speak', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'break', \"what'\", 'new', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hey', 'r', 'u', 'camera', 'cuz', 'everytim', 'look', 'u', 'smile', ':)', 'liveonstream']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['done', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['seen', 'mani', 'menac', 'sinc', 'finish', 'jojo', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['aarww', '...', 'wish', 'could', ':(', 'xx']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hate', 'see', 'sad', ':(', 'x']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['last', 'cross', 'countri', 'ij', 'amaz', 'üí•', '‚ú®', 'üí´', \"can't\", 'bear', 'leav', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['isco', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['want', 'stay', 'till', \"infinite'\", 'win', '5:30', 'alreadi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lmfao', 'dont', 'know', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['idea', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cpm', 'condemn', 'rest', 'polit', 'parti', 'quiet', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yaap', 'urwelc', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'may', 'like', 'play', 'newest', 'gamejam', 'game', 'would', 'happi', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['en', 'mi', 'mano', 'hold', 'hand', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['talk', 'shit', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fback', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['congrat', 'roy', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['look', 'like', 'great', 'way', 'spend', 'friday', 'afternoon', 'enjoy', 'marathon', 'iflix', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yummi', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['that', '120', 'wing', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'll\", 'get', 'cement', 'cast', 'tomorrow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['someon', 'talk', \"i'm\", 'boreddd', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['look', 'previou', 'tweet', 'seen', 'messag', 'due', 'hack', ':(', 'hope', 'got', 'sort']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['prompt', 'claim', 'far', 'kinda', '...', 'like', \"i'v\", 'areadi', 'read', 'someth', 'similar', ':(', 'soulmat', 'au', 'canon', '...', 'zzz', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['data', 'deplet', 'wish', 'could', 'come', 'back', 'say', \"i'm\", 'sorri', 'mbasa', 'joke', \"i'm\", 'still', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'miss', 'much']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wrocilam', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['volunt', 'one', ':p']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['<---', 'ff__special', 'acha', 'banda', 'hai', '‚úå', 'tweet', 'bhi', 'achi', 'krta', 'hai', 'handsom', 'bhi', 'hai', 'üòÇ', 'must', 'follow', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hey', 'dave', \"i'd\", 'like', 'help', 'could', 'dm', 'full', 'address', 'drop', 'tweet', 'pleas', ':)', 'haileyhelp']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hope', 'rest', 'night', 'goe', 'quickli', '...', 'bed', '...', 'got', 'music', 'fix', 'time', 'dream', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thankyou', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hope', 'thank', 'mfundo', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['figur', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['liam', 'access', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['think', 'mjhe', 'aaj', 'tak', 'kisi', 'ne', 'dm', 'ni', 'kiya', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oh', 'gosh', 'love', 'much', '__', 'fri', 'noodl', 'love', 'üíú', 'üíú', 'üíú', 'stop', 'sell', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'wayn', \"we'r\", 'sorri', 'hear', 'look', 'leav', ':(', \"what'\", 'happen', 'make', 'want', 'go']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', \"i'm\", 'sorri']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['struggl', 'like', 'crazi', 'get', 'race', 'fan', 'mode', 'head', 'want', 'heart', 'cooper', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'bout', 'instant', 'transmiss']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['doesnt', 'sound', 'appeal', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['donna', 'thurston', 'collin', 'saw', 'today', 'dog', 'sit', 'quietli', 'kennel', ':(', '911', 'need', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['great', 'end', 'work', 'week', 'sunshin', 'low', 'humid', '87', 'chang', 'weekend', 'detail', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hit', 'map', 'googl', 'inform', 'look', 'like', 'quit', 'pretti', 'littl', 'place', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['aug', \"i'm\", 'super', 'pack', 'howev', 'rather', 'thing', 'rather', 'work', 'home', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'sweetest', 'kindest', 'comet', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'mention', 'honor', 'includ', 'among', 'great', 'blog', ':)', 'great', 'weekend', 'share', 'x']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tonight', 'night', '...', ':)', 'nuka', 'penacova', 'djset', 'free', 'edm', 'kizomba', 'latinhous', 'housemus', 'portug', '2015', 'summer']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'bundl', 'show', 'internet', 'price', 'phone', 'realisticli', 'pay', 'net', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['realli', 'happi', 'today', '...', 'wat', 'u', 'done', 'us', 'muahhh', ':)', 'may', 'god', 'bless', 'u', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['reject', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['one', 'cute', 'u', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['..', 'buti', 'thought', 'ure', 'poop', 'friend', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stream', 'said', 'standbi', 'even', 'inatal', 'zenmat', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nam', 'enjoy', 'man', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cotton', 'candi', 'favourit', ':(', 'time', 'stock']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['watch', 'cop', ':(', 'jame']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['zayncomebackto', '1d', 'zayniscomingback', 'fuck', 'play', 'emot', 'pleas', '...', 'start', '...', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step     80: train CrossEntropyLoss |  0.08858261\n",
      "List of words from the processed tweet:\n",
      "['wip', 'let', 'get', 'thing', 'tweak', \"i'll\", 'link', ':)', 'max', 'kal', 'tak']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sound', 'still', 'like', 'good', 'idea', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['req', 'star', ':)', 'fridayloug']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'follow', 'great', 'day', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['dan', 'god', 'bless', 'meet', 'greet', 'soon', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['love', 'pic', 'fab', 'show', ':-)', 'x']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['look', 'forward', 'hear', 'thought', ':)', 'maritimen']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'guy', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['still', 'get', 'paper', 'engg', 'go', 'back', 'cmc', 'mage', 'okay', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['guy', 'never', ':(', '‚Äî', 'aw', 'sorri', \"we'r\", 'realli', 'bust', 'atm', '..', 'shall', 'back', 'soon']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bauuukkk', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['alex', 'fell', 'asleep', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tagal', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'want', 'good', 'least', 'watchabl', 'account', 'neither']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['need', 'like', 'tell', 'na', 'na', 'okay', 'lang', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nobodi', 'pick', 'one', 'right', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step     80: eval  CrossEntropyLoss |  0.02319432\n",
      "Step     80: eval          Accuracy |  1.00000000\n",
      "List of words from the processed tweet:\n",
      "['like', 'u', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hello', ':)', 'get', 'youth', 'job', 'opportun', 'follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['glad', 'could', 'help', 'make', 'sure', 'pass', 'comment', 'thank', 'love', 'tweet', ':)', 'ip']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['make', 'much', 'sens', ':D', 'lol']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pick', 'pleas', 'employ', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['‚Äú', 'emu', 'stoke', 'thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['kyle', 'wont', 'send', 'email', 'right', 'ur', 'paypal', 'confirm', 'need', 'night', ':)', 'see', 'u', 'oct']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':-)', 'read', 'today', \"i'm\", 'excit']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['still', 'gotta', 'glow', 'ok', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"can't\", 'sleep', 'work', '6', '1/2', 'hour', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['know', 'song', 'sing', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['work', '5', 'hour', '...', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['zayn', 'come', 'back', 'juli', '26', 'srsli', 'guy', 'mani', 'time', 'say', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['got', 'home', 'got', 'ol', 'ugh', 'outdat', 'af', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['Ôº†', 'maverickgam', 'juli', '24', '2015', '07:32', 'pm', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cours', ':(', ':(', '..', 'understand', \"y'all\", 'gonna', 'ignor', 'question', 'huh', 'niqqa', 'desper', 'need', 'answer', 'lol']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['garden', 'stun', 'even', 'rain', 'yvett', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['song', 'week', 'ducktail', 'surreal', 'exposur', 'sotw']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['want', 'read', 'orhan', 'pamuk', 'hero', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wait', 'what', 'snpcaht', 'mind', 'ask', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['awww', 'cute', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['last', 'night', 'flip', 'great', 'fun', 'learnt', 'backflip', ':D', 'jumpgiant', 'backflip', 'foampit', '‚Ä¶']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['shadowplayloui', 'recent', 'got', 'mutual', \"i'm\", 'realli', 'love', 'tweet', 'everyth', 'yay', \"that'\", 'excit', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ye', 'keep', 'come', 'list', 'someon', 'els', 'mention', 'persona', '‚Äô', 'tri', 'thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"can't\", 'go', 'ed', ':(', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['see', '4', 'week', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'lmfaooo']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hope', 'person', 'jail', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['plisss', 'tweet', 'elf', ':(', 'teenchoic', 'choiceinternationalartist', 'superjunior']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['teeth', ':(', 'smile', 'wider']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['photo', 'boyirl', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['everybodi', 'fell', 'asleep', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['well', 'made', '13', 'week', 'nwb', 'offici', 'today', ':D', 'wait', 'till', 'monday', 'start', 'long', 'road', 'walk']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['one', 'would', 'awesom', 'well', 'either', 'even', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ouch', 'slip', 'disc', 'sore', 'today', 'bring', 'thw', 'swim', 'chute', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'bro', ':D', 'x']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['starsquad', '‚≠ê', 'pleas', 'get', 'follow', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['great', 'reserv', 'window', '10', ':)', 'mm']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':)', 'yup', 'in-shaa-allah', 'make', 'dua', 'u', 'fr', 'us', 'see', 'thru', 'hatr', 'move', 'toward', 'truth', 'aameen', '4/5']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['well', 'mean', 'ur', 'vinni', 'know', ':)', 'u', 'dont', 'money', 'hahahahaha', '50cent', 'repay', 'ur', 'debt']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['justin', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['alter', 'sandra', 'bland', 'storifi', 'btw', 'quit', 'angri', 'wrote', 'origin', 'best', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['end', 'ncc', 'head', 'train', 'gonna', 'late', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['quit', 'know', 'yet', 'come', 'hulk', 'hogan', 'today', 'clearli', 'hulkamania', 'offici', 'dead', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yup', 'alway', 'order', 'anyway', 'üòÇ', 'hope', 'push', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['gone', ':(', 'hello', 'offer', 'free', 'perfum', 'sampl', 'chanel', 'burberri', 'prada', 'giveaway', 'site', 'pleas', 'check', 'bio']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'cough', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['see', 'penni', '...', ':-(', 'dm', 'email', 'address', \"i'll\", 'send', 'ebook']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeah', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['updat', 'soon', \"i'm\", 'sure', 'sign', 'set', 'text', 'alert', ':)', 'da']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'jack', 'best', 'thing', 'would', 'call', '101', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['guy', 'say', 'omegl', 'üòÅ', 'could', 'use', 'googl', ':)', \"that'\", \"i'm\"]\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['knee', 'plz', 'mine', 'im', 'unmarri', '...', 'check', 'finger', '...', \"there'\", 'rock', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['play', 'v.ball', 'atm', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['delv', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bbmme', 'pin', '7df9e60a', 'someon', 'add', 'bbm', 'pin', 'okayi', ':)', 'bbmme', 'bbmpin', 'addmeonbbm', 'add', 'bbm', 'bbmme', 'bbmpin', 'addm', 'mi', 'pin']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'teenchoic', 'choiceinternationalartist', 'superjunior']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['good', 'eve', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bum', 'zara', 'trouser', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ahahah', 'consid', 'petit', 'love', 'hate', 'relationship', 'height', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['want', 'chocol', 'lucozad', ':(', ':(', ':(', ':(', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['today', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'feedback', 'think', 'perhap', 'mobitel', 'may', 'better', 'heck', 'guess', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"we'r\", 'go', 'moment', ':(', 'percentag', 'ohh', 'pleas']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"that'\", 'great', 'stephani', 'good', 'luck', 'train', ':)', 'findyourfit']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tbh', 'dont', 'know', 'yet', 'havent', 'told', 'us', 'much', 'get', 'back', 'u', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stat', 'day', 'arriv', '2', 'new', 'follow', 'unfollow', ':)', 'via']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happyyi', 'birthdaaayyy', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pick', 'pleas', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bout', 'lay', 'watch', 'jurass', 'world', 'love', 'littl', 'movi', 'night', ':)', 'üíï']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeah', 'fun', ':)', 'flipkartfashionfriday']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['followfriday', 'top', 'influenc', 'commun', 'week', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sick', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['movi', 'marathon', 'anyonnee', ':(', 'loner', 'af']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['omg', 'wtf', 'peopl', 'apb', 'got', 'schedul', 'alreadi', 'wish', 'us', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ohmygod', 'worst', 'case', 'scenario', 'na', 'tooo', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['two', 'month', 'im', 'still', 'hurt', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['met', 'new', 'kind', 'peopl', 'new', 'classmat', 'new', 'set', 'friend', 'everyth', 'new', 'dont', 'find', 'bestfriend', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['...', 'mental', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', 'geneva', 'much', 'england', 'aint', 'got', 'half', 'sun', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wait', 'patient', 'mr', 'crush', ':)', 'first', 'let', 'go', 'sleep', 'üò¥']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tud', 'jst', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['want', 'full', 'thing', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['...', 'except', 'lord', \"masha'allah\", ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['aaahhh', 'see', '...', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['problemmm', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fantast', 'brilliant', 'news', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['look', 'made', 'final', 'design', 'hope', 'like', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['scare', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['‡§è‡§ï', '‡§¨‡§æ‡§∞', '‡§´‡§ø‡§∞', '‡§∏‡•á‡§Å', '‡§ß‡•ã‡§ñ‡§æ', '...', '>:(', '‚Äî', 'feel', 'silli', 'chandauli', 'majhwar', 'railway', 'station']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow', 'thank', 'pleas', 'follow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['jmu', 'camp', 'teas', 'bc', 'wanna', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sad', 'manag', 'get', 'ticket', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['valentin', 'et', 'al', 'found', \"r'ship\", 'btwn', 'homo', 'biphob', 'comment', 'certain', 'disciplin', 'incl', 'european', 'lang', 'lit', 'educ', ':(', 'fresherstofin']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['im', 'sure', 'tho', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['noo', \"he'\", 'random', 'person', 'class', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'much', 'pippa', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['live', 'northern', 'ireland', ':)', 'sight', 'seen']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'bam', 'follow', 'bestfriend', 'love', 'lot', ':)', 'see', 'warsaw', '<3', 'love', '<3', 'x30']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['link', 'plz', ':p']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['come', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['would', 'drop', '10', 'wine', 'bottl', 'spill', 'wine', 'work', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sad', 'see', 'becom', 'lab', 'place', 'take', 'ovh', 'noth', 'ovh', 'support', 'suck', 'much', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'hungri', 'cant', 'make', 'food', 'sinc', 'brother', 'sleep', 'live', 'room', '>:(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['rlyyi', 'wanna', 'get', 'septum', 'pierc', 'mum', 'let', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['like', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeah', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nearli', '50', 'dead', '100', 'wound', 'report', 'bomb', 'attack', 'iraq', 'one', 'week', '300', 'human', 'be', 'kill', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['exit', 'get', 'macci', 'breakfast', '10', 'minut', 'late', 'get', 'burger', 'chees', 'bite', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['white', 'thing', 'ngeze', 'turn', ':(', 'white', 'parti', \"i'v\", 'noth', 'cocktaili', 'classi', 'üò≠', 'üò≠', 'üò≠', 'stress']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['aw', 'yk', 'wish', 'could', ':(', 'go', 'sleep', 'babe', 'need', 'resi']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pretti', 'amaz', 'hell', '13', 'oh', 'yeah', 'play', 'video', 'game', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'peep', ':)', 'happyweekend']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['morn', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'wendi', 'provid', 'fresh', 'water', 'area', 'provid', 'drainag', 'servic', 'ami', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ÔøΩ', 'pandimension', 'see', 'manifest', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['anoth', 'jumpsuit', '...', 'bt', 'denim', '...', ':)', 'flipkartfashionfriday']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['jordanian', 'number', '00962778381', '838', 'whatsapp', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['1', '100', 'children', 'psychopath', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['valentin', 'et', 'al', '2009', 'found', 'relationship', 'homo', 'biphob', 'comment', 'certain', 'disciplin', 'incl', 'european', 'languag', 'lit', 'educ', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pleas', 'releas', 'new', 'album', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'v\", 'fan', '5so', 'sinc', 'earli', 'june', 'last', 'year', ':(', 'realli', 'hate', '‚Ä¶', 'get', 'upset', 'parent']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['someon', 'gift', 'calibraska', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['home', 'alon', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['feel', 'sick', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['read', 'donat', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"can't\", 'anymor', \"i'm\", 'sorri', 'bc', 'anymor', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happi', 'day', 'get', 'tkt', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cold', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'love', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['perfect', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happi', 'friday', 'whn', 'u', 'birthday', 'u', 'younger', 'sister', ':D', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['tire', 'yesterday', 'choroo', 'ga', 'nahi', 'us', 'ko', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hey', 'talk', 'friend', 'concert', 'say', 'goodby', 'cool', 'meet', 'two', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['quit', ':)', 'sell', 'young', 'understand', 'muntu']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hate', 'weekend', 'mean', 'work', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oh', \"doedn't\", 'sound', 'good', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['isco', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['new', 'guitar', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['chanc', 'find', 'black', 'guin', 'wallet', 'venu', 'last', 'night', 'show', 'lost', 'wallet', ':(', ':(', ':(', 'thank']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stomachach', 'bc', 'slice', 'cake', ':(', '¬¥', '‡ΩÄ', '„Äç', '‚à†', '):', '‚Ä¶']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['suck', 'want', 'go', 'home', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['need', 'reset', 'phone', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['kindli', 'check', 'email', \"we'r\", 'done', 'send', 'invoic', 'sinc', 'kanina', 'pong', 'umaga', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['welcom', 'jim', 'made', 'chuckl', 'train', 'morn', ':-)', 'happi', 'friday']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['good', 'night', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['listen', ':-)', 'buddi', ':-)', 'u', 'wanna', 'play', 'like', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['invit', 'pin', '5878e503', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['appreci', 'recent', 'retweet', 'great', 'friday', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['long', 'sublimin', 'messag', ':)', 'h', 'eatmeat', 'brewproject', 'lovenafianna']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['scheme', 'scheme', 'plan', 'yet', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow', 'thank', 'pleas', 'follow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['three', 'favourit', 'pictur', 'took', 'brad', ':-(', 'miss', 'tour', 'much']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ye', 'thank', 'u', 'oh', 'damn', 'hella', 'suck', ':-(', 'least', 'u', 'realli', 'good', 'time', \"that'\", 'matter']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ice', 'cream', 'van', 'busi', ':(', 'mayb', 'later', '...', 'ubericecream']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['eye', 'open', 'realis', 'thing', 'like', 'happen', 'scari', 'bad', 'someon', 'would', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'sound', 'like', 'reach', 'level']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['appar', 'shower', 'friday', 'subway', 'one', 'kind', 'enough', 'tell', 'cool', 'guy', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step     90: train CrossEntropyLoss |  0.05699894\n",
      "List of words from the processed tweet:\n",
      "['happi', 'friday', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"that'\", 'true', 'inde', 'curvi', 'rout', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cute', 'boy', 'jule', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['one', 'last', 'long', 'lol', 'block', 'alreadi', 'must', 'nice', 'tweet', 'hate', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['favorit', 'appl', 'sauc', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['far', 'lfc', 'fan', 'make', 'expert', 'spot', 'mental', 'weak', 'lack', 'consist', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['abp', 'news', 'ka', 'articl', 'read', 'kiya', ':D', 'bajrangibhaijaanhighestweek', '1']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nd', 'go', 'expir', 'next', 'year', 'dat', 'mtn', '6gb', 'u', ':D', 'realli', 'like']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['none', 'hide', 'behind', 'shower', 'curtain', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['payday', 'feel', 'like', 'payhour', 'gone', 'bill', 'immedi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['end', 'state', 'last', 'night', ':(', ':(', ':(', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['dream', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'tire', ':(', 'got', 'good', 'sleep']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['want', 'rt', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['guy', 'add', 'snapchat', 'amargolonnard', 'snapchat', 'kikmeboy', 'sexi', 'french', 'dirtykik', 'newmus', 'sexcam', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['alon', ':-(', \":'(\", ':-\\\\']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step     90: eval  CrossEntropyLoss |  0.01778970\n",
      "Step     90: eval          Accuracy |  1.00000000\n",
      "List of words from the processed tweet:\n",
      "['call', 'throwbackfriday', 'christen', 'ki', 'yaad', 'aa', 'gayi', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['whenev', 'anoth', 'movi', 'night', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['awesom', 'catch', 'buzz', 'video', 'burnt', 'brian', 'smoke', 'mist', 'cool', 'extrem', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'helen', 'keep', 'touch', 'ever', 'want', 'film', 'blood', 'servic', 'know', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['certainli', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'manthan', 'stay', 'tune', 'us', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oh', 'yeah', 'one', 'major', 'sugarrush', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cute', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow', 'back', 'thank', 'pleas', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['huhu', 'know', ':-(', 'thank', 'satya', 'xx']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['...', ':(', 'hate', 'jong', 'dongwoo', 'photocard', \"i'v\", 'get', 'wh', 'dw', 'sj']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['cant', 'believ', 'miss', 'follow', 'spree', 'today', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', 'al', 'kati', 'zaz', 'ami', 'lot', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lart', 'rt', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"can't\", 'open', ':-(', 'stockholm']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happi', 'friday', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lol', 'selo', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fresh', 'beauti', 'smile', 'mother', 'determin', 'make', 'day', ':)', 'maxfreshmov']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yayi', 'good', 'work', \"can't\", 'wait', 'campu', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wohoo', '...', 'colour', 'perfect', ':)', 'wsalelov']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hardli', 'surpris', 'rose', 'inspir', 'much', 'emot', 'poetri', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lol', 'whole', 'famili', 'threaten', 'daili', 'basi', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stop', 'get', 'hate', 'whatev', 'kind', 'is.h', 'made', 'mistak', 'understood', 'stop', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['controversi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['make', 'acc', 'feel', 'bad', ':(', \"that'\", 'rude']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"cathy'\", 'stori', 'basic', 'black', 'guy', 'sing', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hope', 'pray', \"he'\", 'good', 'hand', \"i'm\", 'far', 'abl', 'see', \"can't\", 'call', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'tell', ':(', 'shaa', 'allah', 'na', 'forc', 'chang', 'phone', 'mana', 'üòí']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['isco', 'close', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thought', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['time', 'jhalak', 'bore', '. ..', 'concept', 'bakwa', '. ..', 'old', 'concept', 'much', 'better', 'interest', '. ...', ':-(', ':-(', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['atleast', 'honest', '..', 'still', 'get', 'bitch', 'tho', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ye', 'surpris', 'see', 'bit', 'rain', 'today', '...', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oh', 'certainli', 'watch', 'anyway', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['‚ñ™', 'Ô∏èbea', 'miller', 'lockscreen', '‚ñ™', 'Ô∏è', 'rt', 'fav', 'want', 'mbf', 'dm', ':)', 'keesh']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['call', 'anyth', 'along', 'line', 'jojo', 'chainsaw', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['think', \"it'll\", 'sam', 'would', 'love', \"he'\", 'made', 'biggest', 'improv', 'start', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bu', 'girl', 'use', 'long', 'cardi', 'almost', 'everyday', 'everytim', 'see', 'u', 'laugh', 'smile', ':)', 'cute', 'smile']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':)', 'choic', 'food', 'left', 'individu']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['aww', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wait', 'noth', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"can't\", 'afford', 'pleas', 'give', 'first', 'full', 'mashup', 'pack', 'pleas', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'care', '...', 'namecheck', 'bring', 'like', 'dog', 'whistl', 'without']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['jackson', 'look', 'exhaust', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['school', 'start', 'next', 'monday', 'ohgod', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pl', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hey', 'mayb', 'get', 'pictur', 'warm-up', 'gone', 'sign', 'session', 'arriv', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['top', 'cuti', 'bohol', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['haha', 'whatev', 'jess', 'know', 'thought', 'surri', 'hill', 'near', 'botani', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'share', 'articl', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'play', 'brain', 'dot', 'braindot']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['brand', 'want', 'review', 'beauti', 'product', 'blog', 'mail', 'ladolcevitainluxembourg@hotmail.com', 'prrequest', 'journorequest', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['head', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'follow', 'welcom', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['download', 'uber', 'mobil', 'applic', 'place', 'order', ':)', 'enjoy']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"there'\", 'free', 'wifi', 'network', 'avail', 'within', 'rang', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['snapchat', 'lynettelow', 'snapchat', 'kikhorni', 'interraci', 'bestoftheday', 'nake', 'goodmus', 'sexysasunday', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['blood', 'plagu', 'pit', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', 'baao', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"can't\", 'tell', 'sad', 'book', 'anoth', 'cruis', 'line', 'could', 'done', 'doubl', 'wail', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['im', 'almost', 'end', 'far', 'cri', '4', 'journey', ':(', 'similar', 'game', 'recommend', 'say', 'cod', 'gta', 'cut', 'bitch']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"jimin'\", 'fancaf', 'post', 'yesterday', '>:(', 'gone']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['right', 'right', ':(', 'alway', 'good', 'memori', 'guess', 'üòî']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['still', 'suck', 'edit', 'turn', 'someth', 'special', 'owe', 'guy', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['friday', 'mood', ':)', 'sourc', 'ellen', 'degener', 'show']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ye', 'mam', ':)', 'wsalelov']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['fback', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['go', 'well', 'oppayaa', \"how'\", ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['common', 'celebr', 'never', 'fail', 'get', 'good', 'mood', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow', 'mixcloud', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['mean', 'alreadi', 'dream', 'come', 'true', 'notic', 'that', 'enough', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['aku', 'chelsea', 'koe', 'emyu', 'see', 'miss', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'u', 'okay']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['know', \"i'm\", 'blockjam', 'sinc', \"schedule'\", 'whack', 'right', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['last', 'dick', 'pic', 'got', 'aw', 'ruin', 'walnut', 'whip', 'life', ':-(', 'also', 'name', 'turn', 'wife', 'follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stress', 'upset', 'day', 'yesterday', 'uk', 'govern', 'suck', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'onlin', 'day', 'ago', 'like', '50', 'notif', ':(', '2', 'day', 'dammit', 'stop', 'guy', 'lol', 'jk', '<3', 'u']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hellooo', 'friday', 'meet', ':)', 'almost', 'weekend', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happi', 'long', 'time', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hurray', 'look', 'forward', 'meet', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['gorgeou', 'son', 'morocco', 'last', 'year', 'special', 'holiday', 'peru', 'nxt', 'year', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bbi', 'might', 'take', 'one', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thought', 'might', 'saw', 'love', 'vers', 'butterfli', 'part', 'bonu', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'girl', ':)', 'wish', 'happi', 'friday.and', 'magnific', 'fantast', 'w', 'e', '‚ô°', '‚ô•', '‚ô°', '‚ô•']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['exactli', ':)', 'mauliat', 'ito']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bot', 'follow', 'plz', 'send', 'mention', \"can't\", 'notic', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yass', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['diz', 'person', 'account', 'also', 'fan', 'acc', 'link', 'biooo', ':-(', 'ay', 'taray', 'yumu-youtub']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['megan', 'fell', 'asleep', 'alreadi', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['10', 'year', \"old'\", 'cat', 'first', 'loss', 'pet', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['confus', 'tho', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['25', ':(', '25x30']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lol', 'realli', 'unlik', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['toni', 'stark', ':)', 'incred', 'toni', 'pictur']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ye', 'avail', ':D', 'wanna', 'join']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['birthday', 'week', 'today', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'mom', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['keep', 'calm', 'stay', 'kepo', ':)', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bet', 'well', 'get', 'sleep', '9pm', 'shift', '9am', 'quit', 'hard', 'work', 'thank', ':)', 'xx']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['well', 'done', 'good', 'get', 'thing', 'way', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['steak', 'lot', 'younger', 'fat', 'make', 'g', 'g', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'suck', 'tri', 'vitamin', 'e', 'oil', 'massag', 'everyday', 'make', 'healthier', 'easier', 'stretch']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', 'someon', 'come', 'ill', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['forev', 'food', 'home', 'food', 'school', 'even', 'nice', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['seen', '2', 'peregrin', 'today', '8.40', 'one', 'pigeon', 'feet', 'lucki', 'see', 'flew', 'tram', 'stop', 'hav', 'phone', 'take', 'pic', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['u', 'never', 'call', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'updat']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['snapchat', 'lisah', '19', 'snapchat', 'kikm', 'kikmeboy', 'woman', 'eboni', 'weloveyounamjoon', 'sexi', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['nice', 'connect', 'yael', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['id', 'conflict', 'thank', 'help', ':D', \"here'\", 'screenshot', 'work']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['never', 'bad', 'time', 'bath', 'us', 'rubber', 'duck', ':-)', 'quacketyquack']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['actual', 'gonna', 'start', '1', 'pm', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['welcom', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['huxley', 'look', 'cool', 'basket', 'good', 'choic', 'favourit', 'shop', ':)', 'ben']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['final', 'fantasi', 'vii', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['mr2', 'climb', 'give', 'cuddl', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['want', 'brace', 'back', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['could', ':(', 'drive', 'pass', 'advert', 'everyday', 'go', 'work', 'determin', 'roseburi', 'familyhom']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ye', 'your', 'right', '..', 'sad', 'mani', 'intoler', 'scare', 'xenophob', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hush', 'gurli', 'bring', 'purti', 'mouth', 'closer', 'made', 'shiver', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['due', 'date', 'fix', 'error', '59', 'friend', \"can't\", 'still', 'play', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['a9', 'would', 'sad', 'world', 'women', ':(', 'wsalelov']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['airport', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'hug', 'hope', 'rest', 'soon']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wagga', 'wagga', 'phase', '2', 'coursework', 'come', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['soo', 'hot', 'sunnyday', 'sunshin', 'sun', 'smile', 'lol', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['long', 'hair', 'men', 'make', 'world', 'better', 'place']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lol', 'one', 'word', 'david', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'god', 'friday', ':)', 'jgh']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['get', '4', 'need', 'homework', 'prob', 'upload', 'tomorrow', 'see', 'time', '2', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['goodnight', 'fuck', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['anonym', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['look', 'fun', 'snapchat', 'jillcruz', '18', 'snapchat', 'kikhorni', 'talk', 'teen', 'oralsex', 'batalladelosgallo', 'kikhorni', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stomach', 'hurt', 'much', 'want', 'cri', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['...', 'set', 'two', 'pabebegirl', 'aw', 'pabeb', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['alon', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hate', 'u', '>:(', 'h_my_k']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['feel', 'like', 'movi', 'popcorn', 'today', 'cinema', '...', 'one', 'go', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['car', 'batteri', 'expens', ':(', 'hope', 'get', 'new', 'life']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['great', 'attend', 'digitalexet', 'last', 'night', 'hear', 'interest', 'talk', 'digitalmarket', 'sociamedia', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['randomli', 'book', 'ticket', 'go', 'buckinghampalac', \"can't\", 'wait', ':)', 'good']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'ff', ':)', 'x']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bet', 'excit', ':)', \":')\", 'xxx', 'good', 'luck', 'hope', 'get', 'number', 'one']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeah', 'boii', 'look', 'arriv', 'post', 'morn', 'latest', 'smashingbook', '5', 'new', 'bibl', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'news', 'man', ':)', 'utc']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['awak', ':p']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['save', '21pic', 'master', 'nim', 'say', \"there'r\", '22pic', 'ask', 'mani', 'pic', 'see', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['s2', 'air', 'spain', 'delay', 'kill', 'us', '...', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['want', 'fuck', 'day', 'collar', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['feel', 'sir', ':(', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['everyon', 'stun', '...', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['aaaaaaaaah', 'sound', 'kany', 'make', 'hey', 'mama', 'cute', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['get', '7th', 'win', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['itong', 'shirt', 'oh', ':(', 'want', 'thaaat', ':(', 'ctto']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['poor', 'kiddo', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step    100: train CrossEntropyLoss |  0.03663783\n",
      "List of words from the processed tweet:\n",
      "['right', 'thx', 'andrew', 'great', 'day', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['bom-dia', ':)', 'apod', 'ultraviolet', 'ring', 'm31', '2015', 'jul', '24']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wish', 'happi', 'birthday', 'awesom', 'fun', 'fill', 'day', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'like', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['west', 'countri', 'weekend', 'next', 'ride', 'day', 'tuesday', ':)', 'sunni', 'weekend', 'around']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['love', 'great', 'collag', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['call', 'night', 'go', 'sleep', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ye', 'alreadi', 'ad', 'pleas', 'check', 'final', 'design', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['wish', 'could', 'hear', 'food', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['want', 'watch', 'paper', 'town', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['miss', '11:11', ':(', 'btw', 'wish', 'follow', '‚ô•', 'loveyoutilltheendcart', '‚ò∫']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['appar', 'isnt', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['care', 'muslim', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['omg', 'r', 'hahah', ':(', 'omg', 'debut', 'mo', 'na', 'next', 'year', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['kinda', 'miss', 'pamela', 'good', 'anna', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['weird', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Step    100: eval  CrossEntropyLoss |  0.00210550\n",
      "Step    100: eval          Accuracy |  1.00000000\n"
     ]
    }
   ],
   "source": [
    "training_loop = train_model(model, train_task, eval_task, 100, output_dir_expand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wxn24gyx1Xpd"
   },
   "source": [
    "##### Expected output (Approximately)\n",
    "```CPP\n",
    "Step      1: train CrossEntropyLoss |  0.88939196\n",
    "Step      1: eval  CrossEntropyLoss |  0.68833977\n",
    "Step      1: eval          Accuracy |  0.50000000\n",
    "Step     10: train CrossEntropyLoss |  0.61036736\n",
    "Step     10: eval  CrossEntropyLoss |  0.52182281\n",
    "Step     10: eval          Accuracy |  0.68750000\n",
    "Step     20: train CrossEntropyLoss |  0.34137666\n",
    "Step     20: eval  CrossEntropyLoss |  0.20654774\n",
    "Step     20: eval          Accuracy |  1.00000000\n",
    "Step     30: train CrossEntropyLoss |  0.20208922\n",
    "Step     30: eval  CrossEntropyLoss |  0.21594886\n",
    "Step     30: eval          Accuracy |  0.93750000\n",
    "Step     40: train CrossEntropyLoss |  0.19611198\n",
    "Step     40: eval  CrossEntropyLoss |  0.17582777\n",
    "Step     40: eval          Accuracy |  1.00000000\n",
    "Step     50: train CrossEntropyLoss |  0.11203773\n",
    "Step     50: eval  CrossEntropyLoss |  0.07589275\n",
    "Step     50: eval          Accuracy |  1.00000000\n",
    "Step     60: train CrossEntropyLoss |  0.09375446\n",
    "Step     60: eval  CrossEntropyLoss |  0.09290724\n",
    "Step     60: eval          Accuracy |  1.00000000\n",
    "Step     70: train CrossEntropyLoss |  0.08785903\n",
    "Step     70: eval  CrossEntropyLoss |  0.09610598\n",
    "Step     70: eval          Accuracy |  1.00000000\n",
    "Step     80: train CrossEntropyLoss |  0.08858261\n",
    "Step     80: eval  CrossEntropyLoss |  0.02319432\n",
    "Step     80: eval          Accuracy |  1.00000000\n",
    "Step     90: train CrossEntropyLoss |  0.05699894\n",
    "Step     90: eval  CrossEntropyLoss |  0.01778970\n",
    "Step     90: eval          Accuracy |  1.00000000\n",
    "Step    100: train CrossEntropyLoss |  0.03663783\n",
    "Step    100: eval  CrossEntropyLoss |  0.00210550\n",
    "Step    100: eval          Accuracy |  1.00000000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVMcsw2kjCX9"
   },
   "source": [
    "<a name=\"4.2\"></a>\n",
    "## 4.2  Practice Making a prediction\n",
    "\n",
    "Now that you have trained a model, you can access it as `training_loop.model` object. We will actually use `training_loop.eval_model` and in the next weeks you will learn why we sometimes use a different model for evaluation, e.g., one without dropout. For now, make predictions with your model.\n",
    "\n",
    "Use the training data just to see how the prediction process works.  \n",
    "- Later, you will use validation data to evaluate your model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "WAMgXWY4jCX-",
    "outputId": "7d732b79-6528-49cf-a78a-2f3ee4891681"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words from the processed tweet:\n",
      "['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hey', 'jame', 'odd', ':/', 'pleas', 'call', 'contact', 'centr', '02392441234', 'abl', 'assist', ':)', 'mani', 'thank']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['listen', 'last', 'night', ':)', 'bleed', 'amaz', 'track', 'scotland']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['congrat', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yeaaah', 'yipppi', 'accnt', 'verifi', 'rqst', 'succeed', 'got', 'blue', 'tick', 'mark', 'fb', 'profil', ':)', '15', 'day']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['one', 'irresist', ':)', 'flipkartfashionfriday']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['like', 'keep', 'love', 'custom', 'wait', 'long', 'hope', 'enjoy', 'happi', 'friday', 'lwwf', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['second', 'thought', '‚Äô', 'enough', 'time', 'dd', ':)', 'new', 'short', 'enter', 'system', 'sheep', 'must', 'buy']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hopeless', 'tmr', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['everyth', 'kid', 'section', 'ikea', 'cute', 'shame', \"i'm\", 'nearli', '19', '2', 'month', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['heart', 'slide', 'wast', 'basket', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['‚Äú', 'hate', 'japanes', 'call', 'bani', ':(', ':(', '‚Äù']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['dang', 'start', 'next', 'week', 'work', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['oh', 'god', 'babi', 'face', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['make', 'smile', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['work', 'neighbour', 'motor', 'ask', 'said', 'hate', 'updat', 'search', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "The batch is a tuple of length 3 because position 0 contains the tweets, and position 1 contains the targets.\n",
      "The shape of the tweet tensors is (16, 15) (num of examples, length of tweet tensors)\n",
      "The shape of the labels is (16,), which is the batch size.\n",
      "The shape of the example_weights is (16,), which is the same as inputs/targets size.\n"
     ]
    }
   ],
   "source": [
    "# Create a generator object\n",
    "tmp_train_generator = train_generator(16)\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_train_generator)\n",
    "\n",
    "# Position 0 has the model inputs (tweets as tensors)\n",
    "# position 1 has the targets (the actual labels)\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
    "\n",
    "print(f\"The batch is a tuple of length {len(tmp_batch)} because position 0 contains the tweets, and position 1 contains the targets.\") \n",
    "print(f\"The shape of the tweet tensors is {tmp_inputs.shape} (num of examples, length of tweet tensors)\")\n",
    "print(f\"The shape of the labels is {tmp_targets.shape}, which is the batch size.\")\n",
    "print(f\"The shape of the example_weights is {tmp_example_weights.shape}, which is the same as inputs/targets size.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "5XoxD6u5jCX_",
    "outputId": "d857441c-0977-411f-a8de-2037820d8fa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prediction shape is (16, 2), num of tensor_tweets as rows\n",
      "Column 0 is the probability of a negative sentiment (class 0)\n",
      "Column 1 is the probability of a positive sentiment (class 1)\n",
      "\n",
      "View the prediction array\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([[-4.9417334e+00, -7.1678162e-03],\n",
       "             [-6.5846415e+00, -1.3823509e-03],\n",
       "             [-5.4463043e+00, -4.3215752e-03],\n",
       "             [-4.3487482e+00, -1.3007164e-02],\n",
       "             [-4.9131694e+00, -7.3764324e-03],\n",
       "             [-4.7097692e+00, -9.0477467e-03],\n",
       "             [-5.2801600e+00, -5.1045418e-03],\n",
       "             [-4.1103225e+00, -1.6538620e-02],\n",
       "             [-1.8327236e-03, -6.3028107e+00],\n",
       "             [-4.7376156e-03, -5.3545618e+00],\n",
       "             [-3.4697056e-03, -5.6654320e+00],\n",
       "             [-1.1444092e-05, -1.1379558e+01],\n",
       "             [-1.0051131e-02, -4.6050973e+00],\n",
       "             [-1.0130405e-03, -6.8951964e+00],\n",
       "             [-6.1047077e-03, -5.1017356e+00],\n",
       "             [-7.4422359e-03, -4.9043016e+00]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feed the tweet tensors into the model to get a prediction\n",
    "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
    "print(f\"The prediction shape is {tmp_pred.shape}, num of tensor_tweets as rows\")\n",
    "print(\"Column 0 is the probability of a negative sentiment (class 0)\")\n",
    "print(\"Column 1 is the probability of a positive sentiment (class 1)\")\n",
    "print()\n",
    "print(\"View the prediction array\")\n",
    "tmp_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0aJpFcyljCYB"
   },
   "source": [
    "To turn these probabilities into categories (negative or positive sentiment prediction), for each row:\n",
    "- Compare the probabilities in each column.\n",
    "- If column 1 has a value greater than column 0, classify that as a positive tweet.\n",
    "- Otherwise if column 1 is less than or equal to column 0, classify that example as a negative tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "6wJHv0TNjCYC",
    "outputId": "0367db61-6e29-44b5-be45-7534600e6931"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg log prob -4.9417\tPos log prob -0.0072\t is positive? True\t actual 1\n",
      "Neg log prob -6.5846\tPos log prob -0.0014\t is positive? True\t actual 1\n",
      "Neg log prob -5.4463\tPos log prob -0.0043\t is positive? True\t actual 1\n",
      "Neg log prob -4.3487\tPos log prob -0.0130\t is positive? True\t actual 1\n",
      "Neg log prob -4.9132\tPos log prob -0.0074\t is positive? True\t actual 1\n",
      "Neg log prob -4.7098\tPos log prob -0.0090\t is positive? True\t actual 1\n",
      "Neg log prob -5.2802\tPos log prob -0.0051\t is positive? True\t actual 1\n",
      "Neg log prob -4.1103\tPos log prob -0.0165\t is positive? True\t actual 1\n",
      "Neg log prob -0.0018\tPos log prob -6.3028\t is positive? False\t actual 0\n",
      "Neg log prob -0.0047\tPos log prob -5.3546\t is positive? False\t actual 0\n",
      "Neg log prob -0.0035\tPos log prob -5.6654\t is positive? False\t actual 0\n",
      "Neg log prob -0.0000\tPos log prob -11.3796\t is positive? False\t actual 0\n",
      "Neg log prob -0.0101\tPos log prob -4.6051\t is positive? False\t actual 0\n",
      "Neg log prob -0.0010\tPos log prob -6.8952\t is positive? False\t actual 0\n",
      "Neg log prob -0.0061\tPos log prob -5.1017\t is positive? False\t actual 0\n",
      "Neg log prob -0.0074\tPos log prob -4.9043\t is positive? False\t actual 0\n"
     ]
    }
   ],
   "source": [
    "# turn probabilites into category predictions\n",
    "tmp_is_positive = tmp_pred[:,1] > tmp_pred[:,0]\n",
    "for i, p in enumerate(tmp_is_positive):\n",
    "    print(f\"Neg log prob {tmp_pred[i,0]:.4f}\\tPos log prob {tmp_pred[i,1]:.4f}\\t is positive? {p}\\t actual {tmp_targets[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TywSi02cjCYF"
   },
   "source": [
    "Notice that since you are making a prediction using a training batch, it's more likely that the model's predictions match the actual targets (labels).  \n",
    "- Every prediction that the tweet is positive is also matching the actual target of 1 (positive sentiment).\n",
    "- Similarly, all predictions that the sentiment is not positive matches the actual target of 0 (negative sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6X_0K_EjCYF"
   },
   "source": [
    "One more useful thing to know is how to compare if the prediction is matching the actual target (label).  \n",
    "- The result of calculation `is_positive` is a boolean.\n",
    "- The target is a type trax.fastmath.numpy.int32\n",
    "- If you expect to be doing division, you may prefer to work with decimal numbers with the data type type trax.fastmath.numpy.int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "CQgx_ar9jCYG",
    "outputId": "16b00f0d-9f1a-4602-f38e-b58ea3ab52a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of booleans\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([ True,  True,  True,  True,  True,  True,  True,  True,\n",
       "             False, False, False, False, False, False, False, False],            dtype=bool)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of integers\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array of floats\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray([1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "             0.], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View the array of booleans\n",
    "print(\"Array of booleans\")\n",
    "display(tmp_is_positive)\n",
    "\n",
    "# convert boolean to type int32\n",
    "# True is converted to 1\n",
    "# False is converted to 0\n",
    "tmp_is_positive_int = tmp_is_positive.astype(np.int32)\n",
    "\n",
    "\n",
    "# View the array of integers\n",
    "print(\"Array of integers\")\n",
    "display(tmp_is_positive_int)\n",
    "\n",
    "# convert boolean to type float32\n",
    "tmp_is_positive_float = tmp_is_positive.astype(np.float32)\n",
    "\n",
    "# View the array of floats\n",
    "print(\"Array of floats\")\n",
    "display(tmp_is_positive_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "O_GmTvaTjCYH",
    "outputId": "218a3de9-38b4-44b7-bdd4-a7e8306b41d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 2)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8gJ3n4UljCYJ"
   },
   "source": [
    "Note that Python usually does type conversion for you when you compare a boolean to an integer\n",
    "- True compared to 1 is True, otherwise any other integer is False.\n",
    "- False compared to 0 is True, otherwise any ohter integer is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "GbKFCf0njCYJ",
    "outputId": "c18362fd-4202-47b5-cfb6-a6797863fff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True == 1: True\n",
      "True == 2: False\n",
      "False == 0: True\n",
      "False == 2: False\n"
     ]
    }
   ],
   "source": [
    "print(f\"True == 1: {True == 1}\")\n",
    "print(f\"True == 2: {True == 2}\")\n",
    "print(f\"False == 0: {False == 0}\")\n",
    "print(f\"False == 2: {False == 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jh5XfviCjCYK"
   },
   "source": [
    "However, we recommend that you keep track of the data type of your variables to avoid unexpected outcomes.  So it helps to convert the booleans into integers\n",
    "- Compare 1 to 1 rather than comparing True to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37PNk6IzjCYL"
   },
   "source": [
    "Hopefully you are now familiar with what kinds of inputs and outputs the model uses when making a prediction.\n",
    "- This will help you implement a function that estimates the accuracy of the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fRRrgOHJgrhI"
   },
   "source": [
    "<a name=\"5\"></a>\n",
    "# Part 5:  Evaluation  \n",
    "\n",
    "<a name=\"5.1\"></a>\n",
    "## 5.1  Computing the accuracy on a batch\n",
    "\n",
    "You will now write a function that evaluates your model on the validation set and returns the accuracy. \n",
    "- `preds` contains the predictions.\n",
    "    - Its dimensions are `(batch_size, output_dim)`.  `output_dim` is two in this case.  Column 0 contains the probability that the tweet belongs to class 0 (negative sentiment). Column 1 contains probability that it belongs to class 1 (positive sentiment).\n",
    "    - If the probability in column 1 is greater than the probability in column 0, then interpret this as the model's prediction that the example has label 1 (positive sentiment).  \n",
    "    - Otherwise, if the probabilities are equal or the probability in column 0 is higher, the model's prediction is 0 (negative sentiment).\n",
    "- `y` contains the actual labels.\n",
    "- `y_weights` contains the weights to give to predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2hdfk3LEjCYL"
   },
   "source": [
    "<a name=\"ex07\"></a>\n",
    "### Exercise 07\n",
    "Implement `compute_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WBqaN5f9grhJ"
   },
   "outputs": [],
   "source": [
    "# UNQ_C7 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: compute_accuracy\n",
    "def compute_accuracy(preds, y, y_weights):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        preds: a tensor of shape (dim_batch, output_dim) \n",
    "        y: a tensor of shape (dim_batch,) with the true labels\n",
    "        y_weights: a n.ndarray with the a weight for each example\n",
    "    Output: \n",
    "        accuracy: a float between 0-1 \n",
    "        weighted_num_correct (np.float32): Sum of the weighted correct predictions\n",
    "        sum_weights (np.float32): Sum of the weights\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    # Create an array of booleans, \n",
    "    # True if the probability of positive sentiment is greater than\n",
    "    # the probability of negative sentiment\n",
    "    # else False\n",
    "    is_pos =  preds[:, 1] > preds[:, 0] \n",
    "\n",
    "    # convert the array of booleans into an array of np.int32\n",
    "    is_pos_int = is_pos.astype(np.int32)\n",
    "    \n",
    "    # compare the array of predictions (as int32) with the target (labels) of type int32\n",
    "    correct = is_pos_int==y\n",
    "\n",
    "    # Count the sum of the weights.\n",
    "    sum_weights = np.sum(y_weights)\n",
    "    \n",
    "    # convert the array of correct predictions (boolean) into an arrayof np.float32\n",
    "    correct_float = correct.astype(np.float32)\n",
    "    \n",
    "    # Multiply each prediction with its corresponding weight.\n",
    "    weighted_correct_float = correct_float * y_weights\n",
    "\n",
    "    # Sum up the weighted correct predictions (of type np.float32), to go in the\n",
    "    # denominator.\n",
    "    weighted_num_correct = np.sum(weighted_correct_float)\n",
    " \n",
    "    # Divide the number of weighted correct predictions by the sum of the\n",
    "    # weights.\n",
    "    accuracy = weighted_num_correct / sum_weights\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return accuracy, weighted_num_correct, sum_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "1c7ZOeO0jCYN",
    "outputId": "a2a7414d-0168-4c55-b31f-bf263718c330"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words from the processed tweet:\n",
      "['bro', 'u', 'wan', 'cut', 'hair', 'anot', 'ur', 'hair', 'long', 'liao', 'bo', 'sinc', 'ord', 'liao', 'take', 'easi', 'lor', 'treat', 'save', 'leav', 'longer', ':)', 'bro', 'lol', 'sibei', 'xialan']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['back', 'thnx', 'god', \"i'm\", 'happi', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thought', 'ear', 'malfunct', 'thank', 'good', 'clear', 'one', 'apolog', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stuck', 'centr', 'right', 'clown', 'right', 'joker', 'left', '...', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happi', 'friday', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow', ':)', 'x']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['teenchoic', 'choiceinternationalartist', 'superjunior', 'fight', 'oppa', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['birthday', 'today', 'birthday', 'wish', 'hope', \"there'\", 'good', 'news', 'ben', 'soon', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['good', 'morn', ':-)', 'friday', '\\U000fec00', 'plan', 'day', 'current', 'play', 'shop', '...']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happi', 'friday', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['3', 'good', 'nigth', ':)', 'estoy', 'escuchando', 'enemi', 'god']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['actual', 'bye', 'bye', 'inde', 'go', 'take', 'drama', 'elsewher', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ff', 'readi', 'weekend', ':)', 'smile']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['work', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':)', 'mood', 'bipolar']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hahahahahahahahahahahahahaha', 'die', ':)', 'liter', 'front', 'like']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['yoohoo', 'shatter', 'record', 'bajrangibhaijaanstorm', 'superhappi', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"who'\", 'awak', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happi', 'friday', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['pleasur', 'doll', 'thank', 'wonder', 'energi', 'class', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['f', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['delight', \"m'dear\", ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hi', 'emma', ':-)', 'ask', 'bellybutton', 'inni', 'outi']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['followback', 'beauti', 'girl', 'today', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['lyka', 'followback', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['alrd', 'joy', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['ourdaughtersourprid', 'dhan', 'dhan', 'satguru', 'tera', 'hi', 'aasra', '...', 'mani', 'congratul', 'pita', 'g', '...', 'keep', 'bless', 'alway', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thank', 'ye', \"let'\", 'hope', 'work', 'miss', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['keeo', 'guess', 'what', 'behind', 'white', 'cover', ':)', 'special', '..', 'happi', 'bday', 'darl', '‚Ä¶']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happi', 'friday', 'everyon', ':)', 'big', 'followfriday']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['awkward', 'moment', 'name', 'akarshan', 'end', 'stay', 'singl', ':D', 'foreveralon']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['help', '...', 'stop', 'cri', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['otl', 'nevermind', ':(', 'least', 'got', 'jeon']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['soon', 'tweet', 'plant', 'claw', 'thigh', 'traction', 'zoom', 'away', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['damnit', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['use', 'pri', 'pv', '...', 'wish', 'could', 'reliv', 'day', 'becom', 'nyc', 'pv', 'buy', 'way', 'commun', 'nyc', 'usa', 'klm', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['realli', 'hot', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['monday', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['go', 'stop', 'breakfast', 'earli', 'might', 'want', 'remov', '11am', 'websit', 'even', \"mcd'\", 'pull', 'trick', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['mean', 'way', ':(', '3rd', 'load', 'hung']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[':(', 'wtf', 'suppos', 'without']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['headach', 'strike', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['english', 'weather', 'need', 'fix', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['live', 'fam', 'bam', 'cough', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['absolut', 'gut', 'jame', 'bay', 'ticket', 'sold', 'manchest', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['seventh', 'spot', 'na', 'lang', ':(', 'otwolgrandtrail']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['splendour', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['omg', 'swedish', 'hair', 'metal', 'legend', 'h√§irf√∏rc', 'studio', 'leav', 'nooo', ':(', 'givecodpieceach']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['sore', 'alic', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'readi', 'work', 'yet', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['someon', 'unfav', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stile', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['funni', 'mo', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['someon', 'explain', 'sandrabland', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['dont', 'sad', ':(', 'ili']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['go', 'spend', 'night', 'pragu', 'leav', 'tomorrow', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['one', 'friend', 'follow', 'littl', 'heart', 'attack', 'im', 'sorri', 'your', 'block', ':(', 'sadi']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "[\"i'm\", 'annoy', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['consid', 'lucki', 'favourit', 'charact', 'even', 'make', 'season', 'final', '100', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['boo', 'alreadi', 'turn', 'ye', '915', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['itun', \"i'm\", 'wait', 'notanapolog', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['layout', 'match', 'closest', 'could', 'find', 'header', ':(', 'someon', 'help']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['hayee', ':(', 'hayee', ':(', 'patwari', 'mam', \"ik'\", 'vision', 'would', 'say', 'noth', 'rather', 'lil', 'laugh']\n",
      "The unique integer ID for the unk_token is 2\n",
      "Model's prediction accuracy on a single training batch is: 100.0%\n",
      "Weighted number of correct predictions 64.0; weighted number of total observations predicted 64\n"
     ]
    }
   ],
   "source": [
    "# test your function\n",
    "tmp_val_generator = val_generator(64)\n",
    "\n",
    "# get one batch\n",
    "tmp_batch = next(tmp_val_generator)\n",
    "\n",
    "# Position 0 has the model inputs (tweets as tensors)\n",
    "# position 1 has the targets (the actual labels)\n",
    "tmp_inputs, tmp_targets, tmp_example_weights = tmp_batch\n",
    "\n",
    "# feed the tweet tensors into the model to get a prediction\n",
    "tmp_pred = training_loop.eval_model(tmp_inputs)\n",
    "\n",
    "tmp_acc, tmp_num_correct, tmp_num_predictions = compute_accuracy(preds=tmp_pred, y=tmp_targets, y_weights=tmp_example_weights)\n",
    "\n",
    "print(f\"Model's prediction accuracy on a single training batch is: {100 * tmp_acc}%\")\n",
    "print(f\"Weighted number of correct predictions {tmp_num_correct}; weighted number of total observations predicted {tmp_num_predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h2ep7nNejCYP"
   },
   "source": [
    "##### Expected output (Approximately)\n",
    "\n",
    "```\n",
    "Model's prediction accuracy on a single training batch is: 100.0%\n",
    "Weighted number of correct predictions 64.0; weighted number of total observations predicted 64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqle69F1grhM"
   },
   "source": [
    "<a name=\"5.2\"></a>\n",
    "## 5.2  Testing your model on Validation Data\n",
    "\n",
    "Now you will write test your model's prediction accuracy on validation data. \n",
    "\n",
    "This program will take in a data generator and your model. \n",
    "- The generator allows you to get batches of data. You can use it with a `for` loop:\n",
    "\n",
    "```\n",
    "for batch in iterator: \n",
    "   # do something with that batch\n",
    "```\n",
    "\n",
    "`batch` has dimensions `(X, Y, weights)`. \n",
    "- Column 0 corresponds to the tweet as a tensor (input).\n",
    "- Column 1 corresponds to its target (actual label, positive or negative sentiment).\n",
    "- Column 2 corresponds to the weights associated (example weights)\n",
    "- You can feed the tweet into model and it will return the predictions for the batch. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1zwYl_f9jCYP"
   },
   "source": [
    "<a name=\"ex08\"></a>\n",
    "### Exercise 08\n",
    "\n",
    "**Instructions:** \n",
    "- Compute the accuracy over all the batches in the validation iterator. \n",
    "- Make use of `compute_accuracy`, which you recently implemented, and return the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HKoTad4ggrhN"
   },
   "outputs": [],
   "source": [
    "# UNQ_C8 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: test_model\n",
    "def test_model(generator, model):\n",
    "    '''\n",
    "    Input: \n",
    "        generator: an iterator instance that provides batches of inputs and targets\n",
    "        model: a model instance \n",
    "    Output: \n",
    "        accuracy: float corresponding to the accuracy\n",
    "    '''\n",
    "    \n",
    "    accuracy = 0.\n",
    "    total_num_correct = 0\n",
    "    total_num_pred = 0\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    for batch in generator: \n",
    "        \n",
    "        # Retrieve the inputs from the batch\n",
    "        inputs = batch[0]\n",
    "        \n",
    "        # Retrieve the targets (actual labels) from the batch\n",
    "        targets = batch[1]\n",
    "        \n",
    "        # Retrieve the example weight.\n",
    "        example_weight = batch[2]\n",
    "\n",
    "        # Make predictions using the inputs\n",
    "        pred = model(inputs)\n",
    "        \n",
    "        # Calculate accuracy for the batch by comparing its predictions and targets\n",
    "        batch_accuracy, batch_num_correct, batch_num_pred = compute_accuracy(pred, targets, example_weights)\n",
    "\n",
    "        \n",
    "        # Update the total number of correct predictions\n",
    "        # by adding the number of correct predictions from this batch\n",
    "        total_num_correct += batch_num_correct\n",
    "        \n",
    "        # Update the total number of predictions \n",
    "        # by adding the number of predictions made for the batch\n",
    "        total_num_pred += batch_num_pred\n",
    "\n",
    "    # Calculate accuracy over all examples\n",
    "    accuracy = total_num_correct / total_num_pred\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1Rm_k21XgrhQ",
    "outputId": "65957ec4-d72b-4363-a5c2-20aa071e9005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words from the processed tweet:\n",
      "['bro', 'u', 'wan', 'cut', 'hair', 'anot', 'ur', 'hair', 'long', 'liao', 'bo', 'sinc', 'ord', 'liao', 'take', 'easi', 'lor', 'treat', 'save', 'leav', 'longer', ':)', 'bro', 'lol', 'sibei', 'xialan']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['back', 'thnx', 'god', \"i'm\", 'happi', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['thought', 'ear', 'malfunct', 'thank', 'good', 'clear', 'one', 'apolog', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['stuck', 'centr', 'right', 'clown', 'right', 'joker', 'left', '...', ':)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['happi', 'friday', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['follow', ':)', 'x']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['teenchoic', 'choiceinternationalartist', 'superjunior', 'fight', 'oppa', ':D']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['birthday', 'today', 'birthday', 'wish', 'hope', \"there'\", 'good', 'news', 'ben', 'soon', ':-)']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['help', '...', 'stop', 'cri', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['otl', 'nevermind', ':(', 'least', 'got', 'jeon']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['soon', 'tweet', 'plant', 'claw', 'thigh', 'traction', 'zoom', 'away', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['damnit', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['use', 'pri', 'pv', '...', 'wish', 'could', 'reliv', 'day', 'becom', 'nyc', 'pv', 'buy', 'way', 'commun', 'nyc', 'usa', 'klm', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['realli', 'hot', ':-(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['monday', ':(']\n",
      "The unique integer ID for the unk_token is 2\n",
      "List of words from the processed tweet:\n",
      "['go', 'stop', 'breakfast', 'earli', 'might', 'want', 'remov', '11am', 'websit', 'even', \"mcd'\", 'pull', 'trick', ':(']\n",
      "The unique integer ID for the unk_token is 2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "mul got incompatible shapes for broadcasting: (16,), (4,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-5a6afde371c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# testing the accuracy of your model: this takes around 20 seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'The accuracy of your model on the validation set is {accuracy:.4f}'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-5d4106c5b2d5>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(generator, model)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Calculate accuracy for the batch by comparing its predictions and targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mbatch_accuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_num_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_num_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-ab0e9657cb29>\u001b[0m in \u001b[0;36mcompute_accuracy\u001b[0;34m(preds, y, y_weights)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# Multiply each prediction with its corresponding weight.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mweighted_correct_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect_float\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Sum up the weighted correct predictions (of type np.float32), to go in the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/jax/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mdeferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   4257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_scalar_types\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_arraylike_types\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTracer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4258\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4259\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4260\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeferring_binary_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/jax/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_promote_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlax_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mbool_\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mbool_lax_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_wraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/jax/lax/lax.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[0;34mr\"\"\"Elementwise multiplication: :math:`x \\times y`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mmul_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0mtop_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_top_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtop_trace\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0mtracers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mapply_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m   \u001b[0;34m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m   \u001b[0mcompiled_fun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxla_primitive_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0munsafe_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/jax/interpreters/xla.py\u001b[0m in \u001b[0;36mxla_primitive_callable\u001b[0;34m(prim, *arg_specs, **params)\u001b[0m\n\u001b[1;32m    242\u001b[0m     return _xla_callable(lu.wrap_init(prim_fun), device, None, \"prim\", donated_invars,\n\u001b[1;32m    243\u001b[0m                          *arg_specs)\n\u001b[0;32m--> 244\u001b[0;31m   \u001b[0maval_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstract_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mavals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0mhandle_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maval_to_result_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maval_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/jax/lax/lax.py\u001b[0m in \u001b[0;36mstandard_abstract_eval\u001b[0;34m(prim, shape_rule, dtype_rule, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1743\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mConcreteArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1744\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mleast_specialized\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mShapedArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mShapedArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1746\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mleast_specialized\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mUnshapedArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1747\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnshapedArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/jax/lax/lax.py\u001b[0m in \u001b[0;36m_broadcasting_shape_rule\u001b[0;34m(name, *avals)\u001b[0m\n\u001b[1;32m   1798\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mresult_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{} got incompatible shapes for broadcasting: {}.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1800\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: mul got incompatible shapes for broadcasting: (16,), (4,)."
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT THIS CELL\n",
    "# testing the accuracy of your model: this takes around 20 seconds\n",
    "model = training_loop.eval_model\n",
    "accuracy = test_model(test_generator(16), model)\n",
    "\n",
    "print(f'The accuracy of your model on the validation set is {accuracy:.4f}', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "esUJRMQPgrhS"
   },
   "source": [
    "##### Expected Output (Approximately)\n",
    "\n",
    "```CPP\n",
    "The accuracy of your model on the validation set is 0.9931\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mct4P9QZgrhT"
   },
   "source": [
    "<a name=\"6\"></a>\n",
    "# Part 6:  Testing with your own input\n",
    "\n",
    "Finally you will test with your own input. You will see that deepnets are more powerful than the older methods you have used before. Although you go close to 100% accuracy on the first two assignments, the task was way easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SUq5cw-xgrhU"
   },
   "outputs": [],
   "source": [
    "# this is used to predict on your own sentnece\n",
    "def predict(sentence):\n",
    "    inputs = np.array(tweet_to_tensor(sentence, vocab_dict=Vocab))\n",
    "    \n",
    "    # Batch size 1, add dimension for batch, to work with the model\n",
    "    inputs = inputs[None, :]  \n",
    "    \n",
    "    # predict with the model\n",
    "    preds_probs = model(inputs)\n",
    "    \n",
    "    # Turn probabilities into categories\n",
    "    preds = int(preds_probs[0, 1] > preds_probs[0, 0])\n",
    "    \n",
    "    sentiment = \"negative\"\n",
    "    if preds == 1:\n",
    "        sentiment = 'positive'\n",
    "\n",
    "    return preds, sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "3RJntC57grhX",
    "outputId": "01f92c2d-738f-424b-d2b0-6e1f6ade4fef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words from the processed tweet:\n",
      "['nice', 'day', 'think', \"i'll\", 'take', 'sid', 'ramsgat', 'fish', 'chip', 'lunch', \"peter'\", 'fish', 'factori', 'beach', 'mayb']\n",
      "The unique integer ID for the unk_token is 2\n",
      "The sentiment of the sentence \n",
      "***\n",
      "\"It's such a nice day, think i'll be taking Sid to Ramsgate fish and chips for lunch at Peter's fish factory and then the beach maybe\"\n",
      "***\n",
      "is positive.\n",
      "\n",
      "List of words from the processed tweet:\n",
      "['hate', 'day', 'worst', \"i'm\", 'sad']\n",
      "The unique integer ID for the unk_token is 2\n",
      "The sentiment of the sentence \n",
      "***\n",
      "\"I hated my day, it was the worst, I'm so sad.\"\n",
      "***\n",
      "is negative.\n"
     ]
    }
   ],
   "source": [
    "# try a positive sentence\n",
    "sentence = \"It's such a nice day, think i'll be taking Sid to Ramsgate fish and chips for lunch at Peter's fish factory and then the beach maybe\"\n",
    "tmp_pred, tmp_sentiment = predict(sentence)\n",
    "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")\n",
    "\n",
    "print()\n",
    "# try a negative sentence\n",
    "sentence = \"I hated my day, it was the worst, I'm so sad.\"\n",
    "tmp_pred, tmp_sentiment = predict(sentence)\n",
    "print(f\"The sentiment of the sentence \\n***\\n\\\"{sentence}\\\"\\n***\\nis {tmp_sentiment}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nZmGCheXjCYX"
   },
   "source": [
    "Notice that the model works well even for complex sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mNg0fAYIgrhd"
   },
   "source": [
    "### On Deep Nets\n",
    "\n",
    "Deep nets allow you to understand and capture dependencies that you would have not been able to capture with a simple linear regression, or logistic regression. \n",
    "- It also allows you to better use pre-trained embeddings for classification and tends to generalize better."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "C3_W1_Assignment_Solution.ipynb",
   "provenance": []
  },
  "coursera": {
   "schema_names": [
    "NLPC3-1A"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
